Args: /home/lfr/MLIR_Tutorial/build/15-lowing_to_llvm/src/Tools/NS-opt/NS-opt15 /home/lfr/MLIR_Tutorial/15-lowing_to_llvm/test/Pipeline/to_llvm_pipeline.mlir --north-star-basic-pipeline=DP_Nums=2 --mlir-print-ir-after-all --debug 
Load new dialect in Context builtin
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemRefLayoutAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TypedAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ElementsAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DistinctAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ResourceBlobManagerDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineBinaryOpExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineConstantExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineDimExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineMapStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::IntegerSetStorage)
Load new dialect in Context builtin
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroOperands<mlir::TypeID::get() [with Trait = mlir::OpTrait::ZeroOperands]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneRegion<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneRegion]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroResults<mlir::TypeID::get() [with Trait = mlir::OpTrait::ZeroResults]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroSuccessors<mlir::TypeID::get() [with Trait = mlir::OpTrait::ZeroSuccessors]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoRegionArguments<mlir::TypeID::get() [with Trait = mlir::OpTrait::NoRegionArguments]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoTerminator<mlir::TypeID::get() [with Trait = mlir::OpTrait::NoTerminator]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlock<mlir::TypeID::get() [with Trait = mlir::OpTrait::SingleBlock]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OpInvariants<mlir::TypeID::get() [with Trait = mlir::OpTrait::OpInvariants]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::BytecodeOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AffineScope<mlir::TypeID::get() [with Trait = mlir::OpTrait::AffineScope]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsIsolatedFromAbove<mlir::TypeID::get() [with Trait = mlir::OpTrait::IsIsolatedFromAbove]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SymbolTable<mlir::TypeID::get() [with Trait = mlir::OpTrait::SymbolTable]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::SymbolOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::OpAsmOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface::Trait<mlir::TypeID::get() [with Trait = mlir::RegionKindInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasOnlyGraphRegion<mlir::TypeID::get() [with Trait = mlir::OpTrait::HasOnlyGraphRegion]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::ModuleOpGenericAdaptorBase::Properties)
Load new dialect in Context func
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolUserOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectInlinerInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConvertToLLVMPatternInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferizableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AutomaticAllocationScope<mlir::TypeID::get() [with Trait = mlir::OpTrait::AutomaticAllocationScope]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::CallableOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::FunctionOpInterface::Trait]::Empty>)
Load new dialect in Context north_star
Load new dialect in Context tensor
Load new dialect in Context affine
Load new dialect in Context arith
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithFastMathInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorUnrollOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferIntRangeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithIntegerOverflowFlagsInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CastOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithRoundingModeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferDeallocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ValueBoundsOpInterface)
Load new dialect in Context ub
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ub::PoisonAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineMapAccessInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineDmaStartOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineDmaWaitOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LoopLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineReadOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineWriteOpInterface)
Load new dialect in Context complex
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ReifyRankedShapedTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedDimOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OffsetSizeAndStrideOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestinationStyleOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::FindPayloadReplacementOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SubsetOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SubsetInsertionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SubsetExtractionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TilingInterface)
Load new dialect in Context linalg
Load new dialect in Context math
Load new dialect in Context memref
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CopyOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableMemOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableAccessorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableAllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableAllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ViewLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::AllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RuntimeVerifiableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::AggregatedOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::LinalgOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::ContractionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::ConvolutionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::FillOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::mesh::ShardingInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PartialReductionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DistributeParallelAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataParallelAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DistributeParallelOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SupportedDataParallelismOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BuiltinFunctionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FusionRegionOpInterfaces)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroRegions<mlir::TypeID::get() [with Trait = mlir::OpTrait::ZeroRegions]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VariadicOperands<mlir::TypeID::get() [with Trait = mlir::OpTrait::VariadicOperands]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::func::FuncOp>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::HasParent<mlir::func::FuncOp>::Impl]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable::Trait<mlir::TypeID::get() [with Trait = mlir::ConditionallySpeculatable::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AlwaysSpeculatableImplTrait<mlir::TypeID::get() [with Trait = mlir::OpTrait::AlwaysSpeculatableImplTrait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::MemoryEffectOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::MemRefsNormalizable<mlir::TypeID::get() [with Trait = mlir::OpTrait::MemRefsNormalizable]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::RegionBranchTerminatorOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ReturnLike<mlir::TypeID::get() [with Trait = mlir::OpTrait::ReturnLike]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsTerminator<mlir::TypeID::get() [with Trait = mlir::OpTrait::IsTerminator]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataLayoutSpecInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneResult<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneResult]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::Type>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneTypedResult<mlir::Type>::Impl]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneOperand<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneOperand]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DistributeParallelOp::Trait<mlir::TypeID::get() [with Trait = mlir::DistributeParallelOp::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SupportedDataParallelismOp::Trait<mlir::TypeID::get() [with Trait = mlir::SupportedDataParallelismOp::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::OpToOpPassAdaptor)
Load new dialect in Context bufferization
Load new dialect in Context llvm
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::DIRecursiveTypeAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMVoidType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMPPCFP128Type)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMX86MMXType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMTokenType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMLabelType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMMetadataType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::LLVMStructType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataLayoutTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::IntegerOverflowFlagsInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::GetResultPtrElementType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::AccessGroupOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::AliasAnalysisOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BranchOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::FastmathFlagsInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::BranchWeightOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SafeMemorySlotAccessOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::FPExceptionBehaviorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::RoundingModeOpInterface)
Load new dialect in Context transform
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::TransformOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::PatternDescriptorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::ConversionPatternDescriptorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::TypeConverterBuilderOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::MatchOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::TransformParamTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::TransformHandleTypeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::TransformValueHandleTypeInterface)
Load new dialect in Context index
Load new dialect in Context scf
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ParallelCombiningOpInterface)
Load new dialect in Context vector
ImplicitTypeIDRegistry::lookupOrInsert(mlir::vector::MaskableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::vector::MaskingOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorTransferOpInterface)
Load new dialect in Context gpu
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::AsyncTokenType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::MMAMatrixType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::SparseDnTensorHandleType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::SparseSpMatHandleType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::SparseSpGEMMOpHandleType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::gpu::AsyncOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DataLayoutOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DeviceMappingAttrInterface)
run in MarkDistributeParallelParametersPass
root op: builtin.module
DPNums: 2
TPNums: 1
EPNums: 0
run out: MarkDistributeParallelParametersPass
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::PreservedAnalyses::AllAnalysesType)
// -----// IR Dump After MarkDistributeParallelParametersPass (mark-distribute-parallel-parameters) //----- //
module @NorthStar {
  func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
    %0 = "north_star.softmax"(%arg0) <{axis = 1 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.softmax"(%0) <{axis = 1 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0>
    return %1 : !north_star.ns_tensor<2x128xf32,0>
  }
}


run in ApplyDistributeTransformPass
root op: func.func
ImplicitTypeIDRegistry::lookupOrInsert(mlir::north_star::detail::BufferCastOpGenericAdaptorBase::Properties)
Apply DataParallelism to north_star.softmax
Apply DataParallelism to north_star.softmax
run out: ApplyDistributeTransformPass
// -----// IR Dump After ApplyDistributeTransformPass (apply-distribute-transform) //----- //
func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %3 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  %4:2 = "north_star.buffer_cast"(%3) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %5 = "north_star.softmax"(%4#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %6 = "north_star.softmax"(%4#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %7 = "north_star.buffer_cast"(%5, %6) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  return %7 : !north_star.ns_tensor<2x128xf32,0>
}

ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VariadicResults<mlir::TypeID::get() [with Trait = mlir::OpTrait::VariadicResults]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectFoldInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ConstantLike<mlir::TypeID::get() [with Trait = mlir::OpTrait::ConstantLike]::Empty>)

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590a104b0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5c9590aa6950) {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)


  * Pattern mlir::north_star::{anonymous}::BufferCastOpFold : 'north_star.buffer_cast -> ()' {
Trying to match "mlir::north_star::{anonymous}::BufferCastOpFold"
"mlir::north_star::{anonymous}::BufferCastOpFold" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590a921e0) {
  %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aa6380) {
  %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5c9590aa69f0) {
  %3 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::BufferCastOpFold : 'north_star.buffer_cast -> ()' {
Trying to match "mlir::north_star::{anonymous}::BufferCastOpFold"
"mlir::north_star::{anonymous}::BufferCastOpFold" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5c9590aa7370) {
  %4:2 = "north_star.buffer_cast"(%3) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)


  * Pattern mlir::north_star::{anonymous}::BufferCastOpFold : 'north_star.buffer_cast -> ()' {
Trying to match "mlir::north_star::{anonymous}::BufferCastOpFold"
    ** Modified: 'north_star.softmax'(0x5c9590a0f8a0)
    ** Modified: 'north_star.softmax'(0x5c9590aa72d0)
    ** Erase   : 'north_star.buffer_cast'(0x5c9590aa7370)
    ** Erase   : 'north_star.buffer_cast'(0x5c9590aa69f0)
"mlir::north_star::{anonymous}::BufferCastOpFold" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %3 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %4 = "north_star.softmax"(%2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = "north_star.buffer_cast"(%3, %4) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  return %5 : !north_star.ns_tensor<2x128xf32,0>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aa6380) {
  %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590a921e0) {
  %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590a104b0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590a0f8a0) {
  %3 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aa72d0) {
  %4 = "north_star.softmax"(%2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5c9590aa7410) {
  %5 = "north_star.buffer_cast"(%3, %4) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::BufferCastOpFold : 'north_star.buffer_cast -> ()' {
Trying to match "mlir::north_star::{anonymous}::BufferCastOpFold"
"mlir::north_star::{anonymous}::BufferCastOpFold" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590a0ffc0) {
  "func.return"(%5) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasRecursiveMemoryEffects<mlir::TypeID::get() [with Trait = mlir::OpTrait::HasRecursiveMemoryEffects]::Empty>)

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590a104b0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5c9590aa6950) {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)


  * Pattern mlir::north_star::{anonymous}::BufferCastOpFold : 'north_star.buffer_cast -> ()' {
Trying to match "mlir::north_star::{anonymous}::BufferCastOpFold"
"mlir::north_star::{anonymous}::BufferCastOpFold" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590a921e0) {
  %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aa6380) {
  %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590a0f8a0) {
  %3 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aa72d0) {
  %4 = "north_star.softmax"(%2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5c9590aa7410) {
  %5 = "north_star.buffer_cast"(%3, %4) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::BufferCastOpFold : 'north_star.buffer_cast -> ()' {
Trying to match "mlir::north_star::{anonymous}::BufferCastOpFold"
"mlir::north_star::{anonymous}::BufferCastOpFold" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590a0ffc0) {
  "func.return"(%5) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module @NorthStar {
  func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
    %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
    %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %3 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %4 = "north_star.softmax"(%2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %5 = "north_star.buffer_cast"(%3, %4) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
    return %5 : !north_star.ns_tensor<2x128xf32,0>
  }
}


run in DeviceRegionFusionPass
root op: func.func

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590a0ffc0) {
  "func.return"(%5) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5c9590aa7410) {
  %5 = "north_star.buffer_cast"(%3, %4) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern {anonymous}::BufferCastOpDeviceRegionFusion : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpDeviceRegionFusion"
"{anonymous}::BufferCastOpDeviceRegionFusion" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aa72d0) {
  %4 = "north_star.softmax"(%2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590a0f8a0) {
  %3 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aa6380) {
  %2 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590a921e0) {
  %1 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5c9590aa6950) {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)


  * Pattern {anonymous}::BufferCastOpDeviceRegionFusion : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpDeviceRegionFusion"
ImplicitTypeIDRegistry::lookupOrInsert(mlir::north_star::detail::DeviceKernelOpGenericAdaptorBase::Properties)
    ** Insert  : 'north_star.device_kernel'(0x5c9590aa7360)
    ** Insert  : 'north_star.return'(0x5c9590aae360)
    ** Modified: 'north_star.buffer_cast'(0x5c9590aa7410)
    ** Insert  : 'north_star.device_kernel'(0x5c9590aae470)
    ** Insert  : 'north_star.return'(0x5c9590aae680)
    ** Modified: 'north_star.buffer_cast'(0x5c9590aa7410)
"{anonymous}::BufferCastOpDeviceRegionFusion" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FusionRegionOpInterfaces::Trait<mlir::TypeID::get() [with Trait = mlir::FusionRegionOpInterfaces::Trait]::Empty>)
func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %1 = "north_star.device_kernel"(%0#0) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,0>):
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::north_star::DeviceKernelOp>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::HasParent<mlir::north_star::DeviceKernelOp>::Impl]::Empty>)
    %8 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %9 = "north_star.softmax"(%8) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    north_star.return %9 : !north_star.ns_tensor<1x128xf32,0>
  }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %2 = "north_star.device_kernel"(%0#1) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
    %8 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %9 = "north_star.softmax"(%8) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    north_star.return %9 : !north_star.ns_tensor<1x128xf32,1>
  }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %3 = "north_star.softmax"(%0#0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %4 = "north_star.softmax"(%0#1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = "north_star.softmax"(%3) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %6 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %7 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  return %7 : !north_star.ns_tensor<2x128xf32,0>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5c9590aae680) {
  "north_star.return"(%9) : (!north_star.ns_tensor<1x128xf32,1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5c9590aae470) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5c9590aa7410) {
  %7 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern {anonymous}::BufferCastOpDeviceRegionFusion : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpDeviceRegionFusion"
"{anonymous}::BufferCastOpDeviceRegionFusion" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5c9590aae360) {
  "north_star.return"(%11) : (!north_star.ns_tensor<1x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5c9590aa7360) {
} -> failure : pattern failed to match
//===-------------------------------------------===//
** Erase   : 'north_star.softmax'(0x5c9590aa72d0)
** Erase   : 'north_star.softmax'(0x5c9590a0f8a0)
** Erase   : 'north_star.softmax'(0x5c9590aa6380)
** Erase   : 'north_star.softmax'(0x5c9590a921e0)

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590a0ffc0) {
  "func.return"(%3) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5c9590aa7410) {
  %3 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern {anonymous}::BufferCastOpDeviceRegionFusion : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpDeviceRegionFusion"
"{anonymous}::BufferCastOpDeviceRegionFusion" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5c9590aae680) {
  "north_star.return"(%5) : (!north_star.ns_tensor<1x128xf32,1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aae600) {
  %5 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5c9590aae470) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aae570) {
  %4 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5c9590aae360) {
  "north_star.return"(%7) : (!north_star.ns_tensor<1x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aae2a0) {
  %7 = "north_star.softmax"(%6) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5c9590aa7360) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590a0fdf0) {
  %6 = "north_star.softmax"(%arg2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5c9590aa6950) {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)


  * Pattern {anonymous}::BufferCastOpDeviceRegionFusion : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpDeviceRegionFusion"
"{anonymous}::BufferCastOpDeviceRegionFusion" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//
region has changed: true
run out: DeviceRegionFusionPass
// -----// IR Dump After DeviceRegionFusionPass (device-region-fusion) //----- //
func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %1 = "north_star.device_kernel"(%0#0) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,0>):
    %4 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %5 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    north_star.return %5 : !north_star.ns_tensor<1x128xf32,0>
  }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %2 = "north_star.device_kernel"(%0#1) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
    %4 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %5 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    north_star.return %5 : !north_star.ns_tensor<1x128xf32,1>
  }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %3 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  return %3 : !north_star.ns_tensor<2x128xf32,0>
}

run in EliminateBufferCastPass

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590a0ffc0) {
  "func.return"(%3) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5c9590aa7410) {
  %3 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern {anonymous}::BufferCastOpToCommunicationPattern : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpToCommunicationPattern"
    ** Insert  : 'north_star.buffer'(0x5c9590aaf710)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::RankedTensorType>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneTypedResult<mlir::RankedTensorType>::Impl]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ReifyRankedShapedTypeOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::ReifyRankedShapedTypeOpInterface::Trait]::Empty>)
    ** Insert  : 'tensor.empty'(0x5c9590aae710)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::north_star::detail::TensorToNSTensorOpGenericAdaptorBase::Properties)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590a921e0)
    ** Insert  : 'north_star.buffer'(0x5c9590aa6380)
    ** Insert  : 'north_star.gather'(0x5c9590a84f40)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::north_star::detail::GetTensorOpGenericAdaptorBase::Properties)
    ** Insert  : 'north_star.get_tensor'(0x5c9590a0f8a0)
    ** Replace : 'north_star.buffer_cast'(0x5c9590aa7410)
    ** Modified: 'func.return'(0x5c9590a0ffc0)
"{anonymous}::BufferCastOpToCommunicationPattern" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %1 = "north_star.device_kernel"(%0#0) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,0>):
    %9 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %10 = "north_star.softmax"(%9) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    north_star.return %10 : !north_star.ns_tensor<1x128xf32,0>
  }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %2 = "north_star.device_kernel"(%0#1) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
    %9 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.softmax"(%9) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    north_star.return %10 : !north_star.ns_tensor<1x128xf32,1>
  }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %3 = "north_star.buffer"(%1, %2) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %4 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %6 = "north_star.buffer"(%5) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%3, %6) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %8 = "north_star.buffer_cast"(%1, %2) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  return %7 : !north_star.ns_tensor<2x128xf32,0>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590a0ffc0) {
  "func.return"(%7) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

ImplicitTypeIDRegistry::lookupOrInsert(mlir::BuiltinFunctionOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::BuiltinFunctionOpInterface::Trait]::Empty>)
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590a0f8a0) {
  %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.gather'(0x5c9590a84f40) {
  "north_star.gather"(%3, %6) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()

ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NOperands<2>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::NOperands<2>::Impl]::Empty>)
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aa6380) {
  %6 = "north_star.buffer"(%5) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590a921e0) {
  %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590aae710) {
  %4 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aaf710) {
  %3 = "north_star.buffer"(%1, %2) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5c9590aae680) {
  "north_star.return"(%10) : (!north_star.ns_tensor<1x128xf32,1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aae600) {
  %10 = "north_star.softmax"(%9) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5c9590aae470) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aae570) {
  %9 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5c9590aae360) {
  "north_star.return"(%12) : (!north_star.ns_tensor<1x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aae2a0) {
  %12 = "north_star.softmax"(%11) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5c9590aa7360) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590a0fdf0) {
  %11 = "north_star.softmax"(%arg2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590a104b0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer_cast'(0x5c9590aa6950) {
  %0:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)


  * Pattern {anonymous}::BufferCastOpToCommunicationPattern : 'north_star.buffer_cast -> ()' {
Trying to match "{anonymous}::BufferCastOpToCommunicationPattern"
    ** Insert  : 'north_star.buffer'(0x5c9590aa72d0)
    ** Insert  : 'tensor.empty'(0x5c9590a9a430)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590aae780)
    ** Insert  : 'tensor.empty'(0x5c9590a9a760)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590aabe30)
    ** Insert  : 'north_star.buffer'(0x5c9590aabec0)
    ** Insert  : 'north_star.scatter'(0x5c9590a9a650)
    ** Insert  : 'north_star.get_tensor'(0x5c9590aabf70)
    ** Insert  : 'north_star.get_tensor'(0x5c9590aac000)
    ** Replace : 'north_star.buffer_cast'(0x5c9590aa6950)
    ** Modified: 'north_star.device_kernel'(0x5c9590aa7360)
    ** Modified: 'north_star.device_kernel'(0x5c9590aae470)
"{anonymous}::BufferCastOpToCommunicationPattern" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
  %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  %1 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %3 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %7 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %8:2 = "north_star.buffer_cast"(%arg0) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<2x128xf32,0>) -> (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>)
  %9 = "north_star.device_kernel"(%6) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,0>):
    %17 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %18 = "north_star.softmax"(%17) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    north_star.return %18 : !north_star.ns_tensor<1x128xf32,0>
  }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %10 = "north_star.device_kernel"(%7) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
    %17 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %18 = "north_star.softmax"(%17) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    north_star.return %18 : !north_star.ns_tensor<1x128xf32,1>
  }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %11 = "north_star.buffer"(%9, %10) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %12 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %13 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %14 = "north_star.buffer"(%13) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%11, %14) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %15 = "north_star.get_tensor"(%14) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %16 = "north_star.buffer_cast"(%9, %10) <{distribute_attr = #north_star.DP<DP = 2 : 0, 1>}> : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<2x128xf32,0>
  return %15 : !north_star.ns_tensor<2x128xf32,0>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5c9590aae470) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5c9590aa7360) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590aac000) {
  %7 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590aabf70) {
  %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.scatter'(0x5c9590a9a650) {
  "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aabec0) {
  %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aabe30) {
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590a9a760) {
  %3 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aae780) {
  %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590a9a430) {
  %1 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590a104b0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aa72d0) {
  %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//
** Erase   : 'north_star.buffer_cast'(0x5c9590aa7410)
** Erase   : 'north_star.buffer_cast'(0x5c9590aa6950)

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590a0ffc0) {
  "func.return"(%14) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590a0f8a0) {
  %14 = "north_star.get_tensor"(%13) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.gather'(0x5c9590a84f40) {
  "north_star.gather"(%10, %13) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aa6380) {
  %13 = "north_star.buffer"(%12) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590a921e0) {
  %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590aae710) {
  %11 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aaf710) {
  %10 = "north_star.buffer"(%8, %9) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5c9590aae680) {
  "north_star.return"(%16) : (!north_star.ns_tensor<1x128xf32,1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aae600) {
  %16 = "north_star.softmax"(%15) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5c9590aae470) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aae570) {
  %15 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5c9590aae360) {
  "north_star.return"(%18) : (!north_star.ns_tensor<1x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590aae2a0) {
  %18 = "north_star.softmax"(%17) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5c9590aa7360) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.softmax'(0x5c9590a0fdf0) {
  %17 = "north_star.softmax"(%arg2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590aac000) {
  %7 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590aabf70) {
  %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.scatter'(0x5c9590a9a650) {
  "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aabec0) {
  %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aabe30) {
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590a9a760) {
  %3 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aae780) {
  %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590a9a430) {
  %1 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590a104b0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aa72d0) {
  %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//
run out: EliminateBufferCastPass
// -----// IR Dump After EliminateBufferCastPass (eliminate-buffercast) //----- //
module @NorthStar {
  func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
    %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    %1 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %3 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %7 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %8 = "north_star.device_kernel"(%6) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
    ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,0>):
      %15 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
      %16 = "north_star.softmax"(%15) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
      north_star.return %16 : !north_star.ns_tensor<1x128xf32,0>
    }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %9 = "north_star.device_kernel"(%7) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
    ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
      %15 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
      %16 = "north_star.softmax"(%15) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
      north_star.return %16 : !north_star.ns_tensor<1x128xf32,1>
    }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.buffer"(%8, %9) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    %11 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %13 = "north_star.buffer"(%12) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%10, %13) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %14 = "north_star.get_tensor"(%13) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    return %14 : !north_star.ns_tensor<2x128xf32,0>
  }
}


run in ConvertNorthStarToLinalgPass

//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x5c95909deb50) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x5c9590a104b0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5c9590aa72d0) {
  %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590a9a430) {
  %1 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aae780) {
  %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590a9a760) {
  %3 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aabe30) {
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5c9590aabec0) {
  %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.scatter'(0x5c9590a9a650) {
  "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.get_tensor'(0x5c9590aabf70) {
  %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.get_tensor'(0x5c9590aac000) {
  %7 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.device_kernel'(0x5c9590aa7360) {
  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.device_kernel -> ()' {
Trying to match "{anonymous}::DeviceKernelOpConvertPattern"
    ** Insert  : 'north_star.device_kernel'(0x5c9590aa6940)
    ** Insert Block into : 'north_star.device_kernel'(0x5c9590aa6940)
    ** Insert  : 'north_star.softmax'(0x5c9590aacec0)
    ** Insert  : 'north_star.softmax'(0x5c9590aaf170)
    ** Insert  : 'north_star.return'(0x5c9590aaf1d0)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590ab0b60)
    ** Replace : 'north_star.device_kernel'(0x5c9590aa7360)
"{anonymous}::DeviceKernelOpConvertPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.softmax'(0x5c9590aacec0) {
      %22 = "north_star.softmax"(%21) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.softmax -> ()' {
Trying to match "{anonymous}::SoftmaxOpToLinalgPattern"
        ** Insert  : 'tensor.empty'(0x5c9590ab3c10)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::detail::SoftmaxOpGenericAdaptorBase::Properties)
        ** Insert  : 'linalg.softmax'(0x5c9590ab3c80)
        ** Replace : 'north_star.softmax'(0x5c9590aacec0)
"{anonymous}::SoftmaxOpToLinalgPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'tensor.empty'(0x5c9590ab3c10) {
          %23 = "tensor.empty"() : () -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'linalg.softmax'(0x5c9590ab3c80) {
          %24 = "linalg.softmax"(%22, %23) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
%0 = "north_star.device_kernel"(<<UNKNOWN SSA VALUE>>) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %1 = builtin.unrealized_conversion_cast %0 : !north_star.ns_tensor<1x128xf32,0> to tensor<1x128xf32>
  %2 = tensor.empty() : tensor<1x128xf32>
  %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
  %4 = "north_star.softmax"(%0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %5 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  north_star.return %5 : !north_star.ns_tensor<1x128xf32,0>
}) : (tensor<1x128xf32>) -> tensor<1x128xf32>


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.device_kernel'(0x5c9590aa6940) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.softmax'(0x5c9590aacec0) {
      %25 = "north_star.softmax"(%21) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

    } -> SUCCESS : operation marked 'ignored' during conversion
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.softmax'(0x5c9590aaf170) {
      %26 = "north_star.softmax"(%25) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.softmax -> ()' {
Trying to match "{anonymous}::SoftmaxOpToLinalgPattern"
        ** Insert  : 'tensor.empty'(0x5c9590ab3e00)
        ** Insert  : 'linalg.softmax'(0x5c9590ab3e70)
        ** Replace : 'north_star.softmax'(0x5c9590aaf170)
"{anonymous}::SoftmaxOpToLinalgPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'tensor.empty'(0x5c9590ab3e00) {
          %26 = "tensor.empty"() : () -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'linalg.softmax'(0x5c9590ab3e70) {
          %27 = "linalg.softmax"(%24, %26) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
%0 = "north_star.device_kernel"(<<UNKNOWN SSA VALUE>>) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %1 = builtin.unrealized_conversion_cast %0 : !north_star.ns_tensor<1x128xf32,0> to tensor<1x128xf32>
  %2 = tensor.empty() : tensor<1x128xf32>
  %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
  %4 = "north_star.softmax"(%0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %5 = tensor.empty() : tensor<1x128xf32>
  %6 = linalg.softmax dimension(1) ins(%3 : tensor<1x128xf32>) outs(%5 : tensor<1x128xf32>) -> tensor<1x128xf32>
  %7 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  north_star.return %7 : !north_star.ns_tensor<1x128xf32,0>
}) : (tensor<1x128xf32>) -> tensor<1x128xf32>


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.return'(0x5c9590aaf1d0) {
      "north_star.return"(%28) : (!north_star.ns_tensor<1x128xf32,0>) -> ()

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.return -> ()' {
Trying to match "{anonymous}::ReturnOpConvertPattern"
        ** Insert  : 'north_star.return'(0x5c9590aad0c0)
        ** Replace : 'north_star.return'(0x5c9590aaf1d0)
"{anonymous}::ReturnOpConvertPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'north_star.return'(0x5c9590aad0c0) {
          "north_star.return"(%27) : (tensor<1x128xf32>) -> ()

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'north_star.return' op must be the last operation in the parent block
mlir-asm-printer: 'north_star.device_kernel' failed to verify and will be printed in generic form
%0 = "north_star.device_kernel"(<<UNKNOWN SSA VALUE>>) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %2 = "builtin.unrealized_conversion_cast"(%1) : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %3 = "tensor.empty"() : () -> tensor<1x128xf32>
  %4 = "linalg.softmax"(%2, %3) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
  %5 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %6 = "tensor.empty"() : () -> tensor<1x128xf32>
  %7 = "linalg.softmax"(%4, %6) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
  %8 = "north_star.softmax"(%5) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  "north_star.return"(%7) : (tensor<1x128xf32>) -> ()
  "north_star.return"(%8) : (!north_star.ns_tensor<1x128xf32,0>) -> ()
}) : (tensor<1x128xf32>) -> tensor<1x128xf32>


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab0b60) {
      %21 = "north_star.tensor_to_ns_tensor"(%arg3) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'north_star.return' op must be the last operation in the parent block
mlir-asm-printer: 'func.func' failed to verify and will be printed in generic form
"func.func"() <{function_type = (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0>, sym_name = "main"}> ({
^bb0(%arg0: !north_star.ns_tensor<2x128xf32,0>):
  %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  %1 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>
  %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %3 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %7 = "builtin.unrealized_conversion_cast"(%6) : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %8 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %9 = "north_star.device_kernel"(%7) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg3: tensor<1x128xf32>):
    %21 = "north_star.tensor_to_ns_tensor"(%arg3) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %22 = "builtin.unrealized_conversion_cast"(%21) : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %23 = "tensor.empty"() : () -> tensor<1x128xf32>
    %24 = "linalg.softmax"(%22, %23) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %25 = "north_star.softmax"(%21) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %26 = "tensor.empty"() : () -> tensor<1x128xf32>
    %27 = "linalg.softmax"(%24, %26) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %28 = "north_star.softmax"(%25) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    "north_star.return"(%27) : (tensor<1x128xf32>) -> ()
    "north_star.return"(%28) : (!north_star.ns_tensor<1x128xf32,0>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %10 = "north_star.device_kernel"(%6) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg2: !north_star.ns_tensor<1x128xf32,0>):
    %19 = "north_star.softmax"(%arg2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %20 = "north_star.softmax"(%19) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    "north_star.return"(%20) : (!north_star.ns_tensor<1x128xf32,0>) -> ()
  }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %11 = "north_star.device_kernel"(%8) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
    %17 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %18 = "north_star.softmax"(%17) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    "north_star.return"(%18) : (!north_star.ns_tensor<1x128xf32,1>) -> ()
  }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %12 = "north_star.buffer"(%10, %11) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %13 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>
  %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %15 = "north_star.buffer"(%14) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%12, %15) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %16 = "north_star.get_tensor"(%15) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  "func.return"(%16) : (!north_star.ns_tensor<2x128xf32,0>) -> ()
}) {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} : () -> ()


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.softmax'(0x5c9590a0fdf0) {
  %19 = "north_star.softmax"(%arg2) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.softmax'(0x5c9590aae2a0) {
  %20 = "north_star.softmax"(%19) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.return'(0x5c9590aae360) {
  "north_star.return"(%20) : (!north_star.ns_tensor<1x128xf32,0>) -> ()

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.device_kernel'(0x5c9590aae470) {
  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.device_kernel -> ()' {
Trying to match "{anonymous}::DeviceKernelOpConvertPattern"
    ** Insert  : 'north_star.device_kernel'(0x5c9590ab11a0)
    ** Insert Block into : 'north_star.device_kernel'(0x5c9590ab11a0)
    ** Insert  : 'north_star.softmax'(0x5c9590ab0c20)
    ** Insert  : 'north_star.softmax'(0x5c9590ab1280)
    ** Insert  : 'north_star.return'(0x5c9590ab12e0)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590ab15c0)
    ** Replace : 'north_star.device_kernel'(0x5c9590aae470)
"{anonymous}::DeviceKernelOpConvertPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.softmax'(0x5c9590ab0c20) {
      %22 = "north_star.softmax"(%21) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.softmax -> ()' {
Trying to match "{anonymous}::SoftmaxOpToLinalgPattern"
        ** Insert  : 'tensor.empty'(0x5c9590a0eb30)
        ** Insert  : 'linalg.softmax'(0x5c9590ab0f40)
        ** Replace : 'north_star.softmax'(0x5c9590ab0c20)
"{anonymous}::SoftmaxOpToLinalgPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'tensor.empty'(0x5c9590a0eb30) {
          %23 = "tensor.empty"() : () -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'linalg.softmax'(0x5c9590ab0f40) {
          %24 = "linalg.softmax"(%22, %23) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
%0 = "north_star.device_kernel"(<<UNKNOWN SSA VALUE>>) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %1 = builtin.unrealized_conversion_cast %0 : !north_star.ns_tensor<1x128xf32,1> to tensor<1x128xf32>
  %2 = tensor.empty() : tensor<1x128xf32>
  %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
  %4 = "north_star.softmax"(%0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  north_star.return %5 : !north_star.ns_tensor<1x128xf32,1>
}) : (tensor<1x128xf32>) -> tensor<1x128xf32>


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.device_kernel'(0x5c9590ab11a0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.softmax'(0x5c9590ab0c20) {
      %25 = "north_star.softmax"(%21) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

    } -> SUCCESS : operation marked 'ignored' during conversion
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.softmax'(0x5c9590ab1280) {
      %26 = "north_star.softmax"(%25) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.softmax -> ()' {
Trying to match "{anonymous}::SoftmaxOpToLinalgPattern"
        ** Insert  : 'tensor.empty'(0x5c9590ab41c0)
        ** Insert  : 'linalg.softmax'(0x5c9590ab4230)
        ** Replace : 'north_star.softmax'(0x5c9590ab1280)
"{anonymous}::SoftmaxOpToLinalgPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'tensor.empty'(0x5c9590ab41c0) {
          %26 = "tensor.empty"() : () -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'linalg.softmax'(0x5c9590ab4230) {
          %27 = "linalg.softmax"(%24, %26) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
%0 = "north_star.device_kernel"(<<UNKNOWN SSA VALUE>>) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %1 = builtin.unrealized_conversion_cast %0 : !north_star.ns_tensor<1x128xf32,1> to tensor<1x128xf32>
  %2 = tensor.empty() : tensor<1x128xf32>
  %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
  %4 = "north_star.softmax"(%0) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = tensor.empty() : tensor<1x128xf32>
  %6 = linalg.softmax dimension(1) ins(%3 : tensor<1x128xf32>) outs(%5 : tensor<1x128xf32>) -> tensor<1x128xf32>
  %7 = "north_star.softmax"(%4) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  north_star.return %7 : !north_star.ns_tensor<1x128xf32,1>
}) : (tensor<1x128xf32>) -> tensor<1x128xf32>


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.return'(0x5c9590ab12e0) {
      "north_star.return"(%28) : (!north_star.ns_tensor<1x128xf32,1>) -> ()

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.return -> ()' {
Trying to match "{anonymous}::ReturnOpConvertPattern"
        ** Insert  : 'north_star.return'(0x5c9590ab42d0)
        ** Replace : 'north_star.return'(0x5c9590ab12e0)
"{anonymous}::ReturnOpConvertPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'north_star.return'(0x5c9590ab42d0) {
          "north_star.return"(%27) : (tensor<1x128xf32>) -> ()

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'north_star.return' op must be the last operation in the parent block
mlir-asm-printer: 'north_star.device_kernel' failed to verify and will be printed in generic form
%0 = "north_star.device_kernel"(<<UNKNOWN SSA VALUE>>) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %2 = "builtin.unrealized_conversion_cast"(%1) : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  %3 = "tensor.empty"() : () -> tensor<1x128xf32>
  %4 = "linalg.softmax"(%2, %3) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
  %5 = "north_star.softmax"(%1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %6 = "tensor.empty"() : () -> tensor<1x128xf32>
  %7 = "linalg.softmax"(%4, %6) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
  %8 = "north_star.softmax"(%5) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  "north_star.return"(%7) : (tensor<1x128xf32>) -> ()
  "north_star.return"(%8) : (!north_star.ns_tensor<1x128xf32,1>) -> ()
}) : (tensor<1x128xf32>) -> tensor<1x128xf32>


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab15c0) {
      %21 = "north_star.tensor_to_ns_tensor"(%arg2) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'north_star.return' op must be the last operation in the parent block
'north_star.return' op must be the last operation in the parent block
mlir-asm-printer: 'func.func' failed to verify and will be printed in generic form
"func.func"() <{function_type = (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0>, sym_name = "main"}> ({
^bb0(%arg0: !north_star.ns_tensor<2x128xf32,0>):
  %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  %1 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>
  %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %3 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %7 = "builtin.unrealized_conversion_cast"(%6) : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %8 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %9 = "builtin.unrealized_conversion_cast"(%8) : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  %10 = "north_star.device_kernel"(%7) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg4: tensor<1x128xf32>):
    %31 = "north_star.tensor_to_ns_tensor"(%arg4) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %32 = "builtin.unrealized_conversion_cast"(%31) : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %33 = "tensor.empty"() : () -> tensor<1x128xf32>
    %34 = "linalg.softmax"(%32, %33) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %35 = "north_star.softmax"(%31) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %36 = "tensor.empty"() : () -> tensor<1x128xf32>
    %37 = "linalg.softmax"(%34, %36) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %38 = "north_star.softmax"(%35) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    "north_star.return"(%37) : (tensor<1x128xf32>) -> ()
    "north_star.return"(%38) : (!north_star.ns_tensor<1x128xf32,0>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %11 = "north_star.device_kernel"(%6) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg3: !north_star.ns_tensor<1x128xf32,0>):
    %29 = "north_star.softmax"(%arg3) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    %30 = "north_star.softmax"(%29) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
    "north_star.return"(%30) : (!north_star.ns_tensor<1x128xf32,0>) -> ()
  }) : (!north_star.ns_tensor<1x128xf32,0>) -> !north_star.ns_tensor<1x128xf32,0>
  %12 = "north_star.device_kernel"(%9) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg2: tensor<1x128xf32>):
    %21 = "north_star.tensor_to_ns_tensor"(%arg2) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %22 = "builtin.unrealized_conversion_cast"(%21) : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %23 = "tensor.empty"() : () -> tensor<1x128xf32>
    %24 = "linalg.softmax"(%22, %23) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %25 = "north_star.softmax"(%21) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %26 = "tensor.empty"() : () -> tensor<1x128xf32>
    %27 = "linalg.softmax"(%24, %26) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %28 = "north_star.softmax"(%25) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    "north_star.return"(%27) : (tensor<1x128xf32>) -> ()
    "north_star.return"(%28) : (!north_star.ns_tensor<1x128xf32,1>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %13 = "north_star.device_kernel"(%8) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<1x128xf32,1>):
    %19 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    %20 = "north_star.softmax"(%19) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
    "north_star.return"(%20) : (!north_star.ns_tensor<1x128xf32,1>) -> ()
  }) : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>
  %14 = "north_star.buffer"(%11, %13) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %15 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>
  %16 = "north_star.tensor_to_ns_tensor"(%15) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %17 = "north_star.buffer"(%16) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%14, %17) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %18 = "north_star.get_tensor"(%17) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  "func.return"(%18) : (!north_star.ns_tensor<2x128xf32,0>) -> ()
}) {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} : () -> ()


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.softmax'(0x5c9590aae570) {
  %19 = "north_star.softmax"(%arg1) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.softmax'(0x5c9590aae600) {
  %20 = "north_star.softmax"(%19) <{axis = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.return'(0x5c9590aae680) {
  "north_star.return"(%20) : (!north_star.ns_tensor<1x128xf32,1>) -> ()

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5c9590aaf710) {
  %14 = "north_star.buffer"(%11, %13) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590aae710) {
  %15 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590a921e0) {
  %16 = "north_star.tensor_to_ns_tensor"(%15) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5c9590aa6380) {
  %17 = "north_star.buffer"(%16) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.gather'(0x5c9590a84f40) {
  "north_star.gather"(%14, %17) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.get_tensor'(0x5c9590a0f8a0) {
  %18 = "north_star.get_tensor"(%17) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x5c9590a0ffc0) {
  "func.return"(%18) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::north_star::detail::NSTensorToTensorOpGenericAdaptorBase::Properties)
** Insert  : 'north_star.ns_tensor_to_tensor'(0x5c9590ab3900)
** Insert  : 'north_star.ns_tensor_to_tensor'(0x5c9590ab4360)
** Insert  : 'north_star.ns_tensor_to_tensor'(0x5c9590ab3990)
** Insert  : 'north_star.ns_tensor_to_tensor'(0x5c9590ab3820)
** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590ab4700)
** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590ab47c0)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestinationStyleOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::DestinationStyleOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::AggregatedOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::linalg::AggregatedOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TilingInterface::Trait<mlir::TypeID::get() [with Trait = mlir::TilingInterface::Trait]::Empty>)

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590a0ffc0) {
  "func.return"(%18) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590a0f8a0) {
  %18 = "north_star.get_tensor"(%17) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.gather'(0x5c9590a84f40) {
  "north_star.gather"(%14, %17) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aa6380) {
  %17 = "north_star.buffer"(%16) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590a921e0) {
  %16 = "north_star.tensor_to_ns_tensor"(%15) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590aae710) {
  %15 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aaf710) {
  %14 = "north_star.buffer"(%11, %13) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab47c0) {
  %13 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5c9590ab42d0) {
  "north_star.return"(%24) : (tensor<1x128xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.softmax'(0x5c9590ab4230) {
  %24 = "linalg.softmax"(%22, %23) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab41c0) {
  %23 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.softmax'(0x5c9590ab0f40) {
  %22 = "linalg.softmax"(%20, %21) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590a0eb30) {
  %21 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab3900) {
  %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5c9590ab11a0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab15c0) {
  %19 = "north_star.tensor_to_ns_tensor"(%arg1) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab4700) {
  %11 = "north_star.tensor_to_ns_tensor"(%10) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.return'(0x5c9590aad0c0) {
  "north_star.return"(%30) : (tensor<1x128xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.softmax'(0x5c9590ab3e70) {
  %30 = "linalg.softmax"(%28, %29) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab3e00) {
  %29 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.softmax'(0x5c9590ab3c80) {
  %28 = "linalg.softmax"(%26, %27) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab3c10) {
  %27 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab3990) {
  %26 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.device_kernel'(0x5c9590aa6940) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab0b60) {
  %25 = "north_star.tensor_to_ns_tensor"(%arg2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab4360) {
  %9 = "north_star.ns_tensor_to_tensor"(%8) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590aac000) {
  %8 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab3820) {
  %7 = "north_star.ns_tensor_to_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590aabf70) {
  %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.scatter'(0x5c9590a9a650) {
  "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aabec0) {
  %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aabe30) {
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590a9a760) {
  %3 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aae780) {
  %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590a9a430) {
  %1 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590a104b0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aa72d0) {
  %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//
run out: ConvertNorthStarToLinalgPass
// -----// IR Dump After ConvertNorthStarToLinalgPass (convert-north-satr-to-linalg) //----- //
module @NorthStar {
  func.func @main(%arg0: !north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0> attributes {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} {
    %0 = "north_star.buffer"(%arg0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    %1 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %2 = "north_star.tensor_to_ns_tensor"(%1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %3 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %5 = "north_star.buffer"(%2, %4) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%0, %5) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %6 = "north_star.get_tensor"(%5) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %7 = "north_star.ns_tensor_to_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %8 = "north_star.get_tensor"(%5) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %9 = "north_star.ns_tensor_to_tensor"(%8) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %10 = "north_star.device_kernel"(%7) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
    ^bb0(%arg1: tensor<1x128xf32>):
      %19 = "north_star.tensor_to_ns_tensor"(%arg1) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
      %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
      %21 = tensor.empty() : tensor<1x128xf32>
      %22 = linalg.softmax dimension(1) ins(%20 : tensor<1x128xf32>) outs(%21 : tensor<1x128xf32>) -> tensor<1x128xf32>
      %23 = tensor.empty() : tensor<1x128xf32>
      %24 = linalg.softmax dimension(1) ins(%22 : tensor<1x128xf32>) outs(%23 : tensor<1x128xf32>) -> tensor<1x128xf32>
      north_star.return %24 : tensor<1x128xf32>
    }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %11 = "north_star.tensor_to_ns_tensor"(%10) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %12 = "north_star.device_kernel"(%9) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
    ^bb0(%arg1: tensor<1x128xf32>):
      %19 = "north_star.tensor_to_ns_tensor"(%arg1) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
      %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
      %21 = tensor.empty() : tensor<1x128xf32>
      %22 = linalg.softmax dimension(1) ins(%20 : tensor<1x128xf32>) outs(%21 : tensor<1x128xf32>) -> tensor<1x128xf32>
      %23 = tensor.empty() : tensor<1x128xf32>
      %24 = linalg.softmax dimension(1) ins(%22 : tensor<1x128xf32>) outs(%23 : tensor<1x128xf32>) -> tensor<1x128xf32>
      north_star.return %24 : tensor<1x128xf32>
    }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %13 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %14 = "north_star.buffer"(%11, %13) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    %15 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %16 = "north_star.tensor_to_ns_tensor"(%15) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %17 = "north_star.buffer"(%16) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%14, %17) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %18 = "north_star.get_tensor"(%17) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    return %18 : !north_star.ns_tensor<2x128xf32,0>
  }
}


run in NorthStarLegalizePass
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::ConstantOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferIntRangeInterface::Trait<mlir::TypeID::get() [with Trait = mlir::InferIntRangeInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::InferTypeOpInterface::Trait]::Empty>)
(i64) -> ()
%c0_i64 = arith.constant 0 : i64
(i64) -> ()
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::CallOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolUserOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::SymbolUserOpInterface::Trait]::Empty>)
%c1_i64 = arith.constant 1 : i64
(i64) -> ()
%c0_i64_0 = arith.constant 0 : i64
(i64) -> ()
%c0_i64_0 = arith.constant 0 : i64
(i64) -> ()
%c1_i64_1 = arith.constant 1 : i64

//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x5c95909deb50) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x5c9590a104b0) {
  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'func.func -> ()' {
Trying to match "{anonymous}::FuncFuncOpRerewriterPattern"
    ** Insert  : 'func.func'(0x5c9590aaf2c0)
    ** Insert Block into : 'func.func'(0x5c9590aaf2c0)
    ** Insert  : 'north_star.buffer'(0x5c9590ab5090)
    ** Insert  : 'arith.constant'(0x5c9590aad010)
    ** Insert  : 'func.call'(0x5c9590aacc80)
    ** Insert  : 'tensor.empty'(0x5c9590aaccf0)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590aacd60)
    ** Insert  : 'arith.constant'(0x5c9590aacdd0)
    ** Insert  : 'func.call'(0x5c9590aafd00)
    ** Insert  : 'tensor.empty'(0x5c9590aafd70)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590aafde0)
    ** Insert  : 'north_star.buffer'(0x5c9590aafe50)
    ** Insert  : 'north_star.scatter'(0x5c9590aafeb0)
    ** Insert  : 'north_star.get_tensor'(0x5c9590aaff20)
    ** Insert  : 'north_star.ns_tensor_to_tensor'(0x5c9590aaff90)
    ** Insert  : 'north_star.get_tensor'(0x5c9590ab0000)
    ** Insert  : 'north_star.ns_tensor_to_tensor'(0x5c9590ab0070)
    ** Insert  : 'arith.constant'(0x5c9590ab00e0)
    ** Insert  : 'func.call'(0x5c9590ab0140)
    ** Insert  : 'north_star.device_kernel'(0x5c9590aaf240)
    ** Insert Block into : 'north_star.device_kernel'(0x5c9590aaf240)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590ab26b0)
    ** Insert  : 'north_star.ns_tensor_to_tensor'(0x5c9590ab2720)
    ** Insert  : 'tensor.empty'(0x5c9590ab2790)
    ** Insert  : 'linalg.softmax'(0x5c9590ab2800)
    ** Insert  : 'tensor.empty'(0x5c9590ab2870)
    ** Insert  : 'linalg.softmax'(0x5c9590ab28e0)
    ** Insert  : 'north_star.return'(0x5c9590ab2940)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590ab01b0)
    ** Insert  : 'arith.constant'(0x5c9590ab0220)
    ** Insert  : 'func.call'(0x5c9590ab0280)
    ** Insert  : 'north_star.device_kernel'(0x5c9590ab3b80)
    ** Insert Block into : 'north_star.device_kernel'(0x5c9590ab3b80)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590ab2e50)
    ** Insert  : 'north_star.ns_tensor_to_tensor'(0x5c9590ab2ec0)
    ** Insert  : 'tensor.empty'(0x5c9590ab2f30)
    ** Insert  : 'linalg.softmax'(0x5c9590ab2fa0)
    ** Insert  : 'tensor.empty'(0x5c9590ab3010)
    ** Insert  : 'linalg.softmax'(0x5c9590ab3080)
    ** Insert  : 'north_star.return'(0x5c9590ab30e0)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590ab02f0)
    ** Insert  : 'north_star.buffer'(0x5c9590ab0360)
    ** Insert  : 'arith.constant'(0x5c9590ab03d0)
    ** Insert  : 'func.call'(0x5c9590ab0430)
    ** Insert  : 'tensor.empty'(0x5c9590ab04a0)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590ab0510)
    ** Insert  : 'north_star.buffer'(0x5c9590ab0580)
    ** Insert  : 'north_star.gather'(0x5c9590ab05e0)
    ** Insert  : 'north_star.get_tensor'(0x5c9590ab0650)
    ** Insert  : 'func.return'(0x5c9590ab06b0)
    ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590ab5d00)
    ** Replace : 'func.func'(0x5c9590a104b0)
"{anonymous}::FuncFuncOpRerewriterPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.buffer'(0x5c9590ab5090) {
      %37 = "north_star.buffer"(%36) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.func'(0x5c9590aaf2c0) {
    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.buffer'(0x5c9590ab5090) {
      %37 = "north_star.buffer"(%36) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590aad010) {
      %38 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590aacc80) {
      "func.call"(%38) <{callee = @__NS__SetDevice}> : (i64) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5c9590aaccf0) {
      %39 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aacd60) {
      %40 = "north_star.tensor_to_ns_tensor"(%39) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590aacdd0) {
      %41 = "arith.constant"() <{value = 1 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590aafd00) {
      "func.call"(%41) <{callee = @__NS__SetDevice}> : (i64) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5c9590aafd70) {
      %42 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aafde0) {
      %43 = "north_star.tensor_to_ns_tensor"(%42) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.buffer'(0x5c9590aafe50) {
      %44 = "north_star.buffer"(%40, %43) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.scatter'(0x5c9590aafeb0) {
      "north_star.scatter"(%37, %44) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.get_tensor'(0x5c9590aaff20) {
      %45 = "north_star.get_tensor"(%44) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590aaff90) {
      %46 = "north_star.ns_tensor_to_tensor"(%45) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.get_tensor'(0x5c9590ab0000) {
      %47 = "north_star.get_tensor"(%44) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab0070) {
      %48 = "north_star.ns_tensor_to_tensor"(%47) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590ab00e0) {
      %49 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ab0140) {
      "func.call"(%49) <{callee = @__NS__SetDevice}> : (i64) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.device_kernel'(0x5c9590aaf240) {
      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.device_kernel -> ()' {
Trying to match "{anonymous}::DeviceKernelOpToFuncPattern"
(tensor<1x128xf32>) -> tensor<1x128xf32>
type of return operand 0 ('!north_star.ns_tensor<2x128xf32,0>') doesn't match function result type ('tensor<2x128xf32>') in function @main
mlir-asm-printer: 'builtin.module' failed to verify and will be printed in generic form
%46 = "north_star.ns_tensor_to_tensor"(%45) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
        ** Insert  : 'func.call'(0x5c9590ab9790)
        ** Insert Block into : 'func.func'(0x5c9590ab92f0)
        ** Insert  : 'north_star.tensor_to_ns_tensor'(0x5c9590ab5380)
        ** Insert  : 'north_star.ns_tensor_to_tensor'(0x5c9590ab7000)
        ** Insert  : 'tensor.empty'(0x5c9590ab7070)
        ** Insert  : 'linalg.softmax'(0x5c9590ab70e0)
        ** Insert  : 'tensor.empty'(0x5c9590ab7150)
        ** Insert  : 'linalg.softmax'(0x5c9590ab71c0)
        ** Insert  : 'north_star.return'(0x5c9590ab7220)
        ** Replace : 'north_star.device_kernel'(0x5c9590aaf240)
"{anonymous}::DeviceKernelOpToFuncPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'func.func'(0x5c9590ab92f0) {
        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'func.call'(0x5c9590ab9790) {
          %56 = "func.call"(%52) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab5380) {
          %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab7000) {
          %1 = "north_star.ns_tensor_to_tensor"(%0) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'tensor.empty'(0x5c9590ab7070) {
          %2 = "tensor.empty"() : () -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'linalg.softmax'(0x5c9590ab70e0) {
          %3 = "linalg.softmax"(%1, %2) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'tensor.empty'(0x5c9590ab7150) {
          %4 = "tensor.empty"() : () -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'linalg.softmax'(0x5c9590ab71c0) {
          %5 = "linalg.softmax"(%3, %4) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//

        //===-------------------------------------------===//
        Legalizing operation : 'north_star.return'(0x5c9590ab7220) {
          "north_star.return"(%5) : (tensor<1x128xf32>) -> ()

          * Fold {
          } -> FAILURE : unable to fold

          * Pattern : 'north_star.return -> ()' {
Trying to match "{anonymous}::ReturnOpToFuncPattern"
            ** Insert  : 'func.return'(0x5c9590ab8f20)
            ** Replace : 'north_star.return'(0x5c9590ab7220)
"{anonymous}::ReturnOpToFuncPattern" result 1

            //===-------------------------------------------===//
            Legalizing operation : 'func.return'(0x5c9590ab8f20) {
              "func.return"(%5) : (tensor<1x128xf32>) -> ()

            } -> SUCCESS : operation marked legal by the target
            //===-------------------------------------------===//
          } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'func.return' op must be the last operation in the parent block
mlir-asm-printer: 'func.func' failed to verify and will be printed in generic form
"func.func"() <{function_type = (tensor<1x128xf32>) -> tensor<1x128xf32>, sym_name = "softmax_1_128_softmax_1_128_fused_kernel", sym_visibility = "private"}> ({
^bb0(%arg0: tensor<1x128xf32>):
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %1 = "north_star.ns_tensor_to_tensor"(%0) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %2 = "tensor.empty"() : () -> tensor<1x128xf32>
  %3 = "linalg.softmax"(%1, %2) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
  %4 = "tensor.empty"() : () -> tensor<1x128xf32>
  %5 = "linalg.softmax"(%3, %4) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
  "func.return"(%5) : (tensor<1x128xf32>) -> ()
  "north_star.return"(%5) : (tensor<1x128xf32>) -> ()
}) {device_kernel} : () -> ()


        } -> SUCCESS
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
type of return operand 0 ('!north_star.ns_tensor<2x128xf32,0>') doesn't match function result type ('tensor<2x128xf32>') in function @main
mlir-asm-printer: 'func.func' failed to verify and will be printed in generic form
"func.func"() <{arg_attrs = [{func.device_id = 0 : i64}], function_type = (tensor<2x128xf32>) -> tensor<2x128xf32>, sym_name = "main"}> ({
^bb0(%arg0: tensor<2x128xf32>):
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  %2 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  "func.call"(%2) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %3 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %5 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  "func.call"(%5) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %6 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>
  %7 = "north_star.tensor_to_ns_tensor"(%6) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %8 = "north_star.buffer"(%4, %7) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%1, %8) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %9 = "north_star.get_tensor"(%8) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %11 = "north_star.get_tensor"(%8) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %12 = "north_star.ns_tensor_to_tensor"(%11) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  %13 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  "func.call"(%13) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %14 = "func.call"(%10) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %15 = "north_star.device_kernel"(%10) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg2: tensor<1x128xf32>):
    %32 = "north_star.tensor_to_ns_tensor"(%arg2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %33 = "north_star.ns_tensor_to_tensor"(%32) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %34 = "tensor.empty"() : () -> tensor<1x128xf32>
    %35 = "linalg.softmax"(%33, %34) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = "tensor.empty"() : () -> tensor<1x128xf32>
    %37 = "linalg.softmax"(%35, %36) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%37) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %16 = "north_star.tensor_to_ns_tensor"(%15) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %17 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  "func.call"(%17) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %18 = "north_star.device_kernel"(%12) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: tensor<1x128xf32>):
    %26 = "north_star.tensor_to_ns_tensor"(%arg1) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %27 = "north_star.ns_tensor_to_tensor"(%26) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %28 = "tensor.empty"() : () -> tensor<1x128xf32>
    %29 = "linalg.softmax"(%27, %28) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %30 = "tensor.empty"() : () -> tensor<1x128xf32>
    %31 = "linalg.softmax"(%29, %30) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%31) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %19 = "north_star.tensor_to_ns_tensor"(%18) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %20 = "north_star.buffer"(%16, %19) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %21 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  "func.call"(%21) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %22 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>
  %23 = "north_star.tensor_to_ns_tensor"(%22) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %24 = "north_star.buffer"(%23) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%20, %24) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %25 = "north_star.get_tensor"(%24) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  "func.return"(%25) : (!north_star.ns_tensor<2x128xf32,0>) -> ()
}) : () -> ()


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab26b0) {
      %74 = "north_star.tensor_to_ns_tensor"(%arg6) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab2720) {
      %75 = "north_star.ns_tensor_to_tensor"(%74) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5c9590ab2790) {
      %76 = "tensor.empty"() : () -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.softmax'(0x5c9590ab2800) {
      %77 = "linalg.softmax"(%75, %76) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5c9590ab2870) {
      %78 = "tensor.empty"() : () -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.softmax'(0x5c9590ab28e0) {
      %79 = "linalg.softmax"(%77, %78) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.return'(0x5c9590ab2940) {
      "north_star.return"(%79) : (tensor<1x128xf32>) -> ()

    } -> SUCCESS : operation marked 'ignored' during conversion
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab01b0) {
      %58 = "north_star.tensor_to_ns_tensor"(%57) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590ab0220) {
      %59 = "arith.constant"() <{value = 1 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ab0280) {
      "func.call"(%59) <{callee = @__NS__SetDevice}> : (i64) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.device_kernel'(0x5c9590ab3b80) {
      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'north_star.device_kernel -> ()' {
Trying to match "{anonymous}::DeviceKernelOpToFuncPattern"
(tensor<1x128xf32>) -> tensor<1x128xf32>
type of return operand 0 ('!north_star.ns_tensor<2x128xf32,0>') doesn't match function result type ('tensor<2x128xf32>') in function @main
'func.return' op must be the last operation in the parent block
mlir-asm-printer: 'builtin.module' failed to verify and will be printed in generic form
%54 = "north_star.ns_tensor_to_tensor"(%53) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
        ** Insert  : 'func.call'(0x5c9590ab78a0)
        ** Replace : 'north_star.device_kernel'(0x5c9590ab3b80)
"{anonymous}::DeviceKernelOpToFuncPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'func.call'(0x5c9590ab78a0) {
          %60 = "func.call"(%54) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
type of return operand 0 ('!north_star.ns_tensor<2x128xf32,0>') doesn't match function result type ('tensor<2x128xf32>') in function @main
mlir-asm-printer: 'func.func' failed to verify and will be printed in generic form
"func.func"() <{arg_attrs = [{func.device_id = 0 : i64}], function_type = (tensor<2x128xf32>) -> tensor<2x128xf32>, sym_name = "main"}> ({
^bb0(%arg0: tensor<2x128xf32>):
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  %2 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  "func.call"(%2) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %3 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %5 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  "func.call"(%5) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %6 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>
  %7 = "north_star.tensor_to_ns_tensor"(%6) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %8 = "north_star.buffer"(%4, %7) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%1, %8) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %9 = "north_star.get_tensor"(%8) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %11 = "north_star.get_tensor"(%8) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %12 = "north_star.ns_tensor_to_tensor"(%11) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  %13 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  "func.call"(%13) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %14 = "func.call"(%10) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %15 = "north_star.device_kernel"(%10) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg2: tensor<1x128xf32>):
    %33 = "north_star.tensor_to_ns_tensor"(%arg2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %34 = "north_star.ns_tensor_to_tensor"(%33) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %35 = "tensor.empty"() : () -> tensor<1x128xf32>
    %36 = "linalg.softmax"(%34, %35) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %37 = "tensor.empty"() : () -> tensor<1x128xf32>
    %38 = "linalg.softmax"(%36, %37) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%38) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %16 = "north_star.tensor_to_ns_tensor"(%15) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %17 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  "func.call"(%17) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %18 = "func.call"(%12) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %19 = "north_star.device_kernel"(%12) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: tensor<1x128xf32>):
    %27 = "north_star.tensor_to_ns_tensor"(%arg1) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %28 = "north_star.ns_tensor_to_tensor"(%27) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %29 = "tensor.empty"() : () -> tensor<1x128xf32>
    %30 = "linalg.softmax"(%28, %29) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = "tensor.empty"() : () -> tensor<1x128xf32>
    %32 = "linalg.softmax"(%30, %31) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%32) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %20 = "north_star.tensor_to_ns_tensor"(%19) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %21 = "north_star.buffer"(%16, %20) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %22 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  "func.call"(%22) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %23 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>
  %24 = "north_star.tensor_to_ns_tensor"(%23) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %25 = "north_star.buffer"(%24) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%21, %25) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %26 = "north_star.get_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  "func.return"(%26) : (!north_star.ns_tensor<2x128xf32,0>) -> ()
}) : () -> ()


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab2e50) {
      %69 = "north_star.tensor_to_ns_tensor"(%arg5) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab2ec0) {
      %70 = "north_star.ns_tensor_to_tensor"(%69) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5c9590ab2f30) {
      %71 = "tensor.empty"() : () -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.softmax'(0x5c9590ab2fa0) {
      %72 = "linalg.softmax"(%70, %71) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5c9590ab3010) {
      %73 = "tensor.empty"() : () -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'linalg.softmax'(0x5c9590ab3080) {
      %74 = "linalg.softmax"(%72, %73) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.return'(0x5c9590ab30e0) {
      "north_star.return"(%74) : (tensor<1x128xf32>) -> ()

    } -> SUCCESS : operation marked 'ignored' during conversion
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab02f0) {
      %62 = "north_star.tensor_to_ns_tensor"(%61) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.buffer'(0x5c9590ab0360) {
      %63 = "north_star.buffer"(%58, %62) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590ab03d0) {
      %64 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ab0430) {
      "func.call"(%64) <{callee = @__NS__SetDevice}> : (i64) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.empty'(0x5c9590ab04a0) {
      %65 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab0510) {
      %66 = "north_star.tensor_to_ns_tensor"(%65) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.buffer'(0x5c9590ab0580) {
      %67 = "north_star.buffer"(%66) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.gather'(0x5c9590ab05e0) {
      "north_star.gather"(%63, %67) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.get_tensor'(0x5c9590ab0650) {
      %68 = "north_star.get_tensor"(%67) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.return'(0x5c9590ab06b0) {
      "func.return"(%68) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

      * Fold {
      } -> FAILURE : unable to fold

      * Pattern : 'func.return -> ()' {
Trying to match "{anonymous}::FuncReturnOpRerewriterPattern"
        ** Insert  : 'func.return'(0x5c9590ab8050)
        ** Replace : 'func.return'(0x5c9590ab06b0)
"{anonymous}::FuncReturnOpRerewriterPattern" result 1

        //===-------------------------------------------===//
        Legalizing operation : 'func.return'(0x5c9590ab8050) {
          "func.return"(%69) : (tensor<2x128xf32>) -> ()

        } -> SUCCESS : operation marked legal by the target
        //===-------------------------------------------===//
      } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'func.return' op must be the last operation in the parent block
mlir-asm-printer: 'func.func' failed to verify and will be printed in generic form
"func.func"() <{arg_attrs = [{func.device_id = 0 : i64}], function_type = (tensor<2x128xf32>) -> tensor<2x128xf32>, sym_name = "main"}> ({
^bb0(%arg0: tensor<2x128xf32>):
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  %2 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  "func.call"(%2) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %3 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %5 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  "func.call"(%5) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %6 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>
  %7 = "north_star.tensor_to_ns_tensor"(%6) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %8 = "north_star.buffer"(%4, %7) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%1, %8) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %9 = "north_star.get_tensor"(%8) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %11 = "north_star.get_tensor"(%8) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %12 = "north_star.ns_tensor_to_tensor"(%11) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  %13 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  "func.call"(%13) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %14 = "func.call"(%10) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %15 = "north_star.device_kernel"(%10) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg2: tensor<1x128xf32>):
    %34 = "north_star.tensor_to_ns_tensor"(%arg2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %35 = "north_star.ns_tensor_to_tensor"(%34) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %36 = "tensor.empty"() : () -> tensor<1x128xf32>
    %37 = "linalg.softmax"(%35, %36) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %38 = "tensor.empty"() : () -> tensor<1x128xf32>
    %39 = "linalg.softmax"(%37, %38) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%39) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %16 = "north_star.tensor_to_ns_tensor"(%15) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %17 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  "func.call"(%17) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %18 = "func.call"(%12) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %19 = "north_star.device_kernel"(%12) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
  ^bb0(%arg1: tensor<1x128xf32>):
    %28 = "north_star.tensor_to_ns_tensor"(%arg1) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %29 = "north_star.ns_tensor_to_tensor"(%28) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %30 = "tensor.empty"() : () -> tensor<1x128xf32>
    %31 = "linalg.softmax"(%29, %30) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %32 = "tensor.empty"() : () -> tensor<1x128xf32>
    %33 = "linalg.softmax"(%31, %32) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "north_star.return"(%33) : (tensor<1x128xf32>) -> ()
  }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %20 = "north_star.tensor_to_ns_tensor"(%19) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %21 = "north_star.buffer"(%16, %20) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  %22 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  "func.call"(%22) <{callee = @__NS__SetDevice}> : (i64) -> ()
  %23 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>
  %24 = "north_star.tensor_to_ns_tensor"(%23) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %25 = "north_star.buffer"(%24) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%21, %25) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %26 = "north_star.get_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %27 = "builtin.unrealized_conversion_cast"(%26) : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  "func.return"(%27) : (tensor<2x128xf32>) -> ()
  "func.return"(%26) : (!north_star.ns_tensor<2x128xf32,0>) -> ()
}) : () -> ()


    } -> SUCCESS
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab5d00) {
      %42 = "north_star.tensor_to_ns_tensor"(%arg4) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
'func.return' op must be the last operation in the parent block
'func.return' op must be the last operation in the parent block
mlir-asm-printer: 'builtin.module' failed to verify and will be printed in generic form
"builtin.module"() <{sym_name = "NorthStar"}> ({
  "func.func"() <{arg_attrs = [{func.device_id = 0 : i64}], function_type = (tensor<2x128xf32>) -> tensor<2x128xf32>, sym_name = "main"}> ({
  ^bb0(%arg4: tensor<2x128xf32>):
    %42 = "north_star.tensor_to_ns_tensor"(%arg4) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %43 = "north_star.buffer"(%42) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    %44 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "func.call"(%44) <{callee = @__NS__SetDevice}> : (i64) -> ()
    %45 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>
    %46 = "north_star.tensor_to_ns_tensor"(%45) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %47 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    "func.call"(%47) <{callee = @__NS__SetDevice}> : (i64) -> ()
    %48 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>
    %49 = "north_star.tensor_to_ns_tensor"(%48) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %50 = "north_star.buffer"(%46, %49) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%43, %50) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %51 = "north_star.get_tensor"(%50) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %52 = "north_star.ns_tensor_to_tensor"(%51) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %53 = "north_star.get_tensor"(%50) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %54 = "north_star.ns_tensor_to_tensor"(%53) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %55 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "func.call"(%55) <{callee = @__NS__SetDevice}> : (i64) -> ()
    %56 = "func.call"(%52) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %57 = "north_star.device_kernel"(%52) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
    ^bb0(%arg6: tensor<1x128xf32>):
      %76 = "north_star.tensor_to_ns_tensor"(%arg6) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
      %77 = "north_star.ns_tensor_to_tensor"(%76) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
      %78 = "tensor.empty"() : () -> tensor<1x128xf32>
      %79 = "linalg.softmax"(%77, %78) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
      %80 = "tensor.empty"() : () -> tensor<1x128xf32>
      %81 = "linalg.softmax"(%79, %80) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
      "north_star.return"(%81) : (tensor<1x128xf32>) -> ()
    }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %58 = "north_star.tensor_to_ns_tensor"(%57) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %59 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    "func.call"(%59) <{callee = @__NS__SetDevice}> : (i64) -> ()
    %60 = "func.call"(%54) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %61 = "north_star.device_kernel"(%54) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
    ^bb0(%arg5: tensor<1x128xf32>):
      %70 = "north_star.tensor_to_ns_tensor"(%arg5) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
      %71 = "north_star.ns_tensor_to_tensor"(%70) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
      %72 = "tensor.empty"() : () -> tensor<1x128xf32>
      %73 = "linalg.softmax"(%71, %72) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
      %74 = "tensor.empty"() : () -> tensor<1x128xf32>
      %75 = "linalg.softmax"(%73, %74) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
      "north_star.return"(%75) : (tensor<1x128xf32>) -> ()
    }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %62 = "north_star.tensor_to_ns_tensor"(%61) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %63 = "north_star.buffer"(%58, %62) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    %64 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "func.call"(%64) <{callee = @__NS__SetDevice}> : (i64) -> ()
    %65 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>
    %66 = "north_star.tensor_to_ns_tensor"(%65) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %67 = "north_star.buffer"(%66) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%63, %67) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %68 = "north_star.get_tensor"(%67) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %69 = "builtin.unrealized_conversion_cast"(%68) : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    "func.return"(%69) : (tensor<2x128xf32>) -> ()
    "func.return"(%68) : (!north_star.ns_tensor<2x128xf32,0>) -> ()
  }) : () -> ()
  "func.func"() <{function_type = (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.ns_tensor<2x128xf32,0>, sym_name = "main"}> ({
  ^bb0(%arg1: !north_star.ns_tensor<2x128xf32,0>):
    %6 = "north_star.buffer"(%arg1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    %7 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "func.call"(%7) <{callee = @__NS__SetDevice}> : (i64) -> ()
    %8 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>
    %9 = "north_star.tensor_to_ns_tensor"(%8) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %10 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    "func.call"(%10) <{callee = @__NS__SetDevice}> : (i64) -> ()
    %11 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %13 = "north_star.buffer"(%9, %12) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%6, %13) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %14 = "north_star.get_tensor"(%13) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %15 = "north_star.ns_tensor_to_tensor"(%14) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %16 = "north_star.get_tensor"(%13) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %17 = "north_star.ns_tensor_to_tensor"(%16) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %18 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "func.call"(%18) <{callee = @__NS__SetDevice}> : (i64) -> ()
    %19 = "north_star.device_kernel"(%15) <{device_id = 0 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
    ^bb0(%arg3: tensor<1x128xf32>):
      %36 = "north_star.tensor_to_ns_tensor"(%arg3) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
      %37 = "north_star.ns_tensor_to_tensor"(%36) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
      %38 = "tensor.empty"() : () -> tensor<1x128xf32>
      %39 = "linalg.softmax"(%37, %38) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
      %40 = "tensor.empty"() : () -> tensor<1x128xf32>
      %41 = "linalg.softmax"(%39, %40) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
      "north_star.return"(%41) : (tensor<1x128xf32>) -> ()
    }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %20 = "north_star.tensor_to_ns_tensor"(%19) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %21 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    "func.call"(%21) <{callee = @__NS__SetDevice}> : (i64) -> ()
    %22 = "north_star.device_kernel"(%17) <{device_id = 1 : i64, sym_name = "softmax_1_128_softmax_1_128_fused_kernel"}> ({
    ^bb0(%arg2: tensor<1x128xf32>):
      %30 = "north_star.tensor_to_ns_tensor"(%arg2) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
      %31 = "north_star.ns_tensor_to_tensor"(%30) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
      %32 = "tensor.empty"() : () -> tensor<1x128xf32>
      %33 = "linalg.softmax"(%31, %32) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
      %34 = "tensor.empty"() : () -> tensor<1x128xf32>
      %35 = "linalg.softmax"(%33, %34) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
      "north_star.return"(%35) : (tensor<1x128xf32>) -> ()
    }) : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %23 = "north_star.tensor_to_ns_tensor"(%22) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %24 = "north_star.buffer"(%20, %23) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    %25 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "func.call"(%25) <{callee = @__NS__SetDevice}> : (i64) -> ()
    %26 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>
    %27 = "north_star.tensor_to_ns_tensor"(%26) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %28 = "north_star.buffer"(%27) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%24, %28) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %29 = "north_star.get_tensor"(%28) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    "func.return"(%29) : (!north_star.ns_tensor<2x128xf32,0>) -> ()
  }) {dp_attr = #north_star.DP<DP = 2 : 0, 1>, host_func} : () -> ()
  "func.func"() <{function_type = (i64) -> (), sym_name = "__NS__SetDevice", sym_visibility = "private"}> ({
  }) : () -> ()
  "func.func"() <{function_type = (tensor<1x128xf32>) -> tensor<1x128xf32>, sym_name = "softmax_1_128_softmax_1_128_fused_kernel", sym_visibility = "private"}> ({
  ^bb0(%arg0: tensor<1x128xf32>):
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %1 = "north_star.ns_tensor_to_tensor"(%0) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %2 = "tensor.empty"() : () -> tensor<1x128xf32>
    %3 = "linalg.softmax"(%1, %2) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %4 = "tensor.empty"() : () -> tensor<1x128xf32>
    %5 = "linalg.softmax"(%3, %4) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    "func.return"(%5) : (tensor<1x128xf32>) -> ()
    "north_star.return"(%5) : (tensor<1x128xf32>) -> ()
  }) {device_kernel} : () -> ()
}) : () -> ()


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5c9590aa72d0) {
  %6 = "north_star.buffer"(%arg1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5c9590a9fb30) {
  %7 = "arith.constant"() <{value = 0 : i64}> : () -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x5c9590ab1670) {
  "func.call"(%7) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590a9a430) {
  %8 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aae780) {
  %9 = "north_star.tensor_to_ns_tensor"(%8) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5c9590aabdc0) {
  %10 = "arith.constant"() <{value = 1 : i64}> : () -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x5c9590aae680) {
  "func.call"(%10) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590a9a760) {
  %11 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aabe30) {
  %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5c9590aabec0) {
  %13 = "north_star.buffer"(%9, %12) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.scatter'(0x5c9590a9a650) {
  "north_star.scatter"(%6, %13) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.get_tensor'(0x5c9590aabf70) {
  %14 = "north_star.get_tensor"(%13) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab3820) {
  %15 = "north_star.ns_tensor_to_tensor"(%14) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.get_tensor'(0x5c9590aac000) {
  %16 = "north_star.get_tensor"(%13) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab4360) {
  %17 = "north_star.ns_tensor_to_tensor"(%16) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5c9590ab0c20) {
  %18 = "arith.constant"() <{value = 0 : i64}> : () -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x5c9590ab1530) {
  "func.call"(%18) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.device_kernel'(0x5c9590aa6940) {
} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab0b60) {
  %36 = "north_star.tensor_to_ns_tensor"(%arg3) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab3990) {
  %37 = "north_star.ns_tensor_to_tensor"(%36) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590ab3c10) {
  %38 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.softmax'(0x5c9590ab3c80) {
  %39 = "linalg.softmax"(%37, %38) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590ab3e00) {
  %40 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.softmax'(0x5c9590ab3e70) {
  %41 = "linalg.softmax"(%39, %40) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.return'(0x5c9590aad0c0) {
  "north_star.return"(%41) : (tensor<1x128xf32>) -> ()

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab4700) {
  %20 = "north_star.tensor_to_ns_tensor"(%19) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5c9590aaf170) {
  %21 = "arith.constant"() <{value = 1 : i64}> : () -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x5c9590ab3d20) {
  "func.call"(%21) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.device_kernel'(0x5c9590ab11a0) {
} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab15c0) {
  %30 = "north_star.tensor_to_ns_tensor"(%arg2) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab3900) {
  %31 = "north_star.ns_tensor_to_tensor"(%30) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590a0eb30) {
  %32 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.softmax'(0x5c9590ab0f40) {
  %33 = "linalg.softmax"(%31, %32) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590ab41c0) {
  %34 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.softmax'(0x5c9590ab4230) {
  %35 = "linalg.softmax"(%33, %34) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.return'(0x5c9590ab42d0) {
  "north_star.return"(%35) : (tensor<1x128xf32>) -> ()

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab47c0) {
  %23 = "north_star.tensor_to_ns_tensor"(%22) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5c9590aaf710) {
  %24 = "north_star.buffer"(%20, %23) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5c9590ab1280) {
  %25 = "arith.constant"() <{value = 0 : i64}> : () -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x5c9590aae360) {
  "func.call"(%25) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590aae710) {
  %26 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590a921e0) {
  %27 = "north_star.tensor_to_ns_tensor"(%26) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5c9590aa6380) {
  %28 = "north_star.buffer"(%27) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.gather'(0x5c9590a84f40) {
  "north_star.gather"(%24, %28) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.get_tensor'(0x5c9590a0f8a0) {
  %29 = "north_star.get_tensor"(%28) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x5c9590a0ffc0) {
  "func.return"(%29) : (!north_star.ns_tensor<2x128xf32,0>) -> ()

} -> SUCCESS : operation marked 'ignored' during conversion
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x5c9590aaf350) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
** Insert  : 'north_star.ns_tensor_to_tensor'(0x5c9590ab80e0)
run out: NorthStarLegalizePass
// -----// IR Dump After NorthStarLegalizePass (north-star-legalize) //----- //
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    %c0_i64 = arith.constant 0 : i64
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %c1_i64 = arith.constant 1 : i64
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    %c0_i64_0 = arith.constant 0 : i64
    call @__NS__SetDevice(%c0_i64_0) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %c1_i64_1 = arith.constant 1 : i64
    call @__NS__SetDevice(%c1_i64_1) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    %c0_i64_2 = arith.constant 0 : i64
    call @__NS__SetDevice(%c0_i64_2) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    %1 = "north_star.ns_tensor_to_tensor"(%0) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %4 = tensor.empty() : tensor<1x128xf32>
    %5 = linalg.softmax dimension(1) ins(%3 : tensor<1x128xf32>) outs(%4 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %5 : tensor<1x128xf32>
  }
}


** Replace : 'arith.constant'(0x5c9590ab00e0)
** Modified: 'func.call'(0x5c9590ab0140)
** Erase   : 'arith.constant'(0x5c9590ab00e0)
** Replace : 'arith.constant'(0x5c9590ab0220)
** Modified: 'func.call'(0x5c9590ab0280)
** Erase   : 'arith.constant'(0x5c9590ab0220)
** Replace : 'arith.constant'(0x5c9590ab03d0)
** Modified: 'func.call'(0x5c9590ab0430)
** Erase   : 'arith.constant'(0x5c9590ab03d0)

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590aaf2c0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab5d00) {
  %8 = "north_star.tensor_to_ns_tensor"(%arg1) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate"
"mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590ab5090) {
  %9 = "north_star.buffer"(%8) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5c9590aad010) {
  %7 = "arith.constant"() <{value = 0 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590aacc80) {
  "func.call"(%7) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590aaccf0) {
  %10 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aacd60) {
  %11 = "north_star.tensor_to_ns_tensor"(%10) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate"
"mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5c9590aacdd0) {
  %6 = "arith.constant"() <{value = 1 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590aafd00) {
  "func.call"(%6) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590aafd70) {
  %12 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aafde0) {
  %13 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>


  * Pattern mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate"
"mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aafe50) {
  %14 = "north_star.buffer"(%11, %13) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.scatter'(0x5c9590aafeb0) {
  "north_star.scatter"(%9, %14) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590aaff20) {
  %15 = "north_star.get_tensor"(%14) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590aaff90) {
  %16 = "north_star.ns_tensor_to_tensor"(%15) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>


  * Pattern mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate : 'north_star.ns_tensor_to_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate"
"mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590ab0000) {
  %17 = "north_star.get_tensor"(%14) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab0070) {
  %18 = "north_star.ns_tensor_to_tensor"(%17) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>


  * Pattern mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate : 'north_star.ns_tensor_to_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate"
"mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab0140) {
  "func.call"(%7) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab9790) {
  %19 = "func.call"(%16) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab01b0) {
  %20 = "north_star.tensor_to_ns_tensor"(%19) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate"
"mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab0280) {
  "func.call"(%6) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab78a0) {
  %21 = "func.call"(%18) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab02f0) {
  %22 = "north_star.tensor_to_ns_tensor"(%21) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>


  * Pattern mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate"
"mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590ab0360) {
  %23 = "north_star.buffer"(%20, %22) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab0430) {
  "func.call"(%7) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab04a0) {
  %24 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab0510) {
  %25 = "north_star.tensor_to_ns_tensor"(%24) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate"
"mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590ab0580) {
  %26 = "north_star.buffer"(%25) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.gather'(0x5c9590ab05e0) {
  "north_star.gather"(%23, %26) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590ab0650) {
  %27 = "north_star.get_tensor"(%26) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab80e0) {
  %28 = "north_star.ns_tensor_to_tensor"(%27) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>


  * Pattern mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate : 'north_star.ns_tensor_to_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate"
"mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590ab8050) {
  "func.return"(%28) : (tensor<2x128xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590aaf350) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590ab92f0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab5380) {
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate"
"mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab7000) {
  %1 = "north_star.ns_tensor_to_tensor"(%0) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>


  * Pattern mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate : 'north_star.ns_tensor_to_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate"
    ** Replace : 'north_star.ns_tensor_to_tensor'(0x5c9590ab7000)
    ** Modified: 'linalg.softmax'(0x5c9590ab70e0)
    ** Erase   : 'north_star.ns_tensor_to_tensor'(0x5c9590ab7000)
"mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  %1 = tensor.empty() : tensor<1x128xf32>
  %2 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%1 : tensor<1x128xf32>) -> tensor<1x128xf32>
  %3 = tensor.empty() : tensor<1x128xf32>
  %4 = linalg.softmax dimension(1) ins(%2 : tensor<1x128xf32>) outs(%3 : tensor<1x128xf32>) -> tensor<1x128xf32>
  return %4 : tensor<1x128xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab5380) {
  %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

  ** Erase   : 'north_star.tensor_to_ns_tensor'(0x5c9590ab5380)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590ab92f0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab7070) {
  %0 = "tensor.empty"() : () -> tensor<1x128xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.softmax'(0x5c9590ab70e0) {
  %1 = "linalg.softmax"(%arg0, %0) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>


  * Pattern FoldTensorCastProducerOp : 'linalg.softmax -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab7150) {
  %2 = "tensor.empty"() : () -> tensor<1x128xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.softmax'(0x5c9590ab71c0) {
  %3 = "linalg.softmax"(%1, %2) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>


  * Pattern FoldTensorCastProducerOp : 'linalg.softmax -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590ab8f20) {
  "func.return"(%3) : (tensor<1x128xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590aaf2c0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5c9590aacdd0) {
  %4 = "arith.constant"() <{value = 1 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5c9590aad010) {
  %5 = "arith.constant"() <{value = 0 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab5d00) {
  %6 = "north_star.tensor_to_ns_tensor"(%arg1) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate"
"mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590ab5090) {
  %7 = "north_star.buffer"(%6) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590aacc80) {
  "func.call"(%5) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590aaccf0) {
  %8 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aacd60) {
  %9 = "north_star.tensor_to_ns_tensor"(%8) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate"
"mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590aafd00) {
  "func.call"(%4) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590aafd70) {
  %10 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aafde0) {
  %11 = "north_star.tensor_to_ns_tensor"(%10) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>


  * Pattern mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate"
"mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590aafe50) {
  %12 = "north_star.buffer"(%9, %11) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.scatter'(0x5c9590aafeb0) {
  "north_star.scatter"(%7, %12) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590aaff20) {
  %13 = "north_star.get_tensor"(%12) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590aaff90) {
  %14 = "north_star.ns_tensor_to_tensor"(%13) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>


  * Pattern mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate : 'north_star.ns_tensor_to_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate"
"mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590ab0000) {
  %15 = "north_star.get_tensor"(%12) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab0070) {
  %16 = "north_star.ns_tensor_to_tensor"(%15) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>


  * Pattern mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate : 'north_star.ns_tensor_to_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate"
"mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab0140) {
  "func.call"(%5) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab9790) {
  %17 = "func.call"(%14) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab01b0) {
  %18 = "north_star.tensor_to_ns_tensor"(%17) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate"
"mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab0280) {
  "func.call"(%4) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab78a0) {
  %19 = "func.call"(%16) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab02f0) {
  %20 = "north_star.tensor_to_ns_tensor"(%19) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>


  * Pattern mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate"
"mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590ab0360) {
  %21 = "north_star.buffer"(%18, %20) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab0430) {
  "func.call"(%5) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab04a0) {
  %22 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab0510) {
  %23 = "north_star.tensor_to_ns_tensor"(%22) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>


  * Pattern mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate"
"mlir::north_star::{anonymous}::TensorToNSTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.buffer'(0x5c9590ab0580) {
  %24 = "north_star.buffer"(%23) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.gather'(0x5c9590ab05e0) {
  "north_star.gather"(%21, %24) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.get_tensor'(0x5c9590ab0650) {
  %25 = "north_star.get_tensor"(%24) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab80e0) {
  %26 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>


  * Pattern mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate : 'north_star.ns_tensor_to_tensor -> ()' {
Trying to match "mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate"
"mlir::north_star::{anonymous}::NSTensorToTensorOpEliminate" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590ab8050) {
  "func.return"(%26) : (tensor<2x128xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590aaf350) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590ab92f0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab7070) {
  %0 = "tensor.empty"() : () -> tensor<1x128xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.softmax'(0x5c9590ab70e0) {
  %1 = "linalg.softmax"(%arg0, %0) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>


  * Pattern FoldTensorCastProducerOp : 'linalg.softmax -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab7150) {
  %2 = "tensor.empty"() : () -> tensor<1x128xf32>


  * Pattern {anonymous}::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "{anonymous}::ReplaceEmptyTensorStaticShapeDims"
"{anonymous}::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.softmax'(0x5c9590ab71c0) {
  %3 = "linalg.softmax"(%1, %2) <{dimension = 1 : i64}> : (tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>


  * Pattern FoldTensorCastProducerOp : 'linalg.softmax -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590ab8f20) {
  "func.return"(%3) : (tensor<1x128xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %3 : tensor<1x128xf32>
  }
}


[transform-dialect-interpreter-utils]: Adding '/home/lfr/MLIR_Tutorial/linalg_include.mlir' to list of files
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::TransformOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::transform::TransformOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::transform::TransformHandleTypeInterface>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneTypedResult<mlir::transform::TransformHandleTypeInterface>::Impl]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::NavigationTransformOpTrait<mlir::TypeID::get() [with Trait = mlir::transform::NavigationTransformOpTrait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::detail::MatchOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::FunctionalStyleTransformOpTrait<mlir::TypeID::get() [with Trait = mlir::transform::FunctionalStyleTransformOpTrait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::TransformEachOpTrait<mlir::TypeID::get() [with Trait = mlir::transform::TransformEachOpTrait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::ReportTrackingListenerFailuresOpTrait<mlir::TypeID::get() [with Trait = mlir::transform::ReportTrackingListenerFailuresOpTrait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::detail::ApplyRegisteredPassOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::detail::OneShotBufferizeOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::PatternDescriptorOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::transform::PatternDescriptorOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::MatchOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::transform::MatchOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::detail::IncludeOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::TransformMappingResource)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffects::Read)
ImplicitTypeIDRegistry::lookupOrInsert(ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffects::Allocatemlir::MemoryEffects::Free)
)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffects::Write)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::PayloadIRResource)
[transform-dialect-utils]: renaming private symbols to resolve conflicts:
[transform-dialect-utils]:   found @linalg_generalize
[transform-dialect-utils]:   found @linalg_specialize
[transform-dialect-utils]:   found @convert_elementwise_to_linalg
[transform-dialect-utils]:   found @elementwise_fuse
[transform-dialect-utils]:   found @linalg_analysis
[transform-dialect-utils]:   found @linalg_decompose
[transform-dialect-utils]:   found @flatten_elementwise
[transform-dialect-utils]:   found @linalg_basic_fuse
[transform-dialect-utils]:   found @linalg_basic_vectorization
[transform-dialect-utils]: moving all symbols into target
[transform-dialect-utils]:   moving @linalg_generalize without collision
[transform-dialect-utils]:   moving @linalg_specialize without collision
[transform-dialect-utils]:   moving @convert_elementwise_to_linalg without collision
[transform-dialect-utils]:   moving @elementwise_fuse without collision
[transform-dialect-utils]:   moving @linalg_analysis without collision
[transform-dialect-utils]:   moving @linalg_decompose without collision
[transform-dialect-utils]:   moving @flatten_elementwise without collision
[transform-dialect-utils]:   moving @linalg_basic_fuse without collision
[transform-dialect-utils]:   moving @linalg_basic_vectorization without collision
[transform-dialect-utils]: done merging ops
[transform-dialect-utils]: renaming private symbols to resolve conflicts:
[transform-dialect-utils]:   found @linalg_generalize
[transform-dialect-utils]:   found @linalg_specialize
[transform-dialect-utils]:   found @convert_elementwise_to_linalg
[transform-dialect-utils]:   found @elementwise_fuse
[transform-dialect-utils]:   found @linalg_analysis
[transform-dialect-utils]:   found @linalg_decompose
[transform-dialect-utils]:   found @flatten_elementwise
[transform-dialect-utils]:   found @linalg_basic_fuse
[transform-dialect-utils]:   found @linalg_basic_vectorization
[transform-dialect-utils]: moving all symbols into target
[transform-dialect-utils]:   moving @linalg_generalize without collision
[transform-dialect-utils]:   moving @linalg_specialize without collision
[transform-dialect-utils]:   moving @convert_elementwise_to_linalg without collision
[transform-dialect-utils]:   moving @elementwise_fuse without collision
[transform-dialect-utils]:   moving @linalg_analysis without collision
[transform-dialect-utils]:   moving @linalg_decompose without collision
[transform-dialect-utils]:   moving @flatten_elementwise without collision
[transform-dialect-utils]:   moving @linalg_basic_fuse without collision
[transform-dialect-utils]:   moving @linalg_basic_vectorization without collision
[transform-dialect-utils]: done merging ops
// -----// IR Dump After PreloadLibraryPass (transform-preload-library) //----- //
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %3 : tensor<1x128xf32>
  }
}


[transform-dialect-interpreter-utils]: Apply
transform.named_sequence @linalg_decompose(%arg0: !transform.any_op {transform.readonly}) {
  %0 = transform.structured.match interface{LinalgOp} in %arg0 : (!transform.any_op) -> !transform.any_op
  %1 = transform.structured.decompose %0 : (!transform.any_op) -> !transform.any_op
  %2 = transform.structured.match ops{["linalg.softmax"]} in %arg0 : (!transform.any_op) -> !transform.any_op
  %3 = transform.structured.decompose_interface %2 : (!transform.any_op) -> !transform.any_op
  transform.yield 
}
[transform-dialect-interpreter-utils]: To
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %3 : tensor<1x128xf32>
  }
}
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::PossibleTopLevelTransformOpTrait<mlir::TypeID::get() [with Trait = mlir::transform::PossibleTopLevelTransformOpTrait]::Empty>)
[transform-dialect] applying: transform.named_sequence @linalg_decompose(%arg0: !transform.any_op {transform.readonly}) {...}
[transform-dialect] Top-level payload before application:
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %3 : tensor<1x128xf32>
  }
}
[transform-dialect] ExpensiveChecksEnabled
[transform-dialect] --Start checkAndRecordHandleInvalidation
[transform-dialect] --End checkAndRecordHandleInvalidation -> SUCCESS
[transform-dialect] applying: %0 = transform.structured.match interface{LinalgOp} in %arg0 : (!transform.any_op) -> !transform.any_op
[transform-dialect] Top-level payload before application:
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %3 : tensor<1x128xf32>
  }
}
[transform-dialect] ExpensiveChecksEnabled
[transform-dialect] --Start checkAndRecordHandleInvalidation
[transform-dialect] ----iterate on handle: <block argument> of type '!transform.any_op' at index: 0
[transform-dialect] ----no consume effect -> SKIP
[transform-dialect] --End checkAndRecordHandleInvalidation -> SUCCESS
[transform-dialect] iterate on handle: <block argument> of type '!transform.any_op' at index: 0
[transform-dialect] --handle not consumed -> SKIP
[transform-dialect] Top-level payload:
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %3 : tensor<1x128xf32>
  }
}
[transform-dialect] applying: %1 = transform.structured.decompose %0 : (!transform.any_op) -> !transform.any_op
[transform-dialect] Top-level payload before application:
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %3 : tensor<1x128xf32>
  }
}
[transform-dialect] ExpensiveChecksEnabled
[transform-dialect] --Start checkAndRecordHandleInvalidation
[transform-dialect] ----iterate on handle: %0 = transform.structured.match interface{LinalgOp} in %arg0 : (!transform.any_op) -> !transform.any_op
[transform-dialect] ----found consume effect
[transform-dialect] ----recordOpHandleInvalidation
[transform-dialect] ----recording invalidation for empty handle: %0 = transform.structured.match interface{LinalgOp} in %arg0 : (!transform.any_op) -> !transform.any_op
[transform-dialect] --End checkAndRecordHandleInvalidation -> SUCCESS
[transform-dialect] iterate on handle: %0 = transform.structured.match interface{LinalgOp} in %arg0 : (!transform.any_op) -> !transform.any_op
[transform-dialect] --handle is consumed
[transform-dialect] --checkRepeatedConsumptionInOperand for Operation*
[transform-dialect] Top-level payload:
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %3 : tensor<1x128xf32>
  }
}
[transform-dialect] applying: %2 = transform.structured.match ops{["linalg.softmax"]} in %arg0 : (!transform.any_op) -> !transform.any_op
[transform-dialect] Top-level payload before application:
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %3 : tensor<1x128xf32>
  }
}
[transform-dialect] ExpensiveChecksEnabled
[transform-dialect] --Start checkAndRecordHandleInvalidation
[transform-dialect] ----iterate on handle: <block argument> of type '!transform.any_op' at index: 0
[transform-dialect] ----no consume effect -> SKIP
[transform-dialect] --End checkAndRecordHandleInvalidation -> SUCCESS
[transform-dialect] iterate on handle: <block argument> of type '!transform.any_op' at index: 0
[transform-dialect] --handle not consumed -> SKIP
[transform-dialect] Top-level payload:
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %3 : tensor<1x128xf32>
  }
}
[transform-dialect] applying: %3 = transform.structured.decompose_interface %2 : (!transform.any_op) -> !transform.any_op
[transform-dialect] Top-level payload before application:
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %3 : tensor<1x128xf32>
  }
}
[transform-dialect] ExpensiveChecksEnabled
[transform-dialect] --Start checkAndRecordHandleInvalidation
[transform-dialect] ----iterate on handle: %2 = transform.structured.match ops{["linalg.softmax"]} in %arg0 : (!transform.any_op) -> !transform.any_op
[transform-dialect] ----found consume effect
[transform-dialect] ----recordOpHandleInvalidation
[transform-dialect] --recordOpHandleInvalidationOne
[transform-dialect] --ancestors: %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>, %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
[transform-dialect] ----handle one ancestor: %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
[transform-dialect] ----of payload with name: "builtin.module"
[transform-dialect] ----of payload: module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %3 : tensor<1x128xf32>
  }
}
[transform-dialect] ----handle one ancestor: %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
[transform-dialect] ----of payload with name: "builtin.module"
[transform-dialect] ----of payload: module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
    %2 = tensor.empty() : tensor<1x128xf32>
    %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
    return %3 : tensor<1x128xf32>
  }
}
[transform-dialect] --recordOpHandleInvalidationOne
[transform-dialect] --ancestors: %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>, %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
[transform-dialect] ----handle one ancestor: %1 = linalg.softmax dimension(1) ins(%arg0 : tensor<1x128xf32>) outs(%0 : tensor<1x128xf32>) -> tensor<1x128xf32>
[transform-dialect] ----of payload with name: "linalg.softmax"
[transform-dialect] ----of payload: %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
[transform-dialect] ----handle one ancestor: %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
[transform-dialect] ----of payload with name: "linalg.softmax"
[transform-dialect] ----of payload: %3 = linalg.softmax dimension(1) ins(%1 : tensor<1x128xf32>) outs(%2 : tensor<1x128xf32>) -> tensor<1x128xf32>
[transform-dialect] --End checkAndRecordHandleInvalidation -> SUCCESS
[transform-dialect] iterate on handle: %2 = transform.structured.match ops{["linalg.softmax"]} in %arg0 : (!transform.any_op) -> !transform.any_op
[transform-dialect] --handle is consumed
[transform-dialect] --checkRepeatedConsumptionInOperand for Operation*
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::detail::GenericOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::MaxNumFOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::SubFOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::math::detail::ExpOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::AddFOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::DivFOpGenericAdaptorBase::Properties)
[transform-dialect] Top-level payload:
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlockImplicitTerminator<mlir::linalg::YieldOp>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::SingleBlockImplicitTerminator<mlir::linalg::YieldOp>::Impl]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AttrSizedOperandSegments<mlir::TypeID::get() [with Trait = mlir::OpTrait::AttrSizedOperandSegments]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::LinalgOp::Trait<mlir::TypeID::get() [with Trait = mlir::linalg::LinalgOp::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::FillOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::linalg::FillOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithFastMathInterface::Trait<mlir::TypeID::get() [with Trait = mlir::arith::ArithFastMathInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultType<mlir::TypeID::get() [with Trait = mlir::OpTrait::SameOperandsAndResultType]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorUnrollOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::VectorUnrollOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Elementwise<mlir::TypeID::get() [with Trait = mlir::OpTrait::Elementwise]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Scalarizable<mlir::TypeID::get() [with Trait = mlir::OpTrait::Scalarizable]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Vectorizable<mlir::TypeID::get() [with Trait = mlir::OpTrait::Vectorizable]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Tensorizable<mlir::TypeID::get() [with Trait = mlir::OpTrait::Tensorizable]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsCommutative<mlir::TypeID::get() [with Trait = mlir::OpTrait::IsCommutative]::Empty>)
#map = ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::StorageUserTrait::IsMutable<mlir::TypeID::get() [with Trait = mlir::detail::StorageUserTrait::IsMutable]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemRefLayoutAttrInterface::Trait<mlir::TypeID::get() [with Trait = mlir::MemRefLayoutAttrInterface::Trait]::Empty>)
affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %cst = arith.constant -3.40282347E+38 : f32
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.subf %in, %in_3 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %cst_0 = arith.constant 0.000000e+00 : f32
    %5 = linalg.fill ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.divf %in, %in_3 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %10 = linalg.fill ins(%cst_1 : f32) outs(%9 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.subf %in, %in_3 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %cst_2 = arith.constant 0.000000e+00 : f32
    %13 = linalg.fill ins(%cst_2 : f32) outs(%9 : tensor<1xf32>) -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.divf %in, %in_3 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
}
[transform-dialect] Top-level payload:
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %cst = arith.constant -3.40282347E+38 : f32
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.subf %in, %in_3 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %cst_0 = arith.constant 0.000000e+00 : f32
    %5 = linalg.fill ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.divf %in, %in_3 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %10 = linalg.fill ins(%cst_1 : f32) outs(%9 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.subf %in, %in_3 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %cst_2 = arith.constant 0.000000e+00 : f32
    %13 = linalg.fill ins(%cst_2 : f32) outs(%9 : tensor<1xf32>) -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.divf %in, %in_3 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
}
// -----// IR Dump After InterpreterPass (transform-interpreter) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %1 = "north_star.buffer"(%0) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %2 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %3 = "north_star.tensor_to_ns_tensor"(%2) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %4 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %5 = "north_star.tensor_to_ns_tensor"(%4) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %6 = "north_star.buffer"(%3, %5) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    "north_star.scatter"(%1, %6) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
    %7 = "north_star.get_tensor"(%6) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
    %8 = "north_star.ns_tensor_to_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
    %9 = "north_star.get_tensor"(%6) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
    %10 = "north_star.ns_tensor_to_tensor"(%9) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %11 = call @softmax_1_128_softmax_1_128_fused_kernel(%8) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = call @softmax_1_128_softmax_1_128_fused_kernel(%10) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %14 = "north_star.tensor_to_ns_tensor"(%13) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
    %15 = "north_star.buffer"(%12, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %16 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %17 = "north_star.tensor_to_ns_tensor"(%16) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
    %18 = "north_star.buffer"(%17) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
    "north_star.gather"(%15, %18) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
    %19 = "north_star.get_tensor"(%18) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
    %20 = "north_star.ns_tensor_to_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
    return %20 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %cst = arith.constant -3.40282347E+38 : f32
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.subf %in, %in_3 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %cst_0 = arith.constant 0.000000e+00 : f32
    %5 = linalg.fill ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.divf %in, %in_3 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %10 = linalg.fill ins(%cst_1 : f32) outs(%9 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.subf %in, %in_3 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %cst_2 = arith.constant 0.000000e+00 : f32
    %13 = linalg.fill ins(%cst_2 : f32) outs(%9 : tensor<1xf32>) -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.divf %in, %in_3 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
}


run in ConvertNorthStarToFuncPass

//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x5c95909deb50) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x5c9590aaf2c0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5c9590aacdd0) {
  %30 = "arith.constant"() <{value = 1 : i64}> : () -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5c9590aad010) {
  %31 = "arith.constant"() <{value = 0 : i64}> : () -> i64

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab5d00) {
  %32 = "north_star.tensor_to_ns_tensor"(%arg29) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "{anonymous}::TensorToNSTensorOpConversionPattern"
    ** Insert  : 'arith.constant'(0x5c9590abe290)
(i64, tensor<2x128xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
%c0_i64_0 = arith.constant 0 : i64
<block argument> of type 'tensor<2x128xf32>' at index: 0
    ** Insert  : 'tensor.cast'(0x5c9590ac92b0)
    ** Insert  : 'func.call'(0x5c9590ac0fb0)
    ** Replace : 'north_star.tensor_to_ns_tensor'(0x5c9590ab5d00)
"{anonymous}::TensorToNSTensorOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590abe290) {
      %33 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x5c9590ac92b0) {
      %30 = "tensor.cast"(%arg29) : (tensor<2x128xf32>) -> tensor<*xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ac0fb0) {
      %34 = "func.call"(%33, %30) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %3 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %4 = "north_star.tensor_to_ns_tensor"(%3) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %5 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %6 = "north_star.tensor_to_ns_tensor"(%5) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %7 = "north_star.buffer"(%4, %6) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%2, %7) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %8 = "north_star.get_tensor"(%7) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %9 = "north_star.ns_tensor_to_tensor"(%8) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %10 = "north_star.get_tensor"(%7) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %11 = "north_star.ns_tensor_to_tensor"(%10) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %12 = call @softmax_1_128_softmax_1_128_fused_kernel(%9) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %13 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %14 = call @softmax_1_128_softmax_1_128_fused_kernel(%11) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %15 = "north_star.tensor_to_ns_tensor"(%14) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %16 = "north_star.buffer"(%13, %15) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %17 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %18 = "north_star.tensor_to_ns_tensor"(%17) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %19 = "north_star.buffer"(%18) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%16, %19) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %20 = "north_star.get_tensor"(%19) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %21 = "north_star.ns_tensor_to_tensor"(%20) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %21 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5c9590ab5090) {
  %36 = "north_star.buffer"(%35) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.buffer -> ()' {
Trying to match "{anonymous}::BufferOpConversionPattern"
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::detail::ConstantOpGenericAdaptorBase::Properties)
    ** Insert  : 'llvm.mlir.constant'(0x5c9590abe760)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::detail::AllocaOpGenericAdaptorBase::Properties)
    ** Insert  : 'llvm.alloca'(0x5c9590abab30)
    ** Insert  : 'llvm.alloca'(0x5c9590abb3c0)
    ** Insert  : 'llvm.getelementptr'(0x5c9590abea30)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::detail::StoreOpGenericAdaptorBase::Properties)
    ** Insert  : 'llvm.store'(0x5c9590ab0d00)
    ** Insert  : 'llvm.getelementptr'(0x5c9590aca830)
    ** Insert  : 'arith.constant'(0x5c9590aca8d0)
    ** Insert  : 'llvm.store'(0x5c95909c1310)
(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::TensorType>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneTypedResult<mlir::TensorType>::Impl]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CastOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::CastOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::LLVM::LLVMPointerType>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneTypedResult<mlir::LLVM::LLVMPointerType>::Impl]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableAllocationOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::PromotableAllocationOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableAllocationOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::DestructurableAllocationOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::GetResultPtrElementType::Trait<mlir::TypeID::get() [with Trait = mlir::LLVM::GetResultPtrElementType::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AtLeastNOperands<1>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::AtLeastNOperands<1>::Impl]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::PromotableOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SafeMemorySlotAccessOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::SafeMemorySlotAccessOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableAccessorOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::DestructurableAccessorOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::AccessGroupOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::LLVM::AccessGroupOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LLVM::AliasAnalysisOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::LLVM::AliasAnalysisOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableMemOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::PromotableMemOpInterface::Trait]::Empty>)
%3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
%4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
%2 = llvm.mlir.constant(1 : i64) : i64
    ** Insert  : 'func.call'(0x5c9590ac7c50)
    ** Replace : 'north_star.buffer'(0x5c9590ab5090)
"{anonymous}::BufferOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.mlir.constant'(0x5c9590abe760) {
      %36 = "llvm.mlir.constant"() <{value = 1 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.alloca'(0x5c9590abab30) {
      %37 = "llvm.alloca"(%36) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>}> : (i64) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.alloca'(0x5c9590abb3c0) {
      %38 = "llvm.alloca"(%36) <{elem_type = i64}> : (i64) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.getelementptr'(0x5c9590abea30) {
      %39 = "llvm.getelementptr"(%37) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.store'(0x5c9590ab0d00) {
      "llvm.store"(%34, %39) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.getelementptr'(0x5c9590aca830) {
      %40 = "llvm.getelementptr"(%38) <{elem_type = i64, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590aca8d0) {
      %41 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.store'(0x5c95909c1310) {
      "llvm.store"(%41, %40) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ac7c50) {
      %42 = "func.call"(%37, %38, %36) <{callee = @__NS__MakeBuffer_f32}> : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %10 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %11 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %12 = "north_star.tensor_to_ns_tensor"(%11) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %13 = "north_star.buffer"(%10, %12) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%8, %13) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %14 = "north_star.get_tensor"(%13) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %15 = "north_star.ns_tensor_to_tensor"(%14) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %16 = "north_star.get_tensor"(%13) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %17 = "north_star.ns_tensor_to_tensor"(%16) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %18 = call @softmax_1_128_softmax_1_128_fused_kernel(%15) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %19 = "north_star.tensor_to_ns_tensor"(%18) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %20 = call @softmax_1_128_softmax_1_128_fused_kernel(%17) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %21 = "north_star.tensor_to_ns_tensor"(%20) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %22 = "north_star.buffer"(%19, %21) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %23 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %24 = "north_star.tensor_to_ns_tensor"(%23) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %25 = "north_star.buffer"(%24) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%22, %25) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %26 = "north_star.get_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %27 = "north_star.ns_tensor_to_tensor"(%26) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %27 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x5c9590aacc80) {
  "func.call"(%32) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590aaccf0) {
  %44 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aacd60) {
  %45 = "north_star.tensor_to_ns_tensor"(%44) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "{anonymous}::TensorToNSTensorOpConversionPattern"
    ** Insert  : 'arith.constant'(0x5c9590ac97b0)
(i64, tensor<1x128xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
%c0_i64_2 = arith.constant 0 : i64
%9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    ** Insert  : 'tensor.cast'(0x5c9590acc9e0)
    ** Insert  : 'func.call'(0x5c9590accad0)
    ** Replace : 'north_star.tensor_to_ns_tensor'(0x5c9590aacd60)
"{anonymous}::TensorToNSTensorOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590ac97b0) {
      %46 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x5c9590acc9e0) {
      %45 = "tensor.cast"(%44) : (tensor<1x128xf32>) -> tensor<*xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590accad0) {
      %47 = "func.call"(%46, %45) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %13 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %14 = "north_star.buffer"(%11, %13) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%8, %14) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %15 = "north_star.get_tensor"(%14) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %16 = "north_star.ns_tensor_to_tensor"(%15) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %17 = "north_star.get_tensor"(%14) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %18 = "north_star.ns_tensor_to_tensor"(%17) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %19 = call @softmax_1_128_softmax_1_128_fused_kernel(%16) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %20 = "north_star.tensor_to_ns_tensor"(%19) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %21 = call @softmax_1_128_softmax_1_128_fused_kernel(%18) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %22 = "north_star.tensor_to_ns_tensor"(%21) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %23 = "north_star.buffer"(%20, %22) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %24 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %25 = "north_star.tensor_to_ns_tensor"(%24) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %26 = "north_star.buffer"(%25) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%23, %26) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %27 = "north_star.get_tensor"(%26) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %28 = "north_star.ns_tensor_to_tensor"(%27) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %28 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x5c9590aafd00) {
  "func.call"(%31) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590aafd70) {
  %49 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590aafde0) {
  %50 = "north_star.tensor_to_ns_tensor"(%49) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "{anonymous}::TensorToNSTensorOpConversionPattern"
    ** Insert  : 'arith.constant'(0x5c9590accea0)
(i64, tensor<1x128xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
%c1_i64_4 = arith.constant 1 : i64
%12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    ** Insert  : 'tensor.cast'(0x5c9590acd020)
    ** Insert  : 'func.call'(0x5c9590acd6c0)
    ** Replace : 'north_star.tensor_to_ns_tensor'(0x5c9590aafde0)
"{anonymous}::TensorToNSTensorOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590accea0) {
      %51 = "arith.constant"() <{value = 1 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x5c9590acd020) {
      %50 = "tensor.cast"(%49) : (tensor<1x128xf32>) -> tensor<*xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590acd6c0) {
      %52 = "func.call"(%51, %50) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%8, %15) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %16 = "north_star.get_tensor"(%15) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %17 = "north_star.ns_tensor_to_tensor"(%16) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %18 = "north_star.get_tensor"(%15) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %19 = "north_star.ns_tensor_to_tensor"(%18) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %20 = call @softmax_1_128_softmax_1_128_fused_kernel(%17) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %21 = "north_star.tensor_to_ns_tensor"(%20) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %22 = call @softmax_1_128_softmax_1_128_fused_kernel(%19) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %23 = "north_star.tensor_to_ns_tensor"(%22) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %24 = "north_star.buffer"(%21, %23) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %25 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %26 = "north_star.tensor_to_ns_tensor"(%25) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %27 = "north_star.buffer"(%26) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%24, %27) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %28 = "north_star.get_tensor"(%27) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %29 = "north_star.ns_tensor_to_tensor"(%28) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %29 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5c9590aafe50) {
  %54 = "north_star.buffer"(%48, %53) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.buffer -> ()' {
Trying to match "{anonymous}::BufferOpConversionPattern"
    ** Insert  : 'llvm.mlir.constant'(0x5c9590acd830)
    ** Insert  : 'llvm.alloca'(0x5c9590acd190)
    ** Insert  : 'llvm.alloca'(0x5c9590aca970)
    ** Insert  : 'llvm.getelementptr'(0x5c9590acd890)
    ** Insert  : 'llvm.store'(0x5c9590acd0a0)
    ** Insert  : 'llvm.getelementptr'(0x5c9590acd930)
    ** Insert  : 'arith.constant'(0x5c9590acd9d0)
    ** Insert  : 'llvm.store'(0x5c9590a00790)
    ** Insert  : 'llvm.getelementptr'(0x5c9590acda40)
    ** Insert  : 'llvm.store'(0x5c95909eba40)
    ** Insert  : 'llvm.getelementptr'(0x5c9590acdae0)
    ** Insert  : 'arith.constant'(0x5c9590acdbb0)
    ** Insert  : 'llvm.store'(0x5c95909d81d0)
(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
%16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
%17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
%15 = llvm.mlir.constant(2 : i64) : i64
    ** Insert  : 'func.call'(0x5c9590ad15e0)
    ** Replace : 'north_star.buffer'(0x5c9590aafe50)
"{anonymous}::BufferOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.mlir.constant'(0x5c9590acd830) {
      %54 = "llvm.mlir.constant"() <{value = 2 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.alloca'(0x5c9590acd190) {
      %55 = "llvm.alloca"(%54) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>}> : (i64) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.alloca'(0x5c9590aca970) {
      %56 = "llvm.alloca"(%54) <{elem_type = i64}> : (i64) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.getelementptr'(0x5c9590acd890) {
      %57 = "llvm.getelementptr"(%55) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.store'(0x5c9590acd0a0) {
      "llvm.store"(%47, %57) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.getelementptr'(0x5c9590acd930) {
      %58 = "llvm.getelementptr"(%56) <{elem_type = i64, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590acd9d0) {
      %59 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.store'(0x5c9590a00790) {
      "llvm.store"(%59, %58) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.getelementptr'(0x5c9590acda40) {
      %60 = "llvm.getelementptr"(%55) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 1>}> : (!llvm.ptr) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.store'(0x5c95909eba40) {
      "llvm.store"(%52, %60) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.getelementptr'(0x5c9590acdae0) {
      %61 = "llvm.getelementptr"(%56) <{elem_type = i64, rawConstantIndices = array<i32: 1>}> : (!llvm.ptr) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590acdbb0) {
      %62 = "arith.constant"() <{value = 1 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.store'(0x5c95909d81d0) {
      "llvm.store"(%62, %61) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ad15e0) {
      %63 = "func.call"(%55, %56, %54) <{callee = @__NS__MakeBuffer_f32}> : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %24 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %25 = "north_star.ns_tensor_to_tensor"(%24) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %26 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %27 = "north_star.ns_tensor_to_tensor"(%26) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %28 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %29 = "north_star.tensor_to_ns_tensor"(%28) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %31 = "north_star.tensor_to_ns_tensor"(%30) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %32 = "north_star.buffer"(%29, %31) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %33 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %34 = "north_star.tensor_to_ns_tensor"(%33) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %35 = "north_star.buffer"(%34) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%32, %35) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %36 = "north_star.get_tensor"(%35) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %37 = "north_star.ns_tensor_to_tensor"(%36) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %37 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.scatter'(0x5c9590aafeb0) {
  "north_star.scatter"(%43, %64) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.scatter -> ()' {
Trying to match "{anonymous}::ScatterOpOpConversionPattern"
(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
%7 = func.call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
%22 = func.call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    ** Insert  : 'func.call'(0x5c9590acdc70)
    ** Replace : 'north_star.scatter'(0x5c9590aafeb0)
"{anonymous}::ScatterOpOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590acdc70) {
      "func.call"(%42, %63) <{callee = @__NS__Scatter}> : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__Scatter(%7, %22) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %24 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %25 = "north_star.ns_tensor_to_tensor"(%24) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %26 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %27 = "north_star.ns_tensor_to_tensor"(%26) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %28 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %29 = "north_star.tensor_to_ns_tensor"(%28) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %31 = "north_star.tensor_to_ns_tensor"(%30) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %32 = "north_star.buffer"(%29, %31) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %33 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %34 = "north_star.tensor_to_ns_tensor"(%33) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %35 = "north_star.buffer"(%34) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%32, %35) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %36 = "north_star.get_tensor"(%35) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %37 = "north_star.ns_tensor_to_tensor"(%36) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %37 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.get_tensor'(0x5c9590aaff20) {
  %65 = "north_star.get_tensor"(%64) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.get_tensor -> ()' {
Trying to match "{anonymous}::GetTensorOpConversionPattern"
    ** Insert  : 'arith.constant'(0x5c9590ad1440)
(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
%c0_i64_8 = arith.constant 0 : i64
%22 = func.call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    ** Insert  : 'func.call'(0x5c9590acece0)
    ** Replace : 'north_star.get_tensor'(0x5c9590aaff20)
"{anonymous}::GetTensorOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590ad1440) {
      %65 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590acece0) {
      %66 = "func.call"(%65, %63) <{callee = @__NS__GetTensor_f32}> : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__Scatter(%7, %22) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %c0_i64_8 = arith.constant 0 : i64
  %24 = call @__NS__GetTensor_f32(%c0_i64_8, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %25 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %26 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %27 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %28 = "north_star.ns_tensor_to_tensor"(%27) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %29 = call @softmax_1_128_softmax_1_128_fused_kernel(%26) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %30 = "north_star.tensor_to_ns_tensor"(%29) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %31 = call @softmax_1_128_softmax_1_128_fused_kernel(%28) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %32 = "north_star.tensor_to_ns_tensor"(%31) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %33 = "north_star.buffer"(%30, %32) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %34 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %35 = "north_star.tensor_to_ns_tensor"(%34) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %36 = "north_star.buffer"(%35) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%33, %36) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %37 = "north_star.get_tensor"(%36) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %38 = "north_star.ns_tensor_to_tensor"(%37) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %38 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590aaff90) {
  %68 = "north_star.ns_tensor_to_tensor"(%67) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.ns_tensor_to_tensor -> ()' {
Trying to match "{anonymous}::NSTensorToTensorOpConversionPattern"
(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<1x128xf32>
%24 = func.call @__NS__GetTensor_f32(%c0_i64_8, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    ** Insert  : 'func.call'(0x5c9590acfc60)
    ** Insert  : 'tensor.cast'(0x5c9590acfcf0)
    ** Replace : 'north_star.ns_tensor_to_tensor'(0x5c9590aaff90)
"{anonymous}::NSTensorToTensorOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590acfc60) {
      %68 = "func.call"(%66) <{callee = @__NS__NSMemrefToMemref_f32}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x5c9590acfcf0) {
      %69 = "tensor.cast"(%68) : (tensor<*xf32>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__Scatter(%7, %22) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %c0_i64_8 = arith.constant 0 : i64
  %24 = call @__NS__GetTensor_f32(%c0_i64_8, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %25 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %26 = call @__NS__NSMemrefToMemref_f32(%24) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_9 = tensor.cast %26 : tensor<*xf32> to tensor<1x128xf32>
  %27 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %28 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %29 = "north_star.ns_tensor_to_tensor"(%28) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %31 = "north_star.tensor_to_ns_tensor"(%30) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %32 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %33 = "north_star.tensor_to_ns_tensor"(%32) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %34 = "north_star.buffer"(%31, %33) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %35 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %36 = "north_star.tensor_to_ns_tensor"(%35) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %37 = "north_star.buffer"(%36) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%34, %37) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %38 = "north_star.get_tensor"(%37) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %39 = "north_star.ns_tensor_to_tensor"(%38) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %39 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.get_tensor'(0x5c9590ab0000) {
  %71 = "north_star.get_tensor"(%64) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.get_tensor -> ()' {
Trying to match "{anonymous}::GetTensorOpConversionPattern"
    ** Insert  : 'arith.constant'(0x5c9590aced90)
(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
%c1_i64_10 = arith.constant 1 : i64
%22 = func.call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    ** Insert  : 'func.call'(0x5c9590ad2090)
    ** Replace : 'north_star.get_tensor'(0x5c9590ab0000)
"{anonymous}::GetTensorOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590aced90) {
      %71 = "arith.constant"() <{value = 1 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ad2090) {
      %72 = "func.call"(%71, %63) <{callee = @__NS__GetTensor_f32}> : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__Scatter(%7, %22) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %c0_i64_8 = arith.constant 0 : i64
  %24 = call @__NS__GetTensor_f32(%c0_i64_8, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %25 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %26 = call @__NS__NSMemrefToMemref_f32(%24) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_9 = tensor.cast %26 : tensor<*xf32> to tensor<1x128xf32>
  %27 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %c1_i64_10 = arith.constant 1 : i64
  %28 = call @__NS__GetTensor_f32(%c1_i64_10, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %29 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %30 = "north_star.ns_tensor_to_tensor"(%29) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %31 = call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %32 = "north_star.tensor_to_ns_tensor"(%31) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %33 = call @softmax_1_128_softmax_1_128_fused_kernel(%30) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %34 = "north_star.tensor_to_ns_tensor"(%33) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %35 = "north_star.buffer"(%32, %34) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %36 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %37 = "north_star.tensor_to_ns_tensor"(%36) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %38 = "north_star.buffer"(%37) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%35, %38) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %39 = "north_star.get_tensor"(%38) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %40 = "north_star.ns_tensor_to_tensor"(%39) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %40 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab0070) {
  %74 = "north_star.ns_tensor_to_tensor"(%73) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.ns_tensor_to_tensor -> ()' {
Trying to match "{anonymous}::NSTensorToTensorOpConversionPattern"
(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<1x128xf32>
%28 = func.call @__NS__GetTensor_f32(%c1_i64_10, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    ** Insert  : 'func.call'(0x5c9590ad1db0)
    ** Insert  : 'tensor.cast'(0x5c9590ad25e0)
    ** Replace : 'north_star.ns_tensor_to_tensor'(0x5c9590ab0070)
"{anonymous}::NSTensorToTensorOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ad1db0) {
      %74 = "func.call"(%72) <{callee = @__NS__NSMemrefToMemref_f32}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x5c9590ad25e0) {
      %75 = "tensor.cast"(%74) : (tensor<*xf32>) -> tensor<1x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__Scatter(%7, %22) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %c0_i64_8 = arith.constant 0 : i64
  %24 = call @__NS__GetTensor_f32(%c0_i64_8, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %25 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %26 = call @__NS__NSMemrefToMemref_f32(%24) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_9 = tensor.cast %26 : tensor<*xf32> to tensor<1x128xf32>
  %27 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %c1_i64_10 = arith.constant 1 : i64
  %28 = call @__NS__GetTensor_f32(%c1_i64_10, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %29 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %30 = call @__NS__NSMemrefToMemref_f32(%28) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_11 = tensor.cast %30 : tensor<*xf32> to tensor<1x128xf32>
  %31 = "north_star.ns_tensor_to_tensor"(%29) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %32 = call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %33 = "north_star.tensor_to_ns_tensor"(%32) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %34 = call @softmax_1_128_softmax_1_128_fused_kernel(%31) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %35 = "north_star.tensor_to_ns_tensor"(%34) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %36 = "north_star.buffer"(%33, %35) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %37 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %38 = "north_star.tensor_to_ns_tensor"(%37) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %39 = "north_star.buffer"(%38) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%36, %39) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %40 = "north_star.get_tensor"(%39) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %41 = "north_star.ns_tensor_to_tensor"(%40) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %41 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x5c9590ab0140) {
  "func.call"(%32) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x5c9590ab9790) {
  %77 = "func.call"(%70) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab01b0) {
  %78 = "north_star.tensor_to_ns_tensor"(%77) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "{anonymous}::TensorToNSTensorOpConversionPattern"
    ** Insert  : 'arith.constant'(0x5c9590ace8f0)
(i64, tensor<1x128xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
%c0_i64_12 = arith.constant 0 : i64
%32 = func.call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    ** Insert  : 'tensor.cast'(0x5c9590ad27a0)
    ** Insert  : 'func.call'(0x5c9590ad2a80)
    ** Replace : 'north_star.tensor_to_ns_tensor'(0x5c9590ab01b0)
"{anonymous}::TensorToNSTensorOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590ace8f0) {
      %79 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x5c9590ad27a0) {
      %78 = "tensor.cast"(%77) : (tensor<1x128xf32>) -> tensor<*xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ad2a80) {
      %80 = "func.call"(%79, %78) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__Scatter(%7, %22) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %c0_i64_8 = arith.constant 0 : i64
  %24 = call @__NS__GetTensor_f32(%c0_i64_8, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %25 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %26 = call @__NS__NSMemrefToMemref_f32(%24) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_9 = tensor.cast %26 : tensor<*xf32> to tensor<1x128xf32>
  %27 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %c1_i64_10 = arith.constant 1 : i64
  %28 = call @__NS__GetTensor_f32(%c1_i64_10, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %29 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %30 = call @__NS__NSMemrefToMemref_f32(%28) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_11 = tensor.cast %30 : tensor<*xf32> to tensor<1x128xf32>
  %31 = "north_star.ns_tensor_to_tensor"(%29) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %32 = call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_12 = tensor.cast %32 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_13 = arith.constant 0 : i64
  %33 = call @__NS__MemrefToNSMemref_f32(%c0_i64_13, %cast_12) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %34 = "north_star.tensor_to_ns_tensor"(%32) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%31) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %36 = "north_star.tensor_to_ns_tensor"(%35) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %37 = "north_star.buffer"(%34, %36) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %38 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %39 = "north_star.tensor_to_ns_tensor"(%38) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %40 = "north_star.buffer"(%39) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%37, %40) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %41 = "north_star.get_tensor"(%40) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %42 = "north_star.ns_tensor_to_tensor"(%41) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %42 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x5c9590ab0280) {
  "func.call"(%31) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x5c9590ab78a0) {
  %82 = "func.call"(%76) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab02f0) {
  %83 = "north_star.tensor_to_ns_tensor"(%82) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "{anonymous}::TensorToNSTensorOpConversionPattern"
    ** Insert  : 'arith.constant'(0x5c9590ad2210)
(i64, tensor<1x128xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
%c1_i64_14 = arith.constant 1 : i64
%35 = func.call @softmax_1_128_softmax_1_128_fused_kernel(%31) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    ** Insert  : 'tensor.cast'(0x5c9590ad3190)
    ** Insert  : 'func.call'(0x5c9590a4b930)
    ** Replace : 'north_star.tensor_to_ns_tensor'(0x5c9590ab02f0)
"{anonymous}::TensorToNSTensorOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590ad2210) {
      %84 = "arith.constant"() <{value = 1 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x5c9590ad3190) {
      %83 = "tensor.cast"(%82) : (tensor<1x128xf32>) -> tensor<*xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590a4b930) {
      %85 = "func.call"(%84, %83) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__Scatter(%7, %22) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %c0_i64_8 = arith.constant 0 : i64
  %24 = call @__NS__GetTensor_f32(%c0_i64_8, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %25 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %26 = call @__NS__NSMemrefToMemref_f32(%24) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_9 = tensor.cast %26 : tensor<*xf32> to tensor<1x128xf32>
  %27 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %c1_i64_10 = arith.constant 1 : i64
  %28 = call @__NS__GetTensor_f32(%c1_i64_10, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %29 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %30 = call @__NS__NSMemrefToMemref_f32(%28) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_11 = tensor.cast %30 : tensor<*xf32> to tensor<1x128xf32>
  %31 = "north_star.ns_tensor_to_tensor"(%29) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %32 = call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_12 = tensor.cast %32 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_13 = arith.constant 0 : i64
  %33 = call @__NS__MemrefToNSMemref_f32(%c0_i64_13, %cast_12) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %34 = "north_star.tensor_to_ns_tensor"(%32) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%31) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_14 = tensor.cast %35 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_15 = arith.constant 1 : i64
  %36 = call @__NS__MemrefToNSMemref_f32(%c1_i64_15, %cast_14) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %37 = "north_star.tensor_to_ns_tensor"(%35) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %38 = "north_star.buffer"(%34, %37) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %39 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %40 = "north_star.tensor_to_ns_tensor"(%39) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %41 = "north_star.buffer"(%40) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%38, %41) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %42 = "north_star.get_tensor"(%41) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %43 = "north_star.ns_tensor_to_tensor"(%42) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %43 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5c9590ab0360) {
  %87 = "north_star.buffer"(%81, %86) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.buffer -> ()' {
Trying to match "{anonymous}::BufferOpConversionPattern"
    ** Insert  : 'llvm.mlir.constant'(0x5c9590ad3ce0)
    ** Insert  : 'llvm.alloca'(0x5c9590acdf50)
    ** Insert  : 'llvm.alloca'(0x5c9590ace3b0)
    ** Insert  : 'llvm.getelementptr'(0x5c9590ad3d40)
    ** Insert  : 'llvm.store'(0x5c9590ad35f0)
    ** Insert  : 'llvm.getelementptr'(0x5c9590ad3de0)
    ** Insert  : 'arith.constant'(0x5c9590a4b860)
    ** Insert  : 'llvm.store'(0x5c9590ad2b20)
    ** Insert  : 'llvm.getelementptr'(0x5c9590ad3e80)
    ** Insert  : 'llvm.store'(0x5c9590ad2660)
    ** Insert  : 'llvm.getelementptr'(0x5c9590ad3f20)
    ** Insert  : 'arith.constant'(0x5c9590ad3ff0)
    ** Insert  : 'llvm.store'(0x5c9590acf6c0)
(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
%39 = llvm.alloca %38 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
%40 = llvm.alloca %38 x i64 : (i64) -> !llvm.ptr
%38 = llvm.mlir.constant(2 : i64) : i64
    ** Insert  : 'func.call'(0x5c9590ad7310)
    ** Replace : 'north_star.buffer'(0x5c9590ab0360)
"{anonymous}::BufferOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.mlir.constant'(0x5c9590ad3ce0) {
      %87 = "llvm.mlir.constant"() <{value = 2 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.alloca'(0x5c9590acdf50) {
      %88 = "llvm.alloca"(%87) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>}> : (i64) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.alloca'(0x5c9590ace3b0) {
      %89 = "llvm.alloca"(%87) <{elem_type = i64}> : (i64) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.getelementptr'(0x5c9590ad3d40) {
      %90 = "llvm.getelementptr"(%88) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.store'(0x5c9590ad35f0) {
      "llvm.store"(%80, %90) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.getelementptr'(0x5c9590ad3de0) {
      %91 = "llvm.getelementptr"(%89) <{elem_type = i64, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590a4b860) {
      %92 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.store'(0x5c9590ad2b20) {
      "llvm.store"(%92, %91) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.getelementptr'(0x5c9590ad3e80) {
      %93 = "llvm.getelementptr"(%88) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 1>}> : (!llvm.ptr) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.store'(0x5c9590ad2660) {
      "llvm.store"(%85, %93) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.getelementptr'(0x5c9590ad3f20) {
      %94 = "llvm.getelementptr"(%89) <{elem_type = i64, rawConstantIndices = array<i32: 1>}> : (!llvm.ptr) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590ad3ff0) {
      %95 = "arith.constant"() <{value = 1 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.store'(0x5c9590acf6c0) {
      "llvm.store"(%95, %94) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ad7310) {
      %96 = "func.call"(%88, %89, %87) <{callee = @__NS__MakeBuffer_f32}> : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__Scatter(%7, %22) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %c0_i64_8 = arith.constant 0 : i64
  %24 = call @__NS__GetTensor_f32(%c0_i64_8, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %25 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %26 = call @__NS__NSMemrefToMemref_f32(%24) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_9 = tensor.cast %26 : tensor<*xf32> to tensor<1x128xf32>
  %27 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %c1_i64_10 = arith.constant 1 : i64
  %28 = call @__NS__GetTensor_f32(%c1_i64_10, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %29 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %30 = call @__NS__NSMemrefToMemref_f32(%28) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_11 = tensor.cast %30 : tensor<*xf32> to tensor<1x128xf32>
  %31 = "north_star.ns_tensor_to_tensor"(%29) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %32 = call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_12 = tensor.cast %32 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_13 = arith.constant 0 : i64
  %33 = call @__NS__MemrefToNSMemref_f32(%c0_i64_13, %cast_12) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %34 = "north_star.tensor_to_ns_tensor"(%32) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%31) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_14 = tensor.cast %35 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_15 = arith.constant 1 : i64
  %36 = call @__NS__MemrefToNSMemref_f32(%c1_i64_15, %cast_14) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %37 = "north_star.tensor_to_ns_tensor"(%35) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %38 = llvm.mlir.constant(2 : i64) : i64
  %39 = llvm.alloca %38 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %40 = llvm.alloca %38 x i64 : (i64) -> !llvm.ptr
  %41 = llvm.getelementptr %39[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %33, %41 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %42 = llvm.getelementptr %40[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_16 = arith.constant 0 : i64
  llvm.store %c0_i64_16, %42 : i64, !llvm.ptr
  %43 = llvm.getelementptr %39[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %36, %43 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %44 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_17 = arith.constant 1 : i64
  llvm.store %c1_i64_17, %44 : i64, !llvm.ptr
  %45 = call @__NS__MakeBuffer_f32(%39, %40, %38) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %46 = "north_star.buffer"(%34, %37) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %47 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %48 = "north_star.tensor_to_ns_tensor"(%47) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %49 = "north_star.buffer"(%48) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%46, %49) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %50 = "north_star.get_tensor"(%49) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %51 = "north_star.ns_tensor_to_tensor"(%50) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %51 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.call'(0x5c9590ab0430) {
  "func.call"(%32) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590ab04a0) {
  %98 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.tensor_to_ns_tensor'(0x5c9590ab0510) {
  %99 = "north_star.tensor_to_ns_tensor"(%98) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.tensor_to_ns_tensor -> ()' {
Trying to match "{anonymous}::TensorToNSTensorOpConversionPattern"
    ** Insert  : 'arith.constant'(0x5c9590ad4190)
(i64, tensor<2x128xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
%c0_i64_18 = arith.constant 0 : i64
%47 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    ** Insert  : 'tensor.cast'(0x5c9590ad3c50)
    ** Insert  : 'func.call'(0x5c9590ad7760)
    ** Replace : 'north_star.tensor_to_ns_tensor'(0x5c9590ab0510)
"{anonymous}::TensorToNSTensorOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590ad4190) {
      %100 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x5c9590ad3c50) {
      %99 = "tensor.cast"(%98) : (tensor<2x128xf32>) -> tensor<*xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ad7760) {
      %101 = "func.call"(%100, %99) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__Scatter(%7, %22) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %c0_i64_8 = arith.constant 0 : i64
  %24 = call @__NS__GetTensor_f32(%c0_i64_8, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %25 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %26 = call @__NS__NSMemrefToMemref_f32(%24) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_9 = tensor.cast %26 : tensor<*xf32> to tensor<1x128xf32>
  %27 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %c1_i64_10 = arith.constant 1 : i64
  %28 = call @__NS__GetTensor_f32(%c1_i64_10, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %29 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %30 = call @__NS__NSMemrefToMemref_f32(%28) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_11 = tensor.cast %30 : tensor<*xf32> to tensor<1x128xf32>
  %31 = "north_star.ns_tensor_to_tensor"(%29) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %32 = call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_12 = tensor.cast %32 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_13 = arith.constant 0 : i64
  %33 = call @__NS__MemrefToNSMemref_f32(%c0_i64_13, %cast_12) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %34 = "north_star.tensor_to_ns_tensor"(%32) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%31) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_14 = tensor.cast %35 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_15 = arith.constant 1 : i64
  %36 = call @__NS__MemrefToNSMemref_f32(%c1_i64_15, %cast_14) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %37 = "north_star.tensor_to_ns_tensor"(%35) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %38 = llvm.mlir.constant(2 : i64) : i64
  %39 = llvm.alloca %38 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %40 = llvm.alloca %38 x i64 : (i64) -> !llvm.ptr
  %41 = llvm.getelementptr %39[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %33, %41 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %42 = llvm.getelementptr %40[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_16 = arith.constant 0 : i64
  llvm.store %c0_i64_16, %42 : i64, !llvm.ptr
  %43 = llvm.getelementptr %39[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %36, %43 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %44 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_17 = arith.constant 1 : i64
  llvm.store %c1_i64_17, %44 : i64, !llvm.ptr
  %45 = call @__NS__MakeBuffer_f32(%39, %40, %38) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %46 = "north_star.buffer"(%34, %37) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %47 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %cast_18 = tensor.cast %47 : tensor<2x128xf32> to tensor<*xf32>
  %c0_i64_19 = arith.constant 0 : i64
  %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64_19, %cast_18) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %49 = "north_star.tensor_to_ns_tensor"(%47) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %50 = "north_star.buffer"(%49) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%46, %50) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %51 = "north_star.get_tensor"(%50) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %52 = "north_star.ns_tensor_to_tensor"(%51) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %52 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.buffer'(0x5c9590ab0580) {
  %103 = "north_star.buffer"(%102) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.buffer -> ()' {
Trying to match "{anonymous}::BufferOpConversionPattern"
    ** Insert  : 'llvm.mlir.constant'(0x5c9590ad6c10)
    ** Insert  : 'llvm.alloca'(0x5c9590ad40c0)
    ** Insert  : 'llvm.alloca'(0x5c9590ad5130)
    ** Insert  : 'llvm.getelementptr'(0x5c9590ad6c70)
    ** Insert  : 'llvm.store'(0x5c9590ad4650)
    ** Insert  : 'llvm.getelementptr'(0x5c9590ad6d10)
    ** Insert  : 'arith.constant'(0x5c9590ad6db0)
    ** Insert  : 'llvm.store'(0x5c95909b9e90)
(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
%51 = llvm.alloca %50 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
%52 = llvm.alloca %50 x i64 : (i64) -> !llvm.ptr
%50 = llvm.mlir.constant(1 : i64) : i64
    ** Insert  : 'func.call'(0x5c9590ad5f50)
    ** Replace : 'north_star.buffer'(0x5c9590ab0580)
"{anonymous}::BufferOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.mlir.constant'(0x5c9590ad6c10) {
      %103 = "llvm.mlir.constant"() <{value = 1 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.alloca'(0x5c9590ad40c0) {
      %104 = "llvm.alloca"(%103) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>}> : (i64) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.alloca'(0x5c9590ad5130) {
      %105 = "llvm.alloca"(%103) <{elem_type = i64}> : (i64) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.getelementptr'(0x5c9590ad6c70) {
      %106 = "llvm.getelementptr"(%104) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.store'(0x5c9590ad4650) {
      "llvm.store"(%101, %106) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.getelementptr'(0x5c9590ad6d10) {
      %107 = "llvm.getelementptr"(%105) <{elem_type = i64, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590ad6db0) {
      %108 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'llvm.store'(0x5c95909b9e90) {
      "llvm.store"(%108, %107) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ad5f50) {
      %109 = "func.call"(%104, %105, %103) <{callee = @__NS__MakeBuffer_f32}> : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__Scatter(%7, %22) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %c0_i64_8 = arith.constant 0 : i64
  %24 = call @__NS__GetTensor_f32(%c0_i64_8, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %25 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %26 = call @__NS__NSMemrefToMemref_f32(%24) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_9 = tensor.cast %26 : tensor<*xf32> to tensor<1x128xf32>
  %27 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %c1_i64_10 = arith.constant 1 : i64
  %28 = call @__NS__GetTensor_f32(%c1_i64_10, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %29 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %30 = call @__NS__NSMemrefToMemref_f32(%28) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_11 = tensor.cast %30 : tensor<*xf32> to tensor<1x128xf32>
  %31 = "north_star.ns_tensor_to_tensor"(%29) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %32 = call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_12 = tensor.cast %32 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_13 = arith.constant 0 : i64
  %33 = call @__NS__MemrefToNSMemref_f32(%c0_i64_13, %cast_12) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %34 = "north_star.tensor_to_ns_tensor"(%32) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%31) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_14 = tensor.cast %35 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_15 = arith.constant 1 : i64
  %36 = call @__NS__MemrefToNSMemref_f32(%c1_i64_15, %cast_14) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %37 = "north_star.tensor_to_ns_tensor"(%35) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %38 = llvm.mlir.constant(2 : i64) : i64
  %39 = llvm.alloca %38 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %40 = llvm.alloca %38 x i64 : (i64) -> !llvm.ptr
  %41 = llvm.getelementptr %39[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %33, %41 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %42 = llvm.getelementptr %40[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_16 = arith.constant 0 : i64
  llvm.store %c0_i64_16, %42 : i64, !llvm.ptr
  %43 = llvm.getelementptr %39[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %36, %43 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %44 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_17 = arith.constant 1 : i64
  llvm.store %c1_i64_17, %44 : i64, !llvm.ptr
  %45 = call @__NS__MakeBuffer_f32(%39, %40, %38) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %46 = "north_star.buffer"(%34, %37) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %47 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %cast_18 = tensor.cast %47 : tensor<2x128xf32> to tensor<*xf32>
  %c0_i64_19 = arith.constant 0 : i64
  %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64_19, %cast_18) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %49 = "north_star.tensor_to_ns_tensor"(%47) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %50 = llvm.mlir.constant(1 : i64) : i64
  %51 = llvm.alloca %50 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %52 = llvm.alloca %50 x i64 : (i64) -> !llvm.ptr
  %53 = llvm.getelementptr %51[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %48, %53 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %54 = llvm.getelementptr %52[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_20 = arith.constant 0 : i64
  llvm.store %c0_i64_20, %54 : i64, !llvm.ptr
  %55 = call @__NS__MakeBuffer_f32(%51, %52, %50) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %56 = "north_star.buffer"(%49) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  "north_star.gather"(%46, %56) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %57 = "north_star.get_tensor"(%56) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %58 = "north_star.ns_tensor_to_tensor"(%57) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %58 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.gather'(0x5c9590ab05e0) {
  "north_star.gather"(%97, %110) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.gather -> ()' {
Trying to match "{anonymous}::GatherOpOpConversionPattern"
(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
%45 = func.call @__NS__MakeBuffer_f32(%39, %40, %38) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
%55 = func.call @__NS__MakeBuffer_f32(%51, %52, %50) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    ** Insert  : 'func.call'(0x5c9590ad6e10)
    ** Replace : 'north_star.gather'(0x5c9590ab05e0)
"{anonymous}::GatherOpOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ad6e10) {
      "func.call"(%96, %109) <{callee = @__NS__Gather}> : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__Scatter(%7, %22) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %c0_i64_8 = arith.constant 0 : i64
  %24 = call @__NS__GetTensor_f32(%c0_i64_8, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %25 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %26 = call @__NS__NSMemrefToMemref_f32(%24) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_9 = tensor.cast %26 : tensor<*xf32> to tensor<1x128xf32>
  %27 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %c1_i64_10 = arith.constant 1 : i64
  %28 = call @__NS__GetTensor_f32(%c1_i64_10, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %29 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %30 = call @__NS__NSMemrefToMemref_f32(%28) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_11 = tensor.cast %30 : tensor<*xf32> to tensor<1x128xf32>
  %31 = "north_star.ns_tensor_to_tensor"(%29) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %32 = call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_12 = tensor.cast %32 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_13 = arith.constant 0 : i64
  %33 = call @__NS__MemrefToNSMemref_f32(%c0_i64_13, %cast_12) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %34 = "north_star.tensor_to_ns_tensor"(%32) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%31) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_14 = tensor.cast %35 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_15 = arith.constant 1 : i64
  %36 = call @__NS__MemrefToNSMemref_f32(%c1_i64_15, %cast_14) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %37 = "north_star.tensor_to_ns_tensor"(%35) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %38 = llvm.mlir.constant(2 : i64) : i64
  %39 = llvm.alloca %38 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %40 = llvm.alloca %38 x i64 : (i64) -> !llvm.ptr
  %41 = llvm.getelementptr %39[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %33, %41 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %42 = llvm.getelementptr %40[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_16 = arith.constant 0 : i64
  llvm.store %c0_i64_16, %42 : i64, !llvm.ptr
  %43 = llvm.getelementptr %39[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %36, %43 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %44 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_17 = arith.constant 1 : i64
  llvm.store %c1_i64_17, %44 : i64, !llvm.ptr
  %45 = call @__NS__MakeBuffer_f32(%39, %40, %38) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %46 = "north_star.buffer"(%34, %37) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %47 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %cast_18 = tensor.cast %47 : tensor<2x128xf32> to tensor<*xf32>
  %c0_i64_19 = arith.constant 0 : i64
  %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64_19, %cast_18) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %49 = "north_star.tensor_to_ns_tensor"(%47) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %50 = llvm.mlir.constant(1 : i64) : i64
  %51 = llvm.alloca %50 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %52 = llvm.alloca %50 x i64 : (i64) -> !llvm.ptr
  %53 = llvm.getelementptr %51[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %48, %53 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %54 = llvm.getelementptr %52[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_20 = arith.constant 0 : i64
  llvm.store %c0_i64_20, %54 : i64, !llvm.ptr
  %55 = call @__NS__MakeBuffer_f32(%51, %52, %50) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %56 = "north_star.buffer"(%49) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__Gather(%45, %55) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.gather"(%46, %56) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %57 = "north_star.get_tensor"(%56) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %58 = "north_star.ns_tensor_to_tensor"(%57) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %58 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.get_tensor'(0x5c9590ab0650) {
  %111 = "north_star.get_tensor"(%110) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.get_tensor -> ()' {
Trying to match "{anonymous}::GetTensorOpConversionPattern"
    ** Insert  : 'arith.constant'(0x5c9590ad6ec0)
(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
%c0_i64_21 = arith.constant 0 : i64
%55 = func.call @__NS__MakeBuffer_f32(%51, %52, %50) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    ** Insert  : 'func.call'(0x5c9590ad7ee0)
    ** Replace : 'north_star.get_tensor'(0x5c9590ab0650)
"{anonymous}::GetTensorOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'arith.constant'(0x5c9590ad6ec0) {
      %111 = "arith.constant"() <{value = 0 : i64}> : () -> i64

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ad7ee0) {
      %112 = "func.call"(%111, %109) <{callee = @__NS__GetTensor_f32}> : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__Scatter(%7, %22) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %c0_i64_8 = arith.constant 0 : i64
  %24 = call @__NS__GetTensor_f32(%c0_i64_8, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %25 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %26 = call @__NS__NSMemrefToMemref_f32(%24) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_9 = tensor.cast %26 : tensor<*xf32> to tensor<1x128xf32>
  %27 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %c1_i64_10 = arith.constant 1 : i64
  %28 = call @__NS__GetTensor_f32(%c1_i64_10, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %29 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %30 = call @__NS__NSMemrefToMemref_f32(%28) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_11 = tensor.cast %30 : tensor<*xf32> to tensor<1x128xf32>
  %31 = "north_star.ns_tensor_to_tensor"(%29) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %32 = call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_12 = tensor.cast %32 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_13 = arith.constant 0 : i64
  %33 = call @__NS__MemrefToNSMemref_f32(%c0_i64_13, %cast_12) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %34 = "north_star.tensor_to_ns_tensor"(%32) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%31) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_14 = tensor.cast %35 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_15 = arith.constant 1 : i64
  %36 = call @__NS__MemrefToNSMemref_f32(%c1_i64_15, %cast_14) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %37 = "north_star.tensor_to_ns_tensor"(%35) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %38 = llvm.mlir.constant(2 : i64) : i64
  %39 = llvm.alloca %38 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %40 = llvm.alloca %38 x i64 : (i64) -> !llvm.ptr
  %41 = llvm.getelementptr %39[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %33, %41 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %42 = llvm.getelementptr %40[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_16 = arith.constant 0 : i64
  llvm.store %c0_i64_16, %42 : i64, !llvm.ptr
  %43 = llvm.getelementptr %39[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %36, %43 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %44 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_17 = arith.constant 1 : i64
  llvm.store %c1_i64_17, %44 : i64, !llvm.ptr
  %45 = call @__NS__MakeBuffer_f32(%39, %40, %38) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %46 = "north_star.buffer"(%34, %37) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %47 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %cast_18 = tensor.cast %47 : tensor<2x128xf32> to tensor<*xf32>
  %c0_i64_19 = arith.constant 0 : i64
  %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64_19, %cast_18) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %49 = "north_star.tensor_to_ns_tensor"(%47) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %50 = llvm.mlir.constant(1 : i64) : i64
  %51 = llvm.alloca %50 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %52 = llvm.alloca %50 x i64 : (i64) -> !llvm.ptr
  %53 = llvm.getelementptr %51[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %48, %53 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %54 = llvm.getelementptr %52[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_20 = arith.constant 0 : i64
  llvm.store %c0_i64_20, %54 : i64, !llvm.ptr
  %55 = call @__NS__MakeBuffer_f32(%51, %52, %50) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %56 = "north_star.buffer"(%49) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__Gather(%45, %55) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.gather"(%46, %56) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %c0_i64_21 = arith.constant 0 : i64
  %57 = call @__NS__GetTensor_f32(%c0_i64_21, %55) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %58 = "north_star.get_tensor"(%56) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %59 = "north_star.ns_tensor_to_tensor"(%58) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %59 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'north_star.ns_tensor_to_tensor'(0x5c9590ab80e0) {
  %114 = "north_star.ns_tensor_to_tensor"(%113) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>

  * Fold {
  } -> FAILURE : unable to fold

  * Pattern : 'north_star.ns_tensor_to_tensor -> ()' {
Trying to match "{anonymous}::NSTensorToTensorOpConversionPattern"
(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<2x128xf32>
%57 = func.call @__NS__GetTensor_f32(%c0_i64_21, %55) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    ** Insert  : 'func.call'(0x5c9590ad6b80)
    ** Insert  : 'tensor.cast'(0x5c9590ad7aa0)
    ** Replace : 'north_star.ns_tensor_to_tensor'(0x5c9590ab80e0)
"{anonymous}::NSTensorToTensorOpConversionPattern" result 1

    //===-------------------------------------------===//
    Legalizing operation : 'func.call'(0x5c9590ad6b80) {
      %114 = "func.call"(%112) <{callee = @__NS__NSMemrefToMemref_f32}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//

    //===-------------------------------------------===//
    Legalizing operation : 'tensor.cast'(0x5c9590ad7aa0) {
      %115 = "tensor.cast"(%114) : (tensor<*xf32>) -> tensor<2x128xf32>

    } -> SUCCESS : operation marked legal by the target
    //===-------------------------------------------===//
  } -> SUCCESS : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %1 = "north_star.tensor_to_ns_tensor"(%arg0) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %0, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_1 = arith.constant 0 : i64
  llvm.store %c0_i64_1, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %8 = "north_star.buffer"(%1) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_2 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_3 = arith.constant 0 : i64
  %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = "north_star.tensor_to_ns_tensor"(%9) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %12 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_4 = tensor.cast %12 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_5 = arith.constant 1 : i64
  %13 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %14 = "north_star.tensor_to_ns_tensor"(%12) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %15 = llvm.mlir.constant(2 : i64) : i64
  %16 = llvm.alloca %15 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %17 = llvm.alloca %15 x i64 : (i64) -> !llvm.ptr
  %18 = llvm.getelementptr %16[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %18 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %19 = llvm.getelementptr %17[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_6 = arith.constant 0 : i64
  llvm.store %c0_i64_6, %19 : i64, !llvm.ptr
  %20 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %13, %20 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %21 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_7 = arith.constant 1 : i64
  llvm.store %c1_i64_7, %21 : i64, !llvm.ptr
  %22 = call @__NS__MakeBuffer_f32(%16, %17, %15) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %23 = "north_star.buffer"(%11, %14) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__Scatter(%7, %22) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.scatter"(%8, %23) : (!north_star.buffer<0>, !north_star.buffer<0, 1>) -> ()
  %c0_i64_8 = arith.constant 0 : i64
  %24 = call @__NS__GetTensor_f32(%c0_i64_8, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %25 = "north_star.get_tensor"(%23) <{device_id = 0 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,0>
  %26 = call @__NS__NSMemrefToMemref_f32(%24) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_9 = tensor.cast %26 : tensor<*xf32> to tensor<1x128xf32>
  %27 = "north_star.ns_tensor_to_tensor"(%25) <{device_id = 0 : i64}> : (!north_star.ns_tensor<1x128xf32,0>) -> tensor<1x128xf32>
  %c1_i64_10 = arith.constant 1 : i64
  %28 = call @__NS__GetTensor_f32(%c1_i64_10, %22) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %29 = "north_star.get_tensor"(%23) <{device_id = 1 : i64}> : (!north_star.buffer<0, 1>) -> !north_star.ns_tensor<1x128xf32,1>
  %30 = call @__NS__NSMemrefToMemref_f32(%28) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_11 = tensor.cast %30 : tensor<*xf32> to tensor<1x128xf32>
  %31 = "north_star.ns_tensor_to_tensor"(%29) <{device_id = 1 : i64}> : (!north_star.ns_tensor<1x128xf32,1>) -> tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %32 = call @softmax_1_128_softmax_1_128_fused_kernel(%27) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_12 = tensor.cast %32 : tensor<1x128xf32> to tensor<*xf32>
  %c0_i64_13 = arith.constant 0 : i64
  %33 = call @__NS__MemrefToNSMemref_f32(%c0_i64_13, %cast_12) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %34 = "north_star.tensor_to_ns_tensor"(%32) <{device_id = 0 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,0>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%31) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_14 = tensor.cast %35 : tensor<1x128xf32> to tensor<*xf32>
  %c1_i64_15 = arith.constant 1 : i64
  %36 = call @__NS__MemrefToNSMemref_f32(%c1_i64_15, %cast_14) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %37 = "north_star.tensor_to_ns_tensor"(%35) <{device_id = 1 : i64}> : (tensor<1x128xf32>) -> !north_star.ns_tensor<1x128xf32,1>
  %38 = llvm.mlir.constant(2 : i64) : i64
  %39 = llvm.alloca %38 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %40 = llvm.alloca %38 x i64 : (i64) -> !llvm.ptr
  %41 = llvm.getelementptr %39[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %33, %41 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %42 = llvm.getelementptr %40[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_16 = arith.constant 0 : i64
  llvm.store %c0_i64_16, %42 : i64, !llvm.ptr
  %43 = llvm.getelementptr %39[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %36, %43 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %44 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, i64
  %c1_i64_17 = arith.constant 1 : i64
  llvm.store %c1_i64_17, %44 : i64, !llvm.ptr
  %45 = call @__NS__MakeBuffer_f32(%39, %40, %38) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %46 = "north_star.buffer"(%34, %37) : (!north_star.ns_tensor<1x128xf32,0>, !north_star.ns_tensor<1x128xf32,1>) -> !north_star.buffer<0, 1>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %47 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %cast_18 = tensor.cast %47 : tensor<2x128xf32> to tensor<*xf32>
  %c0_i64_19 = arith.constant 0 : i64
  %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64_19, %cast_18) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %49 = "north_star.tensor_to_ns_tensor"(%47) <{device_id = 0 : i64}> : (tensor<2x128xf32>) -> !north_star.ns_tensor<2x128xf32,0>
  %50 = llvm.mlir.constant(1 : i64) : i64
  %51 = llvm.alloca %50 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %52 = llvm.alloca %50 x i64 : (i64) -> !llvm.ptr
  %53 = llvm.getelementptr %51[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %48, %53 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %54 = llvm.getelementptr %52[0] : (!llvm.ptr) -> !llvm.ptr, i64
  %c0_i64_20 = arith.constant 0 : i64
  llvm.store %c0_i64_20, %54 : i64, !llvm.ptr
  %55 = call @__NS__MakeBuffer_f32(%51, %52, %50) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  %56 = "north_star.buffer"(%49) : (!north_star.ns_tensor<2x128xf32,0>) -> !north_star.buffer<0>
  call @__NS__Gather(%45, %55) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  "north_star.gather"(%46, %56) : (!north_star.buffer<0, 1>, !north_star.buffer<0>) -> ()
  %c0_i64_21 = arith.constant 0 : i64
  %57 = call @__NS__GetTensor_f32(%c0_i64_21, %55) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %58 = "north_star.get_tensor"(%56) <{device_id = 0 : i64}> : (!north_star.buffer<0>) -> !north_star.ns_tensor<2x128xf32,0>
  %59 = call @__NS__NSMemrefToMemref_f32(%57) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_22 = tensor.cast %59 : tensor<*xf32> to tensor<2x128xf32>
  %60 = "north_star.ns_tensor_to_tensor"(%58) <{device_id = 0 : i64}> : (!north_star.ns_tensor<2x128xf32,0>) -> tensor<2x128xf32>
  return %60 : tensor<2x128xf32>
}


} -> SUCCESS
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x5c9590ab8050) {
  "func.return"(%116) : (tensor<2x128xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x5c9590aaf350) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.func'(0x5c9590ab92f0) {
} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590ab7070) {
  %0 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590aae710) {
  %1 = "tensor.empty"() : () -> tensor<1xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5c9590ab7670) {
  %2 = "arith.constant"() <{value = -3.40282347E+38 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x5c9590a0ce20) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x5c9590ab9d80) {
  "linalg.yield"(%arg27) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x5c95909cc1b0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxnumf'(0x5c9590ac5f40) {
  %29 = "arith.maxnumf"(%arg25, %arg26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x5c9590a0c500) {
  "linalg.yield"(%29) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x5c95909984f0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subf'(0x5c9590ac6510) {
  %27 = "arith.subf"(%arg22, %arg23) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'math.exp'(0x5c9590ab1ea0) {
  %28 = "math.exp"(%27) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x5c9590ab42d0) {
  "linalg.yield"(%28) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5c9590a9a430) {
  %6 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x5c9590ac6a80) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x5c9590ab1530) {
  "linalg.yield"(%arg20) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x5c95909c1f10) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x5c9590ac6c00) {
  %26 = "arith.addf"(%arg18, %arg19) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x5c9590aad0c0) {
  "linalg.yield"(%26) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x5c95909983e0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.divf'(0x5c9590ac6d60) {
  %25 = "arith.divf"(%arg15, %arg16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x5c9590a0b810) {
  "linalg.yield"(%25) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590ab7150) {
  %10 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tensor.empty'(0x5c9590ab70e0) {
  %11 = "tensor.empty"() : () -> tensor<1xf32>

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5c9590ab3e00) {
  %12 = "arith.constant"() <{value = -3.40282347E+38 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x5c9590ac6eb0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x5c9590ab3d20) {
  "linalg.yield"(%arg13) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x5c95909c1d50) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.maxnumf'(0x5c9590ac7080) {
  %24 = "arith.maxnumf"(%arg11, %arg12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x5c9590ac7120) {
  "linalg.yield"(%24) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x5c9590ac73e0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.subf'(0x5c9590ac72b0) {
  %22 = "arith.subf"(%arg8, %arg9) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'math.exp'(0x5c9590ab73b0) {
  %23 = "math.exp"(%22) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x5c9590ac7350) {
  "linalg.yield"(%23) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5c9590ab7600) {
  %16 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.fill'(0x5c9590ac7610) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x5c9590ac7560) {
  "linalg.yield"(%arg6) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x5c95909c1c00) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addf'(0x5c9590ac77e0) {
  %21 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x5c9590ac7880) {
  "linalg.yield"(%21) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.generic'(0x5c9590ac7b40) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.divf'(0x5c9590ac7a10) {
  %20 = "arith.divf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'linalg.yield'(0x5c9590ac7ab0) {
  "linalg.yield"(%20) : (f32) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'func.return'(0x5c9590ab8f20) {
  "func.return"(%19) : (tensor<1x128xf32>) -> ()

} -> SUCCESS : operation marked legal by the target
//===-------------------------------------------===//
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c0_i64_0 = arith.constant 0 : i64
    %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %1 = llvm.mlir.constant(1 : i64) : i64
    %2 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %3 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    %4 = llvm.getelementptr %2[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %0, %4 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, i64
    %c0_i64_1 = arith.constant 0 : i64
    llvm.store %c0_i64_1, %5 : i64, !llvm.ptr
    %6 = call @__NS__MakeBuffer_f32(%2, %3, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %7 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %cast_2 = tensor.cast %7 : tensor<1x128xf32> to tensor<*xf32>
    %c0_i64_3 = arith.constant 0 : i64
    %8 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %9 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %cast_4 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
    %c1_i64_5 = arith.constant 1 : i64
    %10 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %11 = llvm.mlir.constant(2 : i64) : i64
    %12 = llvm.alloca %11 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %13 = llvm.alloca %11 x i64 : (i64) -> !llvm.ptr
    %14 = llvm.getelementptr %12[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %8, %14 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %15 = llvm.getelementptr %13[0] : (!llvm.ptr) -> !llvm.ptr, i64
    %c0_i64_6 = arith.constant 0 : i64
    llvm.store %c0_i64_6, %15 : i64, !llvm.ptr
    %16 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %10, %16 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %17 = llvm.getelementptr %13[1] : (!llvm.ptr) -> !llvm.ptr, i64
    %c1_i64_7 = arith.constant 1 : i64
    llvm.store %c1_i64_7, %17 : i64, !llvm.ptr
    %18 = call @__NS__MakeBuffer_f32(%12, %13, %11) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%6, %18) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %c0_i64_8 = arith.constant 0 : i64
    %19 = call @__NS__GetTensor_f32(%c0_i64_8, %18) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %20 = call @__NS__NSMemrefToMemref_f32(%19) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_9 = tensor.cast %20 : tensor<*xf32> to tensor<1x128xf32>
    %c1_i64_10 = arith.constant 1 : i64
    %21 = call @__NS__GetTensor_f32(%c1_i64_10, %18) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %22 = call @__NS__NSMemrefToMemref_f32(%21) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_11 = tensor.cast %22 : tensor<*xf32> to tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %23 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_9) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %cast_12 = tensor.cast %23 : tensor<1x128xf32> to tensor<*xf32>
    %c0_i64_13 = arith.constant 0 : i64
    %24 = call @__NS__MemrefToNSMemref_f32(%c0_i64_13, %cast_12) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %25 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_11) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %cast_14 = tensor.cast %25 : tensor<1x128xf32> to tensor<*xf32>
    %c1_i64_15 = arith.constant 1 : i64
    %26 = call @__NS__MemrefToNSMemref_f32(%c1_i64_15, %cast_14) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = llvm.mlir.constant(2 : i64) : i64
    %28 = llvm.alloca %27 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %29 = llvm.alloca %27 x i64 : (i64) -> !llvm.ptr
    %30 = llvm.getelementptr %28[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %24, %30 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %31 = llvm.getelementptr %29[0] : (!llvm.ptr) -> !llvm.ptr, i64
    %c0_i64_16 = arith.constant 0 : i64
    llvm.store %c0_i64_16, %31 : i64, !llvm.ptr
    %32 = llvm.getelementptr %28[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %26, %32 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %33 = llvm.getelementptr %29[1] : (!llvm.ptr) -> !llvm.ptr, i64
    %c1_i64_17 = arith.constant 1 : i64
    llvm.store %c1_i64_17, %33 : i64, !llvm.ptr
    %34 = call @__NS__MakeBuffer_f32(%28, %29, %27) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %35 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_18 = tensor.cast %35 : tensor<2x128xf32> to tensor<*xf32>
    %c0_i64_19 = arith.constant 0 : i64
    %36 = call @__NS__MemrefToNSMemref_f32(%c0_i64_19, %cast_18) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %37 = llvm.mlir.constant(1 : i64) : i64
    %38 = llvm.alloca %37 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %39 = llvm.alloca %37 x i64 : (i64) -> !llvm.ptr
    %40 = llvm.getelementptr %38[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %36, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %41 = llvm.getelementptr %39[0] : (!llvm.ptr) -> !llvm.ptr, i64
    %c0_i64_20 = arith.constant 0 : i64
    llvm.store %c0_i64_20, %41 : i64, !llvm.ptr
    %42 = call @__NS__MakeBuffer_f32(%38, %39, %37) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%34, %42) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %c0_i64_21 = arith.constant 0 : i64
    %43 = call @__NS__GetTensor_f32(%c0_i64_21, %42) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %44 = call @__NS__NSMemrefToMemref_f32(%43) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_22 = tensor.cast %44 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_22 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %cst = arith.constant -3.40282347E+38 : f32
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.subf %in, %in_3 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %cst_0 = arith.constant 0.000000e+00 : f32
    %5 = linalg.fill ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.divf %in, %in_3 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %10 = linalg.fill ins(%cst_1 : f32) outs(%9 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.subf %in, %in_3 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %cst_2 = arith.constant 0.000000e+00 : f32
    %13 = linalg.fill ins(%cst_2 : f32) outs(%9 : tensor<1xf32>) -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.divf %in, %in_3 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
run out: ConvertNorthStarToFuncPass
// -----// IR Dump After ConvertNorthStarToFuncPass (convert-north-satr-to-func) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c0_i64_0 = arith.constant 0 : i64
    %0 = call @__NS__MemrefToNSMemref_f32(%c0_i64_0, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %1 = llvm.mlir.constant(1 : i64) : i64
    %2 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %3 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    %4 = llvm.getelementptr %2[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %0, %4 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, i64
    %c0_i64_1 = arith.constant 0 : i64
    llvm.store %c0_i64_1, %5 : i64, !llvm.ptr
    %6 = call @__NS__MakeBuffer_f32(%2, %3, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %7 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %cast_2 = tensor.cast %7 : tensor<1x128xf32> to tensor<*xf32>
    %c0_i64_3 = arith.constant 0 : i64
    %8 = call @__NS__MemrefToNSMemref_f32(%c0_i64_3, %cast_2) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %9 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %cast_4 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
    %c1_i64_5 = arith.constant 1 : i64
    %10 = call @__NS__MemrefToNSMemref_f32(%c1_i64_5, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %11 = llvm.mlir.constant(2 : i64) : i64
    %12 = llvm.alloca %11 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %13 = llvm.alloca %11 x i64 : (i64) -> !llvm.ptr
    %14 = llvm.getelementptr %12[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %8, %14 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %15 = llvm.getelementptr %13[0] : (!llvm.ptr) -> !llvm.ptr, i64
    %c0_i64_6 = arith.constant 0 : i64
    llvm.store %c0_i64_6, %15 : i64, !llvm.ptr
    %16 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %10, %16 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %17 = llvm.getelementptr %13[1] : (!llvm.ptr) -> !llvm.ptr, i64
    %c1_i64_7 = arith.constant 1 : i64
    llvm.store %c1_i64_7, %17 : i64, !llvm.ptr
    %18 = call @__NS__MakeBuffer_f32(%12, %13, %11) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%6, %18) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %c0_i64_8 = arith.constant 0 : i64
    %19 = call @__NS__GetTensor_f32(%c0_i64_8, %18) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %20 = call @__NS__NSMemrefToMemref_f32(%19) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_9 = tensor.cast %20 : tensor<*xf32> to tensor<1x128xf32>
    %c1_i64_10 = arith.constant 1 : i64
    %21 = call @__NS__GetTensor_f32(%c1_i64_10, %18) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %22 = call @__NS__NSMemrefToMemref_f32(%21) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_11 = tensor.cast %22 : tensor<*xf32> to tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %23 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_9) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %cast_12 = tensor.cast %23 : tensor<1x128xf32> to tensor<*xf32>
    %c0_i64_13 = arith.constant 0 : i64
    %24 = call @__NS__MemrefToNSMemref_f32(%c0_i64_13, %cast_12) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %25 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_11) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %cast_14 = tensor.cast %25 : tensor<1x128xf32> to tensor<*xf32>
    %c1_i64_15 = arith.constant 1 : i64
    %26 = call @__NS__MemrefToNSMemref_f32(%c1_i64_15, %cast_14) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = llvm.mlir.constant(2 : i64) : i64
    %28 = llvm.alloca %27 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %29 = llvm.alloca %27 x i64 : (i64) -> !llvm.ptr
    %30 = llvm.getelementptr %28[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %24, %30 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %31 = llvm.getelementptr %29[0] : (!llvm.ptr) -> !llvm.ptr, i64
    %c0_i64_16 = arith.constant 0 : i64
    llvm.store %c0_i64_16, %31 : i64, !llvm.ptr
    %32 = llvm.getelementptr %28[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %26, %32 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %33 = llvm.getelementptr %29[1] : (!llvm.ptr) -> !llvm.ptr, i64
    %c1_i64_17 = arith.constant 1 : i64
    llvm.store %c1_i64_17, %33 : i64, !llvm.ptr
    %34 = call @__NS__MakeBuffer_f32(%28, %29, %27) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %35 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_18 = tensor.cast %35 : tensor<2x128xf32> to tensor<*xf32>
    %c0_i64_19 = arith.constant 0 : i64
    %36 = call @__NS__MemrefToNSMemref_f32(%c0_i64_19, %cast_18) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %37 = llvm.mlir.constant(1 : i64) : i64
    %38 = llvm.alloca %37 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %39 = llvm.alloca %37 x i64 : (i64) -> !llvm.ptr
    %40 = llvm.getelementptr %38[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %36, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %41 = llvm.getelementptr %39[0] : (!llvm.ptr) -> !llvm.ptr, i64
    %c0_i64_20 = arith.constant 0 : i64
    llvm.store %c0_i64_20, %41 : i64, !llvm.ptr
    %42 = call @__NS__MakeBuffer_f32(%38, %39, %37) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%34, %42) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %c0_i64_21 = arith.constant 0 : i64
    %43 = call @__NS__GetTensor_f32(%c0_i64_21, %42) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %44 = call @__NS__NSMemrefToMemref_f32(%43) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_22 = tensor.cast %44 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_22 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %cst = arith.constant -3.40282347E+38 : f32
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.subf %in, %in_3 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %cst_0 = arith.constant 0.000000e+00 : f32
    %5 = linalg.fill ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.divf %in, %in_3 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %10 = linalg.fill ins(%cst_1 : f32) outs(%9 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.subf %in, %in_3 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %cst_2 = arith.constant 0.000000e+00 : f32
    %13 = linalg.fill ins(%cst_2 : f32) outs(%9 : tensor<1xf32>) -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %16 = arith.divf %in, %in_3 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}


** Replace : 'arith.constant'(0x5c9590abe290)
** Modified: 'func.call'(0x5c9590ac0fb0)
** Erase   : 'arith.constant'(0x5c9590abe290)
** Replace : 'arith.constant'(0x5c9590aca8d0)
** Modified: 'llvm.store'(0x5c95909c1310)
** Erase   : 'arith.constant'(0x5c9590aca8d0)
** Replace : 'arith.constant'(0x5c9590ac97b0)
** Modified: 'func.call'(0x5c9590accad0)
** Erase   : 'arith.constant'(0x5c9590ac97b0)
** Replace : 'arith.constant'(0x5c9590accea0)
** Modified: 'func.call'(0x5c9590acd6c0)
** Erase   : 'arith.constant'(0x5c9590accea0)
** Replace : 'arith.constant'(0x5c9590acd9d0)
** Modified: 'llvm.store'(0x5c9590a00790)
** Erase   : 'arith.constant'(0x5c9590acd9d0)
** Replace : 'arith.constant'(0x5c9590acdbb0)
** Modified: 'llvm.store'(0x5c95909d81d0)
** Erase   : 'arith.constant'(0x5c9590acdbb0)
** Replace : 'arith.constant'(0x5c9590ad1440)
** Modified: 'func.call'(0x5c9590acece0)
** Erase   : 'arith.constant'(0x5c9590ad1440)
** Replace : 'arith.constant'(0x5c9590aced90)
** Modified: 'func.call'(0x5c9590ad2090)
** Erase   : 'arith.constant'(0x5c9590aced90)
** Replace : 'arith.constant'(0x5c9590ace8f0)
** Modified: 'func.call'(0x5c9590ad2a80)
** Erase   : 'arith.constant'(0x5c9590ace8f0)
** Replace : 'arith.constant'(0x5c9590ad2210)
** Modified: 'func.call'(0x5c9590a4b930)
** Erase   : 'arith.constant'(0x5c9590ad2210)
** Replace : 'llvm.mlir.constant'(0x5c9590ad3ce0)
** Modified: 'func.call'(0x5c9590ad7310)
** Modified: 'llvm.alloca'(0x5c9590ace3b0)
** Modified: 'llvm.alloca'(0x5c9590acdf50)
** Erase   : 'llvm.mlir.constant'(0x5c9590ad3ce0)
** Replace : 'arith.constant'(0x5c9590a4b860)
** Modified: 'llvm.store'(0x5c9590ad2b20)
** Erase   : 'arith.constant'(0x5c9590a4b860)
** Replace : 'arith.constant'(0x5c9590ad3ff0)
** Modified: 'llvm.store'(0x5c9590acf6c0)
** Erase   : 'arith.constant'(0x5c9590ad3ff0)
** Replace : 'arith.constant'(0x5c9590ad4190)
** Modified: 'func.call'(0x5c9590ad7760)
** Erase   : 'arith.constant'(0x5c9590ad4190)
** Replace : 'llvm.mlir.constant'(0x5c9590ad6c10)
** Modified: 'func.call'(0x5c9590ad5f50)
** Modified: 'llvm.alloca'(0x5c9590ad5130)
** Modified: 'llvm.alloca'(0x5c9590ad40c0)
** Erase   : 'llvm.mlir.constant'(0x5c9590ad6c10)
** Replace : 'arith.constant'(0x5c9590ad6db0)
** Modified: 'llvm.store'(0x5c95909b9e90)
** Erase   : 'arith.constant'(0x5c9590ad6db0)
** Replace : 'arith.constant'(0x5c9590ad6ec0)
** Modified: 'func.call'(0x5c9590ad7ee0)
** Erase   : 'arith.constant'(0x5c9590ad6ec0)
** Replace : 'arith.constant'(0x5c9590ab3e00)
** Modified: 'linalg.fill'(0x5c9590ac6eb0)
** Erase   : 'arith.constant'(0x5c9590ab3e00)
** Replace : 'arith.constant'(0x5c9590ab7600)
** Modified: 'linalg.fill'(0x5c9590ac7610)
** Erase   : 'arith.constant'(0x5c9590ab7600)

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590ad4d10) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590acfbc0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590ace7d0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590acd790) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590acb370) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590ab7430) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590ab8f20) {
  "func.return"(%17) : (tensor<1x128xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ac7ab0) {
  "linalg.yield"(%18) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c9590ac7b40) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.divf'(0x5c9590ac7a10) {
  %18 = "arith.divf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ac7880) {
  "linalg.yield"(%19) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909c1c00) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x5c9590ac77e0) {
  %19 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ac7560) {
  "linalg.yield"(%arg6) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x5c9590ac7610) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.fill -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Insert  : 'linalg.generic'(0x5c9590ad5a60)
    ** Replace : 'linalg.fill'(0x5c9590ac7610)
    ** Modified: 'linalg.generic'(0x5c95909c1c00)
    ** Erase   : 'linalg.fill'(0x5c9590ac7610)
"mlir::linalg::LinalgGeneralizationPattern" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -3.40282347E+38 : f32
  %0 = tensor.empty() : tensor<1x128xf32>
  %1 = tensor.empty() : tensor<1xf32>
  %2 = linalg.fill ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.maxnumf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.subf %in, %in_1 : f32
    %17 = math.exp %16 : f32
    linalg.yield %17 : f32
  } -> tensor<1x128xf32>
  %5 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.addf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.divf %in, %in_1 : f32
    linalg.yield %16 : f32
  } -> tensor<1x128xf32>
  %8 = tensor.empty() : tensor<1x128xf32>
  %9 = tensor.empty() : tensor<1xf32>
  %10 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) -> tensor<1xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.maxnumf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.subf %in, %in_1 : f32
    %17 = math.exp %16 : f32
    linalg.yield %17 : f32
  } -> tensor<1x128xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  } -> tensor<1xf32>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.addf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.divf %in, %in_1 : f32
    linalg.yield %16 : f32
  } -> tensor<1x128xf32>
  return %15 : tensor<1x128xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909c1c00) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c9590ad5a60) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ac7350) {
  "linalg.yield"(%21) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'math.exp'(0x5c9590ab73b0) {
  %21 = "math.exp"(%20) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c9590ac73e0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.subf'(0x5c9590ac72b0) {
  %20 = "arith.subf"(%arg8, %arg9) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ac7120) {
  "linalg.yield"(%22) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909c1d50) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.maxnumf'(0x5c9590ac7080) {
  %22 = "arith.maxnumf"(%arg11, %arg12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ab3d20) {
  "linalg.yield"(%arg13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x5c9590ac6eb0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.fill -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Insert  : 'linalg.generic'(0x5c9590ad7820)
    ** Replace : 'linalg.fill'(0x5c9590ac6eb0)
    ** Modified: 'linalg.generic'(0x5c95909c1d50)
    ** Erase   : 'linalg.fill'(0x5c9590ac6eb0)
"mlir::linalg::LinalgGeneralizationPattern" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -3.40282347E+38 : f32
  %0 = tensor.empty() : tensor<1x128xf32>
  %1 = tensor.empty() : tensor<1xf32>
  %2 = linalg.fill ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.maxnumf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.subf %in, %in_1 : f32
    %17 = math.exp %16 : f32
    linalg.yield %17 : f32
  } -> tensor<1x128xf32>
  %5 = linalg.fill ins(%cst : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.addf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.divf %in, %in_1 : f32
    linalg.yield %16 : f32
  } -> tensor<1x128xf32>
  %8 = tensor.empty() : tensor<1x128xf32>
  %9 = tensor.empty() : tensor<1xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  } -> tensor<1xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.maxnumf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.subf %in, %in_1 : f32
    %17 = math.exp %16 : f32
    linalg.yield %17 : f32
  } -> tensor<1x128xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  } -> tensor<1xf32>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.addf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.divf %in, %in_1 : f32
    linalg.yield %16 : f32
  } -> tensor<1x128xf32>
  return %15 : tensor<1x128xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909c1d50) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c9590ad7820) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab70e0) {
  %11 = "tensor.empty"() : () -> tensor<1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab7150) {
  %10 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590a0b810) {
  "linalg.yield"(%23) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909983e0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.divf'(0x5c9590ac6d60) {
  %23 = "arith.divf"(%arg15, %arg16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590aad0c0) {
  "linalg.yield"(%24) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909c1f10) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x5c9590ac6c00) {
  %24 = "arith.addf"(%arg18, %arg19) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x5c9590ac6a80) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.fill -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Insert  : 'linalg.generic'(0x5c95909c1ab0)
    ** Replace : 'linalg.fill'(0x5c9590ac6a80)
    ** Modified: 'linalg.generic'(0x5c95909c1f10)
    ** Erase   : 'linalg.fill'(0x5c9590ac6a80)
"mlir::linalg::LinalgGeneralizationPattern" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -3.40282347E+38 : f32
  %0 = tensor.empty() : tensor<1x128xf32>
  %1 = tensor.empty() : tensor<1xf32>
  %2 = linalg.fill ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) -> tensor<1xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.maxnumf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.subf %in, %in_1 : f32
    %17 = math.exp %16 : f32
    linalg.yield %17 : f32
  } -> tensor<1x128xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  } -> tensor<1xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.addf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.divf %in, %in_1 : f32
    linalg.yield %16 : f32
  } -> tensor<1x128xf32>
  %8 = tensor.empty() : tensor<1x128xf32>
  %9 = tensor.empty() : tensor<1xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  } -> tensor<1xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.maxnumf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.subf %in, %in_1 : f32
    %17 = math.exp %16 : f32
    linalg.yield %17 : f32
  } -> tensor<1x128xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  } -> tensor<1xf32>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.addf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.divf %in, %in_1 : f32
    linalg.yield %16 : f32
  } -> tensor<1x128xf32>
  return %15 : tensor<1x128xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909c1f10) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909c1ab0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ab1530) {
  "linalg.yield"(%arg20) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5c9590a9a430) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ab42d0) {
  "linalg.yield"(%26) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'math.exp'(0x5c9590ab1ea0) {
  %26 = "math.exp"(%25) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909984f0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.subf'(0x5c9590ac6510) {
  %25 = "arith.subf"(%arg22, %arg23) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590a0c500) {
  "linalg.yield"(%27) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909cc1b0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.maxnumf'(0x5c9590ac5f40) {
  %27 = "arith.maxnumf"(%arg25, %arg26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.fill'(0x5c9590a0ce20) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.fill -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Insert  : 'linalg.generic'(0x5c95909bfdf0)
    ** Replace : 'linalg.fill'(0x5c9590a0ce20)
    ** Modified: 'linalg.generic'(0x5c95909cc1b0)
    ** Erase   : 'linalg.fill'(0x5c9590a0ce20)
"mlir::linalg::LinalgGeneralizationPattern" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -3.40282347E+38 : f32
  %0 = tensor.empty() : tensor<1x128xf32>
  %1 = tensor.empty() : tensor<1xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  } -> tensor<1xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.maxnumf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.subf %in, %in_1 : f32
    %17 = math.exp %16 : f32
    linalg.yield %17 : f32
  } -> tensor<1x128xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  } -> tensor<1xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.addf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.divf %in, %in_1 : f32
    linalg.yield %16 : f32
  } -> tensor<1x128xf32>
  %8 = tensor.empty() : tensor<1x128xf32>
  %9 = tensor.empty() : tensor<1xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  } -> tensor<1xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.maxnumf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.subf %in, %in_1 : f32
    %17 = math.exp %16 : f32
    linalg.yield %17 : f32
  } -> tensor<1x128xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  } -> tensor<1xf32>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %16 = arith.addf %in, %out : f32
    linalg.yield %16 : f32
  } -> tensor<1xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %16 = arith.divf %in, %in_1 : f32
    linalg.yield %16 : f32
  } -> tensor<1x128xf32>
  return %15 : tensor<1x128xf32>
}


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909cc1b0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909bfdf0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ab9d80) {
  "linalg.yield"(%arg27) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5c9590ab7670) {
  %1 = "arith.constant"() <{value = -3.40282347E+38 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590aae710) {
  %3 = "tensor.empty"() : () -> tensor<1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590ab92f0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab7070) {
  %2 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590aaf350) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590ab8050) {
  "func.return"(%81) : (tensor<2x128xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590ad7aa0) {
  %81 = "tensor.cast"(%80) : (tensor<*xf32>) -> tensor<2x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad6b80) {
  %80 = "func.call"(%79) <{callee = @__NS__NSMemrefToMemref_f32}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad7ee0) {
  %79 = "func.call"(%30, %78) <{callee = @__NS__GetTensor_f32}> : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad6e10) {
  "func.call"(%70, %78) <{callee = @__NS__Gather}> : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c95909b9e90) {
  "llvm.store"(%30, %77) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

ImplicitTypeIDRegistry::lookupOrInsert(mlir::SideEffects::DefaultResource)
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590ad6d10) {
  %77 = "llvm.getelementptr"(%75) <{elem_type = i64, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

} -> success : operation was folded
//===-------------------------------------------===//
** Replace : 'llvm.getelementptr'(0x5c9590ad6d10)
** Modified: 'llvm.store'(0x5c95909b9e90)
** Erase   : 'llvm.getelementptr'(0x5c9590ad6d10)
// *** IR Dump After Successful Folding ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %0 = llvm.mlir.constant(2 : i64) : i64
  %1 = llvm.mlir.constant(1 : i64) : i64
  %c0_i64 = arith.constant 0 : i64
  %c1_i64 = arith.constant 1 : i64
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %2 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %3 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %2, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c0_i64, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %8 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_0 = tensor.cast %8 : tensor<1x128xf32> to tensor<*xf32>
  %9 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %10 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_1 = tensor.cast %10 : tensor<1x128xf32> to tensor<*xf32>
  %11 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %12 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %13 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  %14 = llvm.getelementptr %12[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %9, %14 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %15 = llvm.getelementptr %13[0] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c0_i64, %15 : i64, !llvm.ptr
  %16 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %11, %16 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %17 = llvm.getelementptr %13[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %17 : i64, !llvm.ptr
  %18 = call @__NS__MakeBuffer_f32(%12, %13, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Scatter(%7, %18) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %19 = call @__NS__GetTensor_f32(%c0_i64, %18) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %20 = call @__NS__NSMemrefToMemref_f32(%19) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_2 = tensor.cast %20 : tensor<*xf32> to tensor<1x128xf32>
  %21 = call @__NS__GetTensor_f32(%c1_i64, %18) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %22 = call @__NS__NSMemrefToMemref_f32(%21) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_3 = tensor.cast %22 : tensor<*xf32> to tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %23 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_4 = tensor.cast %23 : tensor<1x128xf32> to tensor<*xf32>
  %24 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %25 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_5 = tensor.cast %25 : tensor<1x128xf32> to tensor<*xf32>
  %26 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %27 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %28 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  %29 = llvm.getelementptr %27[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %24, %29 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %30 = llvm.getelementptr %28[0] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c0_i64, %30 : i64, !llvm.ptr
  %31 = llvm.getelementptr %27[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %26, %31 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %32 = llvm.getelementptr %28[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %32 : i64, !llvm.ptr
  %33 = call @__NS__MakeBuffer_f32(%27, %28, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %34 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %cast_6 = tensor.cast %34 : tensor<2x128xf32> to tensor<*xf32>
  %35 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %36 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %37 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  %38 = llvm.getelementptr %36[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %35, %38 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %37 : i64, !llvm.ptr
  %39 = call @__NS__MakeBuffer_f32(%36, %37, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Gather(%33, %39) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %40 = call @__NS__GetTensor_f32(%c0_i64, %39) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %41 = call @__NS__NSMemrefToMemref_f32(%40) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_7 = tensor.cast %41 : tensor<*xf32> to tensor<2x128xf32>
  return %cast_7 : tensor<2x128xf32>
}



//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c95909b9e90) {
  "llvm.store"(%30, %75) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ad4650) {
  "llvm.store"(%73, %76) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590ad6c70) {
  %76 = "llvm.getelementptr"(%74) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

} -> success : operation was folded
//===-------------------------------------------===//
** Replace : 'llvm.getelementptr'(0x5c9590ad6c70)
** Modified: 'llvm.store'(0x5c9590ad4650)
** Erase   : 'llvm.getelementptr'(0x5c9590ad6c70)
// *** IR Dump After Successful Folding ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %0 = llvm.mlir.constant(2 : i64) : i64
  %1 = llvm.mlir.constant(1 : i64) : i64
  %c0_i64 = arith.constant 0 : i64
  %c1_i64 = arith.constant 1 : i64
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %2 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %3 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %2, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c0_i64, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %8 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_0 = tensor.cast %8 : tensor<1x128xf32> to tensor<*xf32>
  %9 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %10 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_1 = tensor.cast %10 : tensor<1x128xf32> to tensor<*xf32>
  %11 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %12 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %13 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  %14 = llvm.getelementptr %12[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %9, %14 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %15 = llvm.getelementptr %13[0] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c0_i64, %15 : i64, !llvm.ptr
  %16 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %11, %16 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %17 = llvm.getelementptr %13[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %17 : i64, !llvm.ptr
  %18 = call @__NS__MakeBuffer_f32(%12, %13, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Scatter(%7, %18) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %19 = call @__NS__GetTensor_f32(%c0_i64, %18) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %20 = call @__NS__NSMemrefToMemref_f32(%19) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_2 = tensor.cast %20 : tensor<*xf32> to tensor<1x128xf32>
  %21 = call @__NS__GetTensor_f32(%c1_i64, %18) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %22 = call @__NS__NSMemrefToMemref_f32(%21) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_3 = tensor.cast %22 : tensor<*xf32> to tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %23 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_4 = tensor.cast %23 : tensor<1x128xf32> to tensor<*xf32>
  %24 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %25 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_5 = tensor.cast %25 : tensor<1x128xf32> to tensor<*xf32>
  %26 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %27 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %28 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  %29 = llvm.getelementptr %27[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %24, %29 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %30 = llvm.getelementptr %28[0] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c0_i64, %30 : i64, !llvm.ptr
  %31 = llvm.getelementptr %27[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %26, %31 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %32 = llvm.getelementptr %28[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %32 : i64, !llvm.ptr
  %33 = call @__NS__MakeBuffer_f32(%27, %28, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %34 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %cast_6 = tensor.cast %34 : tensor<2x128xf32> to tensor<*xf32>
  %35 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %36 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %37 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  llvm.store %35, %36 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %37 : i64, !llvm.ptr
  %38 = call @__NS__MakeBuffer_f32(%36, %37, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Gather(%33, %38) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %39 = call @__NS__GetTensor_f32(%c0_i64, %38) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %40 = call @__NS__NSMemrefToMemref_f32(%39) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_7 = tensor.cast %40 : tensor<*xf32> to tensor<2x128xf32>
  return %cast_7 : tensor<2x128xf32>
}



//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ad4650) {
  "llvm.store"(%73, %74) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590ad40c0) {
  %74 = "llvm.alloca"(%29) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590ad5130) {
  %75 = "llvm.alloca"(%29) <{elem_type = i64}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad5f50) {
  %76 = "func.call"(%74, %75, %29) <{callee = @__NS__MakeBuffer_f32}> : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad7760) {
  %73 = "func.call"(%30, %72) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590ad3c50) {
  %72 = "tensor.cast"(%71) : (tensor<2x128xf32>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab04a0) {
  %71 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab0430) {
  "func.call"(%30) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590acf6c0) {
  "llvm.store"(%31, %69) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590ad3f20) {
  %69 = "llvm.getelementptr"(%65) <{elem_type = i64, rawConstantIndices = array<i32: 1>}> : (!llvm.ptr) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ad2660) {
  "llvm.store"(%63, %68) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590ad3e80) {
  %68 = "llvm.getelementptr"(%64) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 1>}> : (!llvm.ptr) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ad2b20) {
  "llvm.store"(%30, %67) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590ad3de0) {
  %67 = "llvm.getelementptr"(%65) <{elem_type = i64, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

} -> success : operation was folded
//===-------------------------------------------===//
** Replace : 'llvm.getelementptr'(0x5c9590ad3de0)
** Modified: 'llvm.store'(0x5c9590ad2b20)
** Erase   : 'llvm.getelementptr'(0x5c9590ad3de0)
// *** IR Dump After Successful Folding ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %0 = llvm.mlir.constant(2 : i64) : i64
  %1 = llvm.mlir.constant(1 : i64) : i64
  %c0_i64 = arith.constant 0 : i64
  %c1_i64 = arith.constant 1 : i64
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %2 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %3 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %2, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c0_i64, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %8 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_0 = tensor.cast %8 : tensor<1x128xf32> to tensor<*xf32>
  %9 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %10 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_1 = tensor.cast %10 : tensor<1x128xf32> to tensor<*xf32>
  %11 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %12 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %13 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  %14 = llvm.getelementptr %12[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %9, %14 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %15 = llvm.getelementptr %13[0] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c0_i64, %15 : i64, !llvm.ptr
  %16 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %11, %16 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %17 = llvm.getelementptr %13[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %17 : i64, !llvm.ptr
  %18 = call @__NS__MakeBuffer_f32(%12, %13, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Scatter(%7, %18) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %19 = call @__NS__GetTensor_f32(%c0_i64, %18) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %20 = call @__NS__NSMemrefToMemref_f32(%19) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_2 = tensor.cast %20 : tensor<*xf32> to tensor<1x128xf32>
  %21 = call @__NS__GetTensor_f32(%c1_i64, %18) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %22 = call @__NS__NSMemrefToMemref_f32(%21) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_3 = tensor.cast %22 : tensor<*xf32> to tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %23 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_4 = tensor.cast %23 : tensor<1x128xf32> to tensor<*xf32>
  %24 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %25 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_5 = tensor.cast %25 : tensor<1x128xf32> to tensor<*xf32>
  %26 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %27 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %28 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  %29 = llvm.getelementptr %27[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %24, %29 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %28 : i64, !llvm.ptr
  %30 = llvm.getelementptr %27[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %26, %30 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %31 = llvm.getelementptr %28[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %31 : i64, !llvm.ptr
  %32 = call @__NS__MakeBuffer_f32(%27, %28, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %33 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %cast_6 = tensor.cast %33 : tensor<2x128xf32> to tensor<*xf32>
  %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %35 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %36 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  llvm.store %34, %35 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %36 : i64, !llvm.ptr
  %37 = call @__NS__MakeBuffer_f32(%35, %36, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Gather(%32, %37) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %38 = call @__NS__GetTensor_f32(%c0_i64, %37) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %39 = call @__NS__NSMemrefToMemref_f32(%38) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_7 = tensor.cast %39 : tensor<*xf32> to tensor<2x128xf32>
  return %cast_7 : tensor<2x128xf32>
}



//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ad2b20) {
  "llvm.store"(%30, %65) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ad35f0) {
  "llvm.store"(%60, %66) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590ad3d40) {
  %66 = "llvm.getelementptr"(%64) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

} -> success : operation was folded
//===-------------------------------------------===//
** Replace : 'llvm.getelementptr'(0x5c9590ad3d40)
** Modified: 'llvm.store'(0x5c9590ad35f0)
** Erase   : 'llvm.getelementptr'(0x5c9590ad3d40)
// *** IR Dump After Successful Folding ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %0 = llvm.mlir.constant(2 : i64) : i64
  %1 = llvm.mlir.constant(1 : i64) : i64
  %c0_i64 = arith.constant 0 : i64
  %c1_i64 = arith.constant 1 : i64
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %2 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %3 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %2, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c0_i64, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %8 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_0 = tensor.cast %8 : tensor<1x128xf32> to tensor<*xf32>
  %9 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %10 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_1 = tensor.cast %10 : tensor<1x128xf32> to tensor<*xf32>
  %11 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %12 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %13 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  %14 = llvm.getelementptr %12[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %9, %14 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %15 = llvm.getelementptr %13[0] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c0_i64, %15 : i64, !llvm.ptr
  %16 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %11, %16 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %17 = llvm.getelementptr %13[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %17 : i64, !llvm.ptr
  %18 = call @__NS__MakeBuffer_f32(%12, %13, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Scatter(%7, %18) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %19 = call @__NS__GetTensor_f32(%c0_i64, %18) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %20 = call @__NS__NSMemrefToMemref_f32(%19) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_2 = tensor.cast %20 : tensor<*xf32> to tensor<1x128xf32>
  %21 = call @__NS__GetTensor_f32(%c1_i64, %18) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %22 = call @__NS__NSMemrefToMemref_f32(%21) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_3 = tensor.cast %22 : tensor<*xf32> to tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %23 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_4 = tensor.cast %23 : tensor<1x128xf32> to tensor<*xf32>
  %24 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %25 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_5 = tensor.cast %25 : tensor<1x128xf32> to tensor<*xf32>
  %26 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %27 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %28 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  llvm.store %24, %27 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %28 : i64, !llvm.ptr
  %29 = llvm.getelementptr %27[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %26, %29 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %30 = llvm.getelementptr %28[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %30 : i64, !llvm.ptr
  %31 = call @__NS__MakeBuffer_f32(%27, %28, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %32 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %cast_6 = tensor.cast %32 : tensor<2x128xf32> to tensor<*xf32>
  %33 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %34 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %35 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  llvm.store %33, %34 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %35 : i64, !llvm.ptr
  %36 = call @__NS__MakeBuffer_f32(%34, %35, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Gather(%31, %36) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %37 = call @__NS__GetTensor_f32(%c0_i64, %36) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %38 = call @__NS__NSMemrefToMemref_f32(%37) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_7 = tensor.cast %38 : tensor<*xf32> to tensor<2x128xf32>
  return %cast_7 : tensor<2x128xf32>
}



//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ad35f0) {
  "llvm.store"(%60, %64) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590acdf50) {
  %64 = "llvm.alloca"(%28) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590ace3b0) {
  %65 = "llvm.alloca"(%28) <{elem_type = i64}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad7310) {
  %68 = "func.call"(%64, %65, %28) <{callee = @__NS__MakeBuffer_f32}> : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590a4b930) {
  %63 = "func.call"(%31, %62) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590ad3190) {
  %62 = "tensor.cast"(%61) : (tensor<1x128xf32>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab78a0) {
  %61 = "func.call"(%57) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab0280) {
  "func.call"(%31) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad2a80) {
  %60 = "func.call"(%30, %59) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590ad27a0) {
  %59 = "tensor.cast"(%58) : (tensor<1x128xf32>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab9790) {
  %58 = "func.call"(%54) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab0140) {
  "func.call"(%30) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590ad25e0) {
  %57 = "tensor.cast"(%56) : (tensor<*xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad1db0) {
  %56 = "func.call"(%55) <{callee = @__NS__NSMemrefToMemref_f32}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad2090) {
  %55 = "func.call"(%31, %51) <{callee = @__NS__GetTensor_f32}> : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590acfcf0) {
  %54 = "tensor.cast"(%53) : (tensor<*xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590acfc60) {
  %53 = "func.call"(%52) <{callee = @__NS__NSMemrefToMemref_f32}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590acece0) {
  %52 = "func.call"(%30, %51) <{callee = @__NS__GetTensor_f32}> : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590acdc70) {
  "func.call"(%38, %51) <{callee = @__NS__Scatter}> : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad15e0) {
  %51 = "func.call"(%45, %46, %28) <{callee = @__NS__MakeBuffer_f32}> : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c95909d81d0) {
  "llvm.store"(%31, %50) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590acdae0) {
  %50 = "llvm.getelementptr"(%46) <{elem_type = i64, rawConstantIndices = array<i32: 1>}> : (!llvm.ptr) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c95909eba40) {
  "llvm.store"(%44, %49) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590acda40) {
  %49 = "llvm.getelementptr"(%45) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 1>}> : (!llvm.ptr) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590a00790) {
  "llvm.store"(%30, %48) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590acd930) {
  %48 = "llvm.getelementptr"(%46) <{elem_type = i64, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

} -> success : operation was folded
//===-------------------------------------------===//
** Replace : 'llvm.getelementptr'(0x5c9590acd930)
** Modified: 'llvm.store'(0x5c9590a00790)
** Erase   : 'llvm.getelementptr'(0x5c9590acd930)
// *** IR Dump After Successful Folding ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %0 = llvm.mlir.constant(2 : i64) : i64
  %1 = llvm.mlir.constant(1 : i64) : i64
  %c0_i64 = arith.constant 0 : i64
  %c1_i64 = arith.constant 1 : i64
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %2 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %3 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %2, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c0_i64, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %8 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_0 = tensor.cast %8 : tensor<1x128xf32> to tensor<*xf32>
  %9 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %10 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_1 = tensor.cast %10 : tensor<1x128xf32> to tensor<*xf32>
  %11 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %12 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %13 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  %14 = llvm.getelementptr %12[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %9, %14 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %13 : i64, !llvm.ptr
  %15 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %11, %15 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %16 = llvm.getelementptr %13[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %16 : i64, !llvm.ptr
  %17 = call @__NS__MakeBuffer_f32(%12, %13, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Scatter(%7, %17) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %18 = call @__NS__GetTensor_f32(%c0_i64, %17) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %19 = call @__NS__NSMemrefToMemref_f32(%18) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_2 = tensor.cast %19 : tensor<*xf32> to tensor<1x128xf32>
  %20 = call @__NS__GetTensor_f32(%c1_i64, %17) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %21 = call @__NS__NSMemrefToMemref_f32(%20) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_3 = tensor.cast %21 : tensor<*xf32> to tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %22 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_4 = tensor.cast %22 : tensor<1x128xf32> to tensor<*xf32>
  %23 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %24 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_5 = tensor.cast %24 : tensor<1x128xf32> to tensor<*xf32>
  %25 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %26 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %27 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  llvm.store %23, %26 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %27 : i64, !llvm.ptr
  %28 = llvm.getelementptr %26[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %25, %28 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %29 = llvm.getelementptr %27[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %29 : i64, !llvm.ptr
  %30 = call @__NS__MakeBuffer_f32(%26, %27, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %31 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %cast_6 = tensor.cast %31 : tensor<2x128xf32> to tensor<*xf32>
  %32 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %33 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %34 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  llvm.store %32, %33 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %34 : i64, !llvm.ptr
  %35 = call @__NS__MakeBuffer_f32(%33, %34, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Gather(%30, %35) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %36 = call @__NS__GetTensor_f32(%c0_i64, %35) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %37 = call @__NS__NSMemrefToMemref_f32(%36) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_7 = tensor.cast %37 : tensor<*xf32> to tensor<2x128xf32>
  return %cast_7 : tensor<2x128xf32>
}



//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590a00790) {
  "llvm.store"(%30, %46) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590acd0a0) {
  "llvm.store"(%41, %47) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590acd890) {
  %47 = "llvm.getelementptr"(%45) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

} -> success : operation was folded
//===-------------------------------------------===//
** Replace : 'llvm.getelementptr'(0x5c9590acd890)
** Modified: 'llvm.store'(0x5c9590acd0a0)
** Erase   : 'llvm.getelementptr'(0x5c9590acd890)
// *** IR Dump After Successful Folding ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %0 = llvm.mlir.constant(2 : i64) : i64
  %1 = llvm.mlir.constant(1 : i64) : i64
  %c0_i64 = arith.constant 0 : i64
  %c1_i64 = arith.constant 1 : i64
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %2 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %3 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %2, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %6 = llvm.getelementptr %4[0] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c0_i64, %6 : i64, !llvm.ptr
  %7 = call @__NS__MakeBuffer_f32(%3, %4, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %8 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_0 = tensor.cast %8 : tensor<1x128xf32> to tensor<*xf32>
  %9 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %10 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_1 = tensor.cast %10 : tensor<1x128xf32> to tensor<*xf32>
  %11 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %12 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %13 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  llvm.store %9, %12 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %13 : i64, !llvm.ptr
  %14 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %11, %14 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %15 = llvm.getelementptr %13[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %15 : i64, !llvm.ptr
  %16 = call @__NS__MakeBuffer_f32(%12, %13, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Scatter(%7, %16) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %17 = call @__NS__GetTensor_f32(%c0_i64, %16) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %18 = call @__NS__NSMemrefToMemref_f32(%17) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_2 = tensor.cast %18 : tensor<*xf32> to tensor<1x128xf32>
  %19 = call @__NS__GetTensor_f32(%c1_i64, %16) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %20 = call @__NS__NSMemrefToMemref_f32(%19) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_3 = tensor.cast %20 : tensor<*xf32> to tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %21 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_4 = tensor.cast %21 : tensor<1x128xf32> to tensor<*xf32>
  %22 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %23 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_5 = tensor.cast %23 : tensor<1x128xf32> to tensor<*xf32>
  %24 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %25 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %26 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  llvm.store %22, %25 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %26 : i64, !llvm.ptr
  %27 = llvm.getelementptr %25[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %24, %27 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %28 = llvm.getelementptr %26[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %28 : i64, !llvm.ptr
  %29 = call @__NS__MakeBuffer_f32(%25, %26, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %30 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %cast_6 = tensor.cast %30 : tensor<2x128xf32> to tensor<*xf32>
  %31 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %32 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %33 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  llvm.store %31, %32 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %33 : i64, !llvm.ptr
  %34 = call @__NS__MakeBuffer_f32(%32, %33, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Gather(%29, %34) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %35 = call @__NS__GetTensor_f32(%c0_i64, %34) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %36 = call @__NS__NSMemrefToMemref_f32(%35) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_7 = tensor.cast %36 : tensor<*xf32> to tensor<2x128xf32>
  return %cast_7 : tensor<2x128xf32>
}



//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590acd0a0) {
  "llvm.store"(%41, %45) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590aca970) {
  %46 = "llvm.alloca"(%28) <{elem_type = i64}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590acd190) {
  %45 = "llvm.alloca"(%28) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.mlir.constant'(0x5c9590acd830) {
  %28 = "llvm.mlir.constant"() <{value = 2 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590acd6c0) {
  %44 = "func.call"(%31, %43) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590acd020) {
  %43 = "tensor.cast"(%42) : (tensor<1x128xf32>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590aafd70) {
  %42 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590aafd00) {
  "func.call"(%31) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590accad0) {
  %41 = "func.call"(%30, %40) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590acc9e0) {
  %40 = "tensor.cast"(%39) : (tensor<1x128xf32>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590aaccf0) {
  %39 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590aacc80) {
  "func.call"(%30) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ac7c50) {
  %38 = "func.call"(%34, %35, %29) <{callee = @__NS__MakeBuffer_f32}> : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c95909c1310) {
  "llvm.store"(%30, %37) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590aca830) {
  %37 = "llvm.getelementptr"(%35) <{elem_type = i64, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

} -> success : operation was folded
//===-------------------------------------------===//
** Replace : 'llvm.getelementptr'(0x5c9590aca830)
** Modified: 'llvm.store'(0x5c95909c1310)
** Erase   : 'llvm.getelementptr'(0x5c9590aca830)
// *** IR Dump After Successful Folding ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %0 = llvm.mlir.constant(2 : i64) : i64
  %1 = llvm.mlir.constant(1 : i64) : i64
  %c0_i64 = arith.constant 0 : i64
  %c1_i64 = arith.constant 1 : i64
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %2 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %3 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  %5 = llvm.getelementptr %3[0] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %2, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %4 : i64, !llvm.ptr
  %6 = call @__NS__MakeBuffer_f32(%3, %4, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %7 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_0 = tensor.cast %7 : tensor<1x128xf32> to tensor<*xf32>
  %8 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %9 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_1 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
  %10 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %11 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %12 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  llvm.store %8, %11 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %12 : i64, !llvm.ptr
  %13 = llvm.getelementptr %11[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %10, %13 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %14 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %14 : i64, !llvm.ptr
  %15 = call @__NS__MakeBuffer_f32(%11, %12, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Scatter(%6, %15) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %16 = call @__NS__GetTensor_f32(%c0_i64, %15) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %17 = call @__NS__NSMemrefToMemref_f32(%16) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_2 = tensor.cast %17 : tensor<*xf32> to tensor<1x128xf32>
  %18 = call @__NS__GetTensor_f32(%c1_i64, %15) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %19 = call @__NS__NSMemrefToMemref_f32(%18) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_3 = tensor.cast %19 : tensor<*xf32> to tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %20 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_4 = tensor.cast %20 : tensor<1x128xf32> to tensor<*xf32>
  %21 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %22 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_5 = tensor.cast %22 : tensor<1x128xf32> to tensor<*xf32>
  %23 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %24 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %25 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  llvm.store %21, %24 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %25 : i64, !llvm.ptr
  %26 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %23, %26 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %27 = llvm.getelementptr %25[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %27 : i64, !llvm.ptr
  %28 = call @__NS__MakeBuffer_f32(%24, %25, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %29 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %cast_6 = tensor.cast %29 : tensor<2x128xf32> to tensor<*xf32>
  %30 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %31 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %32 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  llvm.store %30, %31 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %32 : i64, !llvm.ptr
  %33 = call @__NS__MakeBuffer_f32(%31, %32, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Gather(%28, %33) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %34 = call @__NS__GetTensor_f32(%c0_i64, %33) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %35 = call @__NS__NSMemrefToMemref_f32(%34) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_7 = tensor.cast %35 : tensor<*xf32> to tensor<2x128xf32>
  return %cast_7 : tensor<2x128xf32>
}



//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c95909c1310) {
  "llvm.store"(%30, %35) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ab0d00) {
  "llvm.store"(%33, %36) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590abea30) {
  %36 = "llvm.getelementptr"(%34) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 0>}> : (!llvm.ptr) -> !llvm.ptr

} -> success : operation was folded
//===-------------------------------------------===//
** Replace : 'llvm.getelementptr'(0x5c9590abea30)
** Modified: 'llvm.store'(0x5c9590ab0d00)
** Erase   : 'llvm.getelementptr'(0x5c9590abea30)
// *** IR Dump After Successful Folding ***
func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
  %0 = llvm.mlir.constant(2 : i64) : i64
  %1 = llvm.mlir.constant(1 : i64) : i64
  %c0_i64 = arith.constant 0 : i64
  %c1_i64 = arith.constant 1 : i64
  %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
  %2 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %3 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %4 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  llvm.store %2, %3 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %4 : i64, !llvm.ptr
  %5 = call @__NS__MakeBuffer_f32(%3, %4, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %6 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
  %cast_0 = tensor.cast %6 : tensor<1x128xf32> to tensor<*xf32>
  %7 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %8 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
  %cast_1 = tensor.cast %8 : tensor<1x128xf32> to tensor<*xf32>
  %9 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %10 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %11 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  llvm.store %7, %10 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %11 : i64, !llvm.ptr
  %12 = llvm.getelementptr %10[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %9, %12 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %13 = llvm.getelementptr %11[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %13 : i64, !llvm.ptr
  %14 = call @__NS__MakeBuffer_f32(%10, %11, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Scatter(%5, %14) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %15 = call @__NS__GetTensor_f32(%c0_i64, %14) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %16 = call @__NS__NSMemrefToMemref_f32(%15) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_2 = tensor.cast %16 : tensor<*xf32> to tensor<1x128xf32>
  %17 = call @__NS__GetTensor_f32(%c1_i64, %14) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %18 = call @__NS__NSMemrefToMemref_f32(%17) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_3 = tensor.cast %18 : tensor<*xf32> to tensor<1x128xf32>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %19 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_4 = tensor.cast %19 : tensor<1x128xf32> to tensor<*xf32>
  %20 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  call @__NS__SetDevice(%c1_i64) : (i64) -> ()
  %21 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
  %cast_5 = tensor.cast %21 : tensor<1x128xf32> to tensor<*xf32>
  %22 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %23 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %24 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
  llvm.store %20, %23 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %24 : i64, !llvm.ptr
  %25 = llvm.getelementptr %23[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
  llvm.store %22, %25 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  %26 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, i64
  llvm.store %c1_i64, %26 : i64, !llvm.ptr
  %27 = call @__NS__MakeBuffer_f32(%23, %24, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__SetDevice(%c0_i64) : (i64) -> ()
  %28 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
  %cast_6 = tensor.cast %28 : tensor<2x128xf32> to tensor<*xf32>
  %29 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %30 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
  %31 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
  llvm.store %29, %30 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
  llvm.store %c0_i64, %31 : i64, !llvm.ptr
  %32 = call @__NS__MakeBuffer_f32(%30, %31, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  call @__NS__Gather(%27, %32) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
  %33 = call @__NS__GetTensor_f32(%c0_i64, %32) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  %34 = call @__NS__NSMemrefToMemref_f32(%33) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  %cast_7 = tensor.cast %34 : tensor<*xf32> to tensor<2x128xf32>
  return %cast_7 : tensor<2x128xf32>
}



//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ab0d00) {
  "llvm.store"(%33, %34) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590abb3c0) {
  %35 = "llvm.alloca"(%29) <{elem_type = i64}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590abab30) {
  %34 = "llvm.alloca"(%29) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.mlir.constant'(0x5c9590abe760) {
  %29 = "llvm.mlir.constant"() <{value = 1 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ac0fb0) {
  %33 = "func.call"(%30, %32) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5c9590aad010) {
  %30 = "arith.constant"() <{value = 0 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5c9590aacdd0) {
  %31 = "arith.constant"() <{value = 1 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590aaf2c0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590ac92b0) {
  %32 = "tensor.cast"(%arg29) : (tensor<2x128xf32>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SideEffects::AutomaticAllocationScopeResource)

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590ad4d10) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590acfbc0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590ace7d0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590acd790) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590acb370) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590ab7430) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590ab8f20) {
  "func.return"(%17) : (tensor<1x128xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ac7ab0) {
  "linalg.yield"(%18) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c9590ac7b40) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.divf'(0x5c9590ac7a10) {
  %18 = "arith.divf"(%arg1, %arg2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ac7880) {
  "linalg.yield"(%19) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909c1c00) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x5c9590ac77e0) {
  %19 = "arith.addf"(%arg4, %arg5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c9590ad5a60) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ac7560) {
  "linalg.yield"(%arg6) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ac7350) {
  "linalg.yield"(%21) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'math.exp'(0x5c9590ab73b0) {
  %21 = "math.exp"(%20) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c9590ac73e0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.subf'(0x5c9590ac72b0) {
  %20 = "arith.subf"(%arg8, %arg9) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ac7120) {
  "linalg.yield"(%22) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909c1d50) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.maxnumf'(0x5c9590ac7080) {
  %22 = "arith.maxnumf"(%arg11, %arg12) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c9590ad7820) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ab3d20) {
  "linalg.yield"(%arg13) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab70e0) {
  %11 = "tensor.empty"() : () -> tensor<1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab7150) {
  %10 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590a0b810) {
  "linalg.yield"(%23) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909983e0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.divf'(0x5c9590ac6d60) {
  %23 = "arith.divf"(%arg15, %arg16) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590aad0c0) {
  "linalg.yield"(%24) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909c1f10) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addf'(0x5c9590ac6c00) {
  %24 = "arith.addf"(%arg18, %arg19) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909c1ab0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ab1530) {
  "linalg.yield"(%arg20) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ab42d0) {
  "linalg.yield"(%26) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'math.exp'(0x5c9590ab1ea0) {
  %26 = "math.exp"(%25) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909984f0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.subf'(0x5c9590ac6510) {
  %25 = "arith.subf"(%arg22, %arg23) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590a0c500) {
  "linalg.yield"(%27) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909cc1b0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.maxnumf'(0x5c9590ac5f40) {
  %27 = "arith.maxnumf"(%arg25, %arg26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.generic'(0x5c95909bfdf0) {

  * Pattern mlir::linalg::LinalgGeneralizationPattern : 'linalg.generic -> ()' {
Trying to match "mlir::linalg::LinalgGeneralizationPattern"
    ** Match Failure : preconditions not met
"mlir::linalg::LinalgGeneralizationPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'linalg.yield'(0x5c9590ab9d80) {
  "linalg.yield"(%arg27) : (f32) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590aae710) {
  %3 = "tensor.empty"() : () -> tensor<1xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab7070) {
  %2 = "tensor.empty"() : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5c9590ab7670) {
  %1 = "arith.constant"() <{value = -3.40282347E+38 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590ab92f0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5c9590a9a430) {
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590aaf350) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.return'(0x5c9590ab8050) {
  "func.return"(%73) : (tensor<2x128xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590ad7aa0) {
  %73 = "tensor.cast"(%72) : (tensor<*xf32>) -> tensor<2x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad6b80) {
  %72 = "func.call"(%71) <{callee = @__NS__NSMemrefToMemref_f32}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad7ee0) {
  %71 = "func.call"(%30, %70) <{callee = @__NS__GetTensor_f32}> : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad6e10) {
  "func.call"(%64, %70) <{callee = @__NS__Gather}> : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad5f50) {
  %70 = "func.call"(%68, %69, %29) <{callee = @__NS__MakeBuffer_f32}> : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c95909b9e90) {
  "llvm.store"(%30, %69) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ad4650) {
  "llvm.store"(%67, %68) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590ad5130) {
  %69 = "llvm.alloca"(%29) <{elem_type = i64}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590ad40c0) {
  %68 = "llvm.alloca"(%29) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad7760) {
  %67 = "func.call"(%30, %66) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590ad3c50) {
  %66 = "tensor.cast"(%65) : (tensor<2x128xf32>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590ab04a0) {
  %65 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<2x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab0430) {
  "func.call"(%30) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad7310) {
  %64 = "func.call"(%60, %61, %28) <{callee = @__NS__MakeBuffer_f32}> : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590acf6c0) {
  "llvm.store"(%31, %63) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590ad3f20) {
  %63 = "llvm.getelementptr"(%61) <{elem_type = i64, rawConstantIndices = array<i32: 1>}> : (!llvm.ptr) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ad2660) {
  "llvm.store"(%59, %62) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590ad3e80) {
  %62 = "llvm.getelementptr"(%60) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 1>}> : (!llvm.ptr) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ad2b20) {
  "llvm.store"(%30, %61) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ad35f0) {
  "llvm.store"(%56, %60) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590ace3b0) {
  %61 = "llvm.alloca"(%28) <{elem_type = i64}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590acdf50) {
  %60 = "llvm.alloca"(%28) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590a4b930) {
  %59 = "func.call"(%31, %58) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590ad3190) {
  %58 = "tensor.cast"(%57) : (tensor<1x128xf32>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab78a0) {
  %57 = "func.call"(%53) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab0280) {
  "func.call"(%31) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad2a80) {
  %56 = "func.call"(%30, %55) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590ad27a0) {
  %55 = "tensor.cast"(%54) : (tensor<1x128xf32>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab9790) {
  %54 = "func.call"(%50) <{callee = @softmax_1_128_softmax_1_128_fused_kernel}> {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ab0140) {
  "func.call"(%30) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590ad25e0) {
  %53 = "tensor.cast"(%52) : (tensor<*xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad1db0) {
  %52 = "func.call"(%51) <{callee = @__NS__NSMemrefToMemref_f32}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad2090) {
  %51 = "func.call"(%31, %47) <{callee = @__NS__GetTensor_f32}> : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590acfcf0) {
  %50 = "tensor.cast"(%49) : (tensor<*xf32>) -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590acfc60) {
  %49 = "func.call"(%48) <{callee = @__NS__NSMemrefToMemref_f32}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590acece0) {
  %48 = "func.call"(%30, %47) <{callee = @__NS__GetTensor_f32}> : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590acdc70) {
  "func.call"(%36, %47) <{callee = @__NS__Scatter}> : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ad15e0) {
  %47 = "func.call"(%43, %44, %28) <{callee = @__NS__MakeBuffer_f32}> : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c95909d81d0) {
  "llvm.store"(%31, %46) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590acdae0) {
  %46 = "llvm.getelementptr"(%44) <{elem_type = i64, rawConstantIndices = array<i32: 1>}> : (!llvm.ptr) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c95909eba40) {
  "llvm.store"(%42, %45) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.getelementptr'(0x5c9590acda40) {
  %45 = "llvm.getelementptr"(%43) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>, rawConstantIndices = array<i32: 1>}> : (!llvm.ptr) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590a00790) {
  "llvm.store"(%30, %44) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590acd0a0) {
  "llvm.store"(%39, %43) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590aca970) {
  %44 = "llvm.alloca"(%28) <{elem_type = i64}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590acd190) {
  %43 = "llvm.alloca"(%28) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590acd6c0) {
  %42 = "func.call"(%31, %41) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590acd020) {
  %41 = "tensor.cast"(%40) : (tensor<1x128xf32>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590aafd70) {
  %40 = "tensor.empty"() {device_id = 1 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590aafd00) {
  "func.call"(%31) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590accad0) {
  %39 = "func.call"(%30, %38) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590acc9e0) {
  %38 = "tensor.cast"(%37) : (tensor<1x128xf32>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5c9590aaccf0) {
  %37 = "tensor.empty"() {device_id = 0 : i64} : () -> tensor<1x128xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590aacc80) {
  "func.call"(%30) <{callee = @__NS__SetDevice}> : (i64) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ac7c50) {
  %36 = "func.call"(%34, %35, %29) <{callee = @__NS__MakeBuffer_f32}> : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c95909c1310) {
  "llvm.store"(%30, %35) <{ordering = 0 : i64}> : (i64, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.store'(0x5c9590ab0d00) {
  "llvm.store"(%33, %34) <{ordering = 0 : i64}> : (!llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590abb3c0) {
  %35 = "llvm.alloca"(%29) <{elem_type = i64}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.alloca'(0x5c9590abab30) {
  %34 = "llvm.alloca"(%29) <{elem_type = !llvm.struct<(i64, struct<(i64, ptr)>)>}> : (i64) -> !llvm.ptr

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.call'(0x5c9590ac0fb0) {
  %33 = "func.call"(%30, %32) <{callee = @__NS__MemrefToNSMemref_f32}> : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.cast'(0x5c9590ac92b0) {
  %32 = "tensor.cast"(%arg29) : (tensor<2x128xf32>) -> tensor<*xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5c9590aacdd0) {
  %31 = "arith.constant"() <{value = 1 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5c9590aad010) {
  %30 = "arith.constant"() <{value = 0 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.mlir.constant'(0x5c9590abe760) {
  %29 = "llvm.mlir.constant"() <{value = 1 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'func.func'(0x5c9590aaf2c0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'llvm.mlir.constant'(0x5c9590acd830) {
  %28 = "llvm.mlir.constant"() <{value = 2 : i64}> : () -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//
// -----// IR Dump After LinalgGeneralizeNamedOpsPass (linalg-generalize-named-ops) //----- //
#map = affine_map<(d0) -> ()>
#map1 = affine_map<(d0) -> (d0)>
#map2 = affine_map<(d0, d1) -> (d0, d1)>
#map3 = affine_map<(d0, d1) -> (d0)>
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = llvm.mlir.constant(2 : i64) : i64
    %1 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
    %2 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %3 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %4 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %2, %3 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %4 : i64, !llvm.ptr
    %5 = call @__NS__MakeBuffer_f32(%3, %4, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %6 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %cast_0 = tensor.cast %6 : tensor<1x128xf32> to tensor<*xf32>
    %7 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %8 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %cast_1 = tensor.cast %8 : tensor<1x128xf32> to tensor<*xf32>
    %9 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %10 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %11 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
    llvm.store %7, %10 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %11 : i64, !llvm.ptr
    %12 = llvm.getelementptr %10[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %9, %12 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %13 = llvm.getelementptr %11[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %13 : i64, !llvm.ptr
    %14 = call @__NS__MakeBuffer_f32(%10, %11, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%5, %14) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %15 = call @__NS__GetTensor_f32(%c0_i64, %14) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %16 = call @__NS__NSMemrefToMemref_f32(%15) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_2 = tensor.cast %16 : tensor<*xf32> to tensor<1x128xf32>
    %17 = call @__NS__GetTensor_f32(%c1_i64, %14) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %18 = call @__NS__NSMemrefToMemref_f32(%17) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_3 = tensor.cast %18 : tensor<*xf32> to tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %19 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %cast_4 = tensor.cast %19 : tensor<1x128xf32> to tensor<*xf32>
    %20 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %21 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %cast_5 = tensor.cast %21 : tensor<1x128xf32> to tensor<*xf32>
    %22 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %24 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
    llvm.store %20, %23 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %24 : i64, !llvm.ptr
    %25 = llvm.getelementptr %23[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %22, %25 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %26 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %26 : i64, !llvm.ptr
    %27 = call @__NS__MakeBuffer_f32(%23, %24, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %28 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_6 = tensor.cast %28 : tensor<2x128xf32> to tensor<*xf32>
    %29 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %30 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %31 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %29, %30 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %31 : i64, !llvm.ptr
    %32 = call @__NS__MakeBuffer_f32(%30, %31, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%27, %32) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %33 = call @__NS__GetTensor_f32(%c0_i64, %32) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %34 = call @__NS__NSMemrefToMemref_f32(%33) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_7 = tensor.cast %34 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_7 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}



- check conflict:
  uRead = operand 1 of %2 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

- check conflict:
  uRead = operand 0 of %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>

- check conflict:
  uRead = operand 0 of %cast_0 = tensor.cast %6 : tensor<1x128xf32> to tensor<*xf32>
  no conflict: read value has no definitions

- check conflict:
  uRead = operand 1 of %7 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  no conflict: read value has no definitions

- check conflict:
  uRead = operand 1 of %9 = func.call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  no conflict: read value has no definitions

- check conflict:
  uRead = operand 0 of %cast_1 = tensor.cast %8 : tensor<1x128xf32> to tensor<*xf32>
  no conflict: read value has no definitions

- check conflict:
  uRead = operand 0 of %cast_2 = tensor.cast %16 : tensor<*xf32> to tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %19 = func.call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %cast_3 = tensor.cast %18 : tensor<*xf32> to tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %21 = func.call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %cast_4 = tensor.cast %19 : tensor<1x128xf32> to tensor<*xf32>

- check conflict:
  uRead = operand 1 of %20 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

- check conflict:
  uRead = operand 0 of %cast_5 = tensor.cast %21 : tensor<1x128xf32> to tensor<*xf32>

- check conflict:
  uRead = operand 1 of %22 = func.call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

- check conflict:
  uRead = operand 1 of %29 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  no conflict: read value has no definitions

- check conflict:
  uRead = operand 0 of %cast_6 = tensor.cast %28 : tensor<2x128xf32> to tensor<*xf32>
  no conflict: read value has no definitions

- check conflict:
  uRead = operand 0 of func.return %cast_7 : tensor<2x128xf32>

- check conflict:
  uRead = operand 0 of %cast_7 = tensor.cast %34 : tensor<*xf32> to tensor<2x128xf32>

- check conflict:
  uRead = operand 1 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 1 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 1 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 1 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of func.return %15 : tensor<1x128xf32>
//===-------------------------------------------===//
Analyzing operand #0 of func.return %15 : tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of func.return %15 : tensor<1x128xf32>
  unConflictingWrite = operand 0 of func.return %15 : tensor<1x128xf32>

- useDominance = 1
  no conflict: read and write are same use
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 1 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #2 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of func.return %15 : tensor<1x128xf32>
  unConflictingWrite = operand 0 of func.return %15 : tensor<1x128xf32>

- useDominance = 1
  no conflict: read and write are same use
  unConflictingWrite = operand 2 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  * definition = %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
    no conflict: definition and write are same
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: definition and write are same

- check conflict:
  uRead = operand 1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read and write are same use
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 1 of %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: write happens before definition

- check conflict:
  uRead = operand 1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read and write are same use
  unConflictingWrite = operand 1 of %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>
    no conflict: definition and write are same
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 1 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #2 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 0 of func.return %15 : tensor<1x128xf32>

- useDominance = 1
  no conflict: read happens before write
  unConflictingWrite = operand 2 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  * definition = %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 2 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  no conflict: op bufferizes to element-wise access

- check conflict:
  uRead = operand 0 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 0 of func.return %15 : tensor<1x128xf32>

- useDominance = 1
  no conflict: read happens before write
  unConflictingWrite = operand 2 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  * definition = %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 2 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  no conflict: read happens before write

- check conflict:
  uRead = operand 0 of func.return %15 : tensor<1x128xf32>
  unConflictingWrite = operand 0 of func.return %15 : tensor<1x128xf32>

- useDominance = 1
  no conflict: read and write are same use
  unConflictingWrite = operand 2 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  * definition = %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
    no conflict: write happens before definition
  unConflictingWrite = operand 2 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  * definition = %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
    no conflict: definition and write are same
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read and write are same use

- check conflict:
  uRead = operand 1 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: definition and write are same
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 1 of %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: write happens before definition
  unConflictingWrite = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: write happens before definition
  unConflictingWrite = operand 1 of %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: write happens before definition

- check conflict:
  uRead = operand 1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read and write are same use
  unConflictingWrite = operand 1 of %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>
    no conflict: write happens before definition
  unConflictingWrite = operand 1 of %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>
    no conflict: write happens before definition

- check conflict:
  uRead = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read happens before write
  unConflictingWrite = operand 1 of %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read happens before write
  unConflictingWrite = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read and write are same use
  unConflictingWrite = operand 1 of %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>
    no conflict: definition and write are same

- check conflict:
  uRead = operand 1 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 1 of %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read happens before write
  unConflictingWrite = operand 1 of %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read happens before write
  unConflictingWrite = operand 1 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 1 of %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: write happens before definition
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 1 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #2 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 2 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  * definition = %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
    no conflict: definition and write are same

- check conflict:
  uRead = operand 0 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 2 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  * definition = %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
    no conflict: definition and write are same
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: definition and write are same

- check conflict:
  uRead = operand 1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read and write are same use
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 1 of %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: write happens before definition

- check conflict:
  uRead = operand 1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read and write are same use
  unConflictingWrite = operand 1 of %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>
    no conflict: definition and write are same
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 1 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #2 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 2 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  no conflict: op bufferizes to element-wise access
  unConflictingWrite = operand 2 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  * definition = %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
    no conflict: definition and write are same

- check conflict:
  uRead = operand 0 of %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 2 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  * definition = %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 2 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  * definition = %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
    no conflict: write happens before definition

- check conflict:
  uRead = operand 0 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 2 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  no conflict: read happens before write
  unConflictingWrite = operand 2 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  * definition = %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
    no conflict: definition and write are same

- check conflict:
  uRead = operand 0 of %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 2 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  * definition = %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 2 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>

- useDominance = 1
  * definition = %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
    no conflict: write happens before definition
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 0 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 1 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read and write are same use

- check conflict:
  uRead = operand 1 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 1 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: definition and write are same
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- check conflict:
  uRead = operand 1 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 1 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read and write are same use
  unConflictingWrite = operand 1 of %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read happens before write
  unConflictingWrite = operand 1 of %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read happens before write

- check conflict:
  uRead = operand 1 of %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.divf %in, %in_1 : f32
  linalg.yield %16 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 1 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: write happens before definition
  unConflictingWrite = operand 1 of %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: write happens before definition
  unConflictingWrite = operand 1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 1 of %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: write happens before definition

- check conflict:
  uRead = operand 1 of %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
^bb0(%in: f32, %in_1: f32, %out: f32):
  %16 = arith.subf %in, %in_1 : f32
  %17 = math.exp %16 : f32
  linalg.yield %17 : f32
} -> tensor<1x128xf32>
  unConflictingWrite = operand 1 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: definition and write are same
  unConflictingWrite = operand 1 of %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
    no conflict: write happens before definition
  unConflictingWrite = operand 1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read happens before write
  unConflictingWrite = operand 1 of %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read happens before write

- check conflict:
  uRead = operand 1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>
  unConflictingWrite = operand 1 of %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.maxnumf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>
    no conflict: write happens before definition
  unConflictingWrite = operand 1 of %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>
    no conflict: write happens before definition
  unConflictingWrite = operand 1 of %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  %16 = arith.addf %in, %out : f32
  linalg.yield %16 : f32
} -> tensor<1xf32>

- useDominance = 1
  no conflict: read and write are same use
  unConflictingWrite = operand 1 of %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>

- useDominance = 1
  * definition = %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
^bb0(%in: f32, %out: f32):
  linalg.yield %in : f32
} -> tensor<1xf32>
    no conflict: definition and write are same
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of func.return %cast_7 : tensor<2x128xf32>
=> NOT WRITABLE
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %cast_7 = tensor.cast %34 : tensor<*xf32> to tensor<2x128xf32>

- check conflict:
  uRead = operand 0 of func.return %cast_7 : tensor<2x128xf32>

- check conflict:
  uRead = operand 0 of %cast_7 = tensor.cast %34 : tensor<*xf32> to tensor<2x128xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %29 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

- check conflict:
  uRead = operand 1 of %29 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  no conflict: read value has no definitions
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %cast_6 = tensor.cast %28 : tensor<2x128xf32> to tensor<*xf32>

- check conflict:
  uRead = operand 1 of %29 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  no conflict: read value has no definitions

- check conflict:
  uRead = operand 0 of %cast_6 = tensor.cast %28 : tensor<2x128xf32> to tensor<*xf32>
  no conflict: read value has no definitions
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %22 = func.call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

- check conflict:
  uRead = operand 1 of %22 = func.call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  unConflictingWrite = operand 1 of %22 = func.call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

- useDominance = 1
  no conflict: read and write are same use
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %cast_5 = tensor.cast %21 : tensor<1x128xf32> to tensor<*xf32>
=> NOT WRITABLE
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %21 = func.call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
=> NOT WRITABLE
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %20 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

- check conflict:
  uRead = operand 1 of %20 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  unConflictingWrite = operand 1 of %20 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

- useDominance = 1
  no conflict: read and write are same use
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %cast_4 = tensor.cast %19 : tensor<1x128xf32> to tensor<*xf32>
=> NOT WRITABLE
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %19 = func.call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
=> NOT WRITABLE
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %cast_3 = tensor.cast %18 : tensor<*xf32> to tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %cast_3 = tensor.cast %18 : tensor<*xf32> to tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %21 = func.call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %cast_2 = tensor.cast %16 : tensor<*xf32> to tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %cast_2 = tensor.cast %16 : tensor<*xf32> to tensor<1x128xf32>

- check conflict:
  uRead = operand 0 of %19 = func.call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %9 = func.call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

- check conflict:
  uRead = operand 1 of %9 = func.call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  no conflict: read value has no definitions
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %cast_1 = tensor.cast %8 : tensor<1x128xf32> to tensor<*xf32>

- check conflict:
  uRead = operand 1 of %9 = func.call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  no conflict: read value has no definitions

- check conflict:
  uRead = operand 0 of %cast_1 = tensor.cast %8 : tensor<1x128xf32> to tensor<*xf32>
  no conflict: read value has no definitions
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %7 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

- check conflict:
  uRead = operand 1 of %7 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  no conflict: read value has no definitions
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %cast_0 = tensor.cast %6 : tensor<1x128xf32> to tensor<*xf32>

- check conflict:
  uRead = operand 0 of %cast_0 = tensor.cast %6 : tensor<1x128xf32> to tensor<*xf32>
  no conflict: read value has no definitions

- check conflict:
  uRead = operand 1 of %7 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  no conflict: read value has no definitions
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #1 of %2 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

- check conflict:
  uRead = operand 1 of %2 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  unConflictingWrite = operand 1 of %2 = func.call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>

- useDominance = 1
  no conflict: read and write are same use
//===-------------------------------------------===//
//===-------------------------------------------===//
Analyzing operand #0 of %cast = tensor.cast %arg0 : tensor<2x128xf32> to tensor<*xf32>
=> NOT WRITABLE
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::detail::AllocTensorOpGenericAdaptorBase::Properties)
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::MemRefType>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneTypedResult<mlir::MemRefType>::Impl]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::BaseMemRefType>::Impl<mlir::TypeID::get() [with Trait = mlir::OpTrait::OneTypedResult<mlir::BaseMemRefType>::Impl]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferizableOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::bufferization::BufferizableOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultShape<mlir::TypeID::get() [with Trait = mlir::OpTrait::SameOperandsAndResultShape]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultElementType<mlir::TypeID::get() [with Trait = mlir::OpTrait::SameOperandsAndResultElementType]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CopyOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::CopyOpInterface::Trait]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsElementType<mlir::TypeID::get() [with Trait = mlir::OpTrait::SameOperandsElementType]::Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsShape<mlir::TypeID::get() [with Trait = mlir::OpTrait::SameOperandsShape]::Empty>)
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = tensor.cast %3 : tensor<2x128xf32> to tensor<*xf32>
    %4 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %5 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %6 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %4, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %6 : i64, !llvm.ptr
    %7 = call @__NS__MakeBuffer_f32(%5, %6, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %8 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %cast_0 = tensor.cast %8 : tensor<1x128xf32> to tensor<*xf32>
    %9 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %10 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %cast_1 = tensor.cast %10 : tensor<1x128xf32> to tensor<*xf32>
    %11 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %12 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %13 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %9, %12 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %13 : i64, !llvm.ptr
    %14 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %11, %14 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %15 = llvm.getelementptr %13[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %15 : i64, !llvm.ptr
    %16 = call @__NS__MakeBuffer_f32(%12, %13, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%7, %16) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %17 = call @__NS__GetTensor_f32(%c0_i64, %16) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %18 = call @__NS__NSMemrefToMemref_f32(%17) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_2 = tensor.cast %18 : tensor<*xf32> to tensor<1x128xf32>
    %19 = call @__NS__GetTensor_f32(%c1_i64, %16) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %20 = call @__NS__NSMemrefToMemref_f32(%19) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_3 = tensor.cast %20 : tensor<*xf32> to tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %21 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %22 = bufferization.alloc_tensor() copy(%21) : tensor<1x128xf32>
    %cast_4 = tensor.cast %22 : tensor<1x128xf32> to tensor<*xf32>
    %23 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %24 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %25 = bufferization.alloc_tensor() copy(%24) : tensor<1x128xf32>
    %cast_5 = tensor.cast %25 : tensor<1x128xf32> to tensor<*xf32>
    %26 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %28 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %23, %27 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %28 : i64, !llvm.ptr
    %29 = llvm.getelementptr %27[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %26, %29 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %30 = llvm.getelementptr %28[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %30 : i64, !llvm.ptr
    %31 = call @__NS__MakeBuffer_f32(%27, %28, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %32 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_6 = tensor.cast %32 : tensor<2x128xf32> to tensor<*xf32>
    %33 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %34 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %35 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %33, %34 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %35 : i64, !llvm.ptr
    %36 = call @__NS__MakeBuffer_f32(%34, %35, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%31, %36) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %37 = call @__NS__GetTensor_f32(%c0_i64, %36) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %38 = call @__NS__NSMemrefToMemref_f32(%37) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_7 = tensor.cast %38 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_7 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.cast
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ViewLikeOpInterface::Trait<mlir::TypeID::get() [with Trait = mlir::ViewLikeOpInterface::Trait]::Empty>)
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = tensor.empty() {device_id = 0 : i64} : tensor<1x128xf32>
    %cast_0 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
    %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %11 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %cast_1 = tensor.cast %11 : tensor<1x128xf32> to tensor<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %13 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %14 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %10, %13 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %14 : i64, !llvm.ptr
    %15 = llvm.getelementptr %13[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %12, %15 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %16 = llvm.getelementptr %14[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %16 : i64, !llvm.ptr
    %17 = call @__NS__MakeBuffer_f32(%13, %14, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %17) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %18 = call @__NS__GetTensor_f32(%c0_i64, %17) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %19 = call @__NS__NSMemrefToMemref_f32(%18) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_2 = tensor.cast %19 : tensor<*xf32> to tensor<1x128xf32>
    %20 = call @__NS__GetTensor_f32(%c1_i64, %17) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %21 = call @__NS__NSMemrefToMemref_f32(%20) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_3 = tensor.cast %21 : tensor<*xf32> to tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %22 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %23 = bufferization.alloc_tensor() copy(%22) : tensor<1x128xf32>
    %cast_4 = tensor.cast %23 : tensor<1x128xf32> to tensor<*xf32>
    %24 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %25 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %26 = bufferization.alloc_tensor() copy(%25) : tensor<1x128xf32>
    %cast_5 = tensor.cast %26 : tensor<1x128xf32> to tensor<*xf32>
    %27 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %28 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %29 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %24, %28 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %29 : i64, !llvm.ptr
    %30 = llvm.getelementptr %28[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %27, %30 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %31 = llvm.getelementptr %29[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %31 : i64, !llvm.ptr
    %32 = call @__NS__MakeBuffer_f32(%28, %29, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %33 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_6 = tensor.cast %33 : tensor<2x128xf32> to tensor<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %35 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %36 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %35 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %36 : i64, !llvm.ptr
    %37 = call @__NS__MakeBuffer_f32(%35, %36, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%32, %37) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %38 = call @__NS__GetTensor_f32(%c0_i64, %37) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %39 = call @__NS__NSMemrefToMemref_f32(%38) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_7 = tensor.cast %39 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_7 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.empty
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %cast_0 = tensor.cast %9 : tensor<1x128xf32> to tensor<*xf32>
    %10 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_0) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %11 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %cast_1 = tensor.cast %11 : tensor<1x128xf32> to tensor<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %13 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %14 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %10, %13 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %14 : i64, !llvm.ptr
    %15 = llvm.getelementptr %13[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %12, %15 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %16 = llvm.getelementptr %14[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %16 : i64, !llvm.ptr
    %17 = call @__NS__MakeBuffer_f32(%13, %14, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %17) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %18 = call @__NS__GetTensor_f32(%c0_i64, %17) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %19 = call @__NS__NSMemrefToMemref_f32(%18) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_2 = tensor.cast %19 : tensor<*xf32> to tensor<1x128xf32>
    %20 = call @__NS__GetTensor_f32(%c1_i64, %17) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %21 = call @__NS__NSMemrefToMemref_f32(%20) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_3 = tensor.cast %21 : tensor<*xf32> to tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %22 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %23 = bufferization.alloc_tensor() copy(%22) : tensor<1x128xf32>
    %cast_4 = tensor.cast %23 : tensor<1x128xf32> to tensor<*xf32>
    %24 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %25 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %26 = bufferization.alloc_tensor() copy(%25) : tensor<1x128xf32>
    %cast_5 = tensor.cast %26 : tensor<1x128xf32> to tensor<*xf32>
    %27 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %28 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %29 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %24, %28 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %29 : i64, !llvm.ptr
    %30 = llvm.getelementptr %28[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %27, %30 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %31 = llvm.getelementptr %29[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %31 : i64, !llvm.ptr
    %32 = call @__NS__MakeBuffer_f32(%28, %29, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %33 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_6 = tensor.cast %33 : tensor<2x128xf32> to tensor<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %35 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %36 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %35 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %36 : i64, !llvm.ptr
    %37 = call @__NS__MakeBuffer_f32(%35, %36, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%32, %37) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %38 = call @__NS__GetTensor_f32(%c0_i64, %37) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %39 = call @__NS__NSMemrefToMemref_f32(%38) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_7 = tensor.cast %39 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_7 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.cast
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = tensor.empty() {device_id = 1 : i64} : tensor<1x128xf32>
    %cast_1 = tensor.cast %13 : tensor<1x128xf32> to tensor<*xf32>
    %14 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %15 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %16 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %15 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %16 : i64, !llvm.ptr
    %17 = llvm.getelementptr %15[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %14, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %18 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %18 : i64, !llvm.ptr
    %19 = call @__NS__MakeBuffer_f32(%15, %16, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %19) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %20 = call @__NS__GetTensor_f32(%c0_i64, %19) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %21 = call @__NS__NSMemrefToMemref_f32(%20) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_2 = tensor.cast %21 : tensor<*xf32> to tensor<1x128xf32>
    %22 = call @__NS__GetTensor_f32(%c1_i64, %19) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_3 = tensor.cast %23 : tensor<*xf32> to tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %24 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %25 = bufferization.alloc_tensor() copy(%24) : tensor<1x128xf32>
    %cast_4 = tensor.cast %25 : tensor<1x128xf32> to tensor<*xf32>
    %26 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %27 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %28 = bufferization.alloc_tensor() copy(%27) : tensor<1x128xf32>
    %cast_5 = tensor.cast %28 : tensor<1x128xf32> to tensor<*xf32>
    %29 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %30 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %31 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %26, %30 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %31 : i64, !llvm.ptr
    %32 = llvm.getelementptr %30[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %29, %32 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %33 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %33 : i64, !llvm.ptr
    %34 = call @__NS__MakeBuffer_f32(%30, %31, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %35 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_6 = tensor.cast %35 : tensor<2x128xf32> to tensor<*xf32>
    %36 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %37 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %38 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %36, %37 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %38 : i64, !llvm.ptr
    %39 = call @__NS__MakeBuffer_f32(%37, %38, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%34, %39) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %40 = call @__NS__GetTensor_f32(%c0_i64, %39) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %41 = call @__NS__NSMemrefToMemref_f32(%40) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_7 = tensor.cast %41 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_7 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.empty
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %cast_1 = tensor.cast %13 : tensor<1x128xf32> to tensor<*xf32>
    %14 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_1) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %15 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %16 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %15 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %16 : i64, !llvm.ptr
    %17 = llvm.getelementptr %15[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %14, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %18 = llvm.getelementptr %16[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %18 : i64, !llvm.ptr
    %19 = call @__NS__MakeBuffer_f32(%15, %16, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %19) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %20 = call @__NS__GetTensor_f32(%c0_i64, %19) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %21 = call @__NS__NSMemrefToMemref_f32(%20) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_2 = tensor.cast %21 : tensor<*xf32> to tensor<1x128xf32>
    %22 = call @__NS__GetTensor_f32(%c1_i64, %19) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_3 = tensor.cast %23 : tensor<*xf32> to tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %24 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %25 = bufferization.alloc_tensor() copy(%24) : tensor<1x128xf32>
    %cast_4 = tensor.cast %25 : tensor<1x128xf32> to tensor<*xf32>
    %26 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %27 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %28 = bufferization.alloc_tensor() copy(%27) : tensor<1x128xf32>
    %cast_5 = tensor.cast %28 : tensor<1x128xf32> to tensor<*xf32>
    %29 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %30 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %31 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %26, %30 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %31 : i64, !llvm.ptr
    %32 = llvm.getelementptr %30[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %29, %32 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %33 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %33 : i64, !llvm.ptr
    %34 = call @__NS__MakeBuffer_f32(%30, %31, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %35 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_6 = tensor.cast %35 : tensor<2x128xf32> to tensor<*xf32>
    %36 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %37 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %38 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %36, %37 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %38 : i64, !llvm.ptr
    %39 = call @__NS__MakeBuffer_f32(%37, %38, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%34, %39) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %40 = call @__NS__GetTensor_f32(%c0_i64, %39) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %41 = call @__NS__NSMemrefToMemref_f32(%40) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_7 = tensor.cast %41 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_7 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.cast
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_2 = tensor.cast %23 : tensor<*xf32> to tensor<1x128xf32>
    %24 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %25 = call @__NS__NSMemrefToMemref_f32(%24) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_3 = tensor.cast %25 : tensor<*xf32> to tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %26 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_2) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %27 = bufferization.alloc_tensor() copy(%26) : tensor<1x128xf32>
    %cast_4 = tensor.cast %27 : tensor<1x128xf32> to tensor<*xf32>
    %28 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %29 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %30 = bufferization.alloc_tensor() copy(%29) : tensor<1x128xf32>
    %cast_5 = tensor.cast %30 : tensor<1x128xf32> to tensor<*xf32>
    %31 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %32 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %33 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %28, %32 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %33 : i64, !llvm.ptr
    %34 = llvm.getelementptr %32[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %31, %34 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %35 = llvm.getelementptr %33[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %35 : i64, !llvm.ptr
    %36 = call @__NS__MakeBuffer_f32(%32, %33, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %37 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_6 = tensor.cast %37 : tensor<2x128xf32> to tensor<*xf32>
    %38 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %39 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %40 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %38, %39 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %40 : i64, !llvm.ptr
    %41 = call @__NS__MakeBuffer_f32(%39, %40, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%36, %41) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %42 = call @__NS__GetTensor_f32(%c0_i64, %41) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %43 = call @__NS__NSMemrefToMemref_f32(%42) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_7 = tensor.cast %43 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_7 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.cast
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_3 = tensor.cast %27 : tensor<*xf32> to tensor<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %28 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %29 = bufferization.alloc_tensor() copy(%28) : tensor<1x128xf32>
    %cast_4 = tensor.cast %29 : tensor<1x128xf32> to tensor<*xf32>
    %30 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %31 = call @softmax_1_128_softmax_1_128_fused_kernel(%cast_3) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %32 = bufferization.alloc_tensor() copy(%31) : tensor<1x128xf32>
    %cast_5 = tensor.cast %32 : tensor<1x128xf32> to tensor<*xf32>
    %33 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %34 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %35 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %30, %34 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %35 : i64, !llvm.ptr
    %36 = llvm.getelementptr %34[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %33, %36 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %37 = llvm.getelementptr %35[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %37 : i64, !llvm.ptr
    %38 = call @__NS__MakeBuffer_f32(%34, %35, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %39 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_6 = tensor.cast %39 : tensor<2x128xf32> to tensor<*xf32>
    %40 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %41 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %42 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %40, %41 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %42 : i64, !llvm.ptr
    %43 = call @__NS__MakeBuffer_f32(%41, %42, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%38, %43) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %44 = call @__NS__GetTensor_f32(%c0_i64, %43) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %45 = call @__NS__NSMemrefToMemref_f32(%44) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_7 = tensor.cast %45 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_7 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.cast
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.alloc_tensor() copy(%30) : tensor<1x128xf32>
    %cast_4 = tensor.cast %31 : tensor<1x128xf32> to tensor<*xf32>
    %32 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %33 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %34 = bufferization.alloc_tensor() copy(%33) : tensor<1x128xf32>
    %cast_5 = tensor.cast %34 : tensor<1x128xf32> to tensor<*xf32>
    %35 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %36 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %37 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %32, %36 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %37 : i64, !llvm.ptr
    %38 = llvm.getelementptr %36[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %35, %38 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %39 = llvm.getelementptr %37[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %39 : i64, !llvm.ptr
    %40 = call @__NS__MakeBuffer_f32(%36, %37, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %41 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_6 = tensor.cast %41 : tensor<2x128xf32> to tensor<*xf32>
    %42 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %43 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %44 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %42, %43 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %44 : i64, !llvm.ptr
    %45 = call @__NS__MakeBuffer_f32(%43, %44, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%40, %45) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %46 = call @__NS__GetTensor_f32(%c0_i64, %45) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %47 = call @__NS__NSMemrefToMemref_f32(%46) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_7 = tensor.cast %47 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_7 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = tensor.cast %32 : tensor<1x128xf32> to tensor<*xf32>
    %33 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_5) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %34 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %35 = bufferization.alloc_tensor() copy(%34) : tensor<1x128xf32>
    %cast_6 = tensor.cast %35 : tensor<1x128xf32> to tensor<*xf32>
    %36 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %37 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %38 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %33, %37 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %38 : i64, !llvm.ptr
    %39 = llvm.getelementptr %37[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %36, %39 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %40 = llvm.getelementptr %38[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %40 : i64, !llvm.ptr
    %41 = call @__NS__MakeBuffer_f32(%37, %38, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %42 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_7 = tensor.cast %42 : tensor<2x128xf32> to tensor<*xf32>
    %43 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_7) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %44 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %45 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %43, %44 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %45 : i64, !llvm.ptr
    %46 = call @__NS__MakeBuffer_f32(%44, %45, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%41, %46) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %47 = call @__NS__GetTensor_f32(%c0_i64, %46) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %48 = call @__NS__NSMemrefToMemref_f32(%47) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_8 = tensor.cast %48 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_8 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.cast
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.alloc_tensor() copy(%35) : tensor<1x128xf32>
    %cast_6 = tensor.cast %36 : tensor<1x128xf32> to tensor<*xf32>
    %37 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_6) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %38 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %39 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %38 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %39 : i64, !llvm.ptr
    %40 = llvm.getelementptr %38[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %37, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %41 = llvm.getelementptr %39[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %41 : i64, !llvm.ptr
    %42 = call @__NS__MakeBuffer_f32(%38, %39, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %43 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_7 = tensor.cast %43 : tensor<2x128xf32> to tensor<*xf32>
    %44 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_7) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %45 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %46 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %44, %45 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %46 : i64, !llvm.ptr
    %47 = call @__NS__MakeBuffer_f32(%45, %46, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%42, %47) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %48 = call @__NS__GetTensor_f32(%c0_i64, %47) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = call @__NS__NSMemrefToMemref_f32(%48) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_8 = tensor.cast %49 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_8 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = tensor.cast %37 : tensor<1x128xf32> to tensor<*xf32>
    %38 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_7) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %39 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %40 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %39 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %40 : i64, !llvm.ptr
    %41 = llvm.getelementptr %39[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %38, %41 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %42 : i64, !llvm.ptr
    %43 = call @__NS__MakeBuffer_f32(%39, %40, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %44 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_8 = tensor.cast %44 : tensor<2x128xf32> to tensor<*xf32>
    %45 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_8) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %46 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %47 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %45, %46 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %47 : i64, !llvm.ptr
    %48 = call @__NS__MakeBuffer_f32(%46, %47, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%43, %48) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %49 = call @__NS__GetTensor_f32(%c0_i64, %48) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %50 = call @__NS__NSMemrefToMemref_f32(%49) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_9 = tensor.cast %50 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_9 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.cast
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = tensor.empty() {device_id = 0 : i64} : tensor<2x128xf32>
    %cast_8 = tensor.cast %45 : tensor<2x128xf32> to tensor<*xf32>
    %46 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_8) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %47 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %48 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %46, %47 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %48 : i64, !llvm.ptr
    %49 = call @__NS__MakeBuffer_f32(%47, %48, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %49) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %50 = call @__NS__GetTensor_f32(%c0_i64, %49) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %51 = call @__NS__NSMemrefToMemref_f32(%50) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_9 = tensor.cast %51 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_9 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.empty
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %cast_8 = tensor.cast %45 : tensor<2x128xf32> to tensor<*xf32>
    %46 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_8) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %47 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %48 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %46, %47 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %48 : i64, !llvm.ptr
    %49 = call @__NS__MakeBuffer_f32(%47, %48, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %49) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %50 = call @__NS__GetTensor_f32(%c0_i64, %49) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %51 = call @__NS__NSMemrefToMemref_f32(%50) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_9 = tensor.cast %51 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_9 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.cast
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %cast_9 = tensor.cast %53 : tensor<*xf32> to tensor<2x128xf32>
    return %cast_9 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.cast
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = tensor.empty() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.empty
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %1 = tensor.empty() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.empty
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %1 = bufferization.alloc_tensor() : tensor<1xf32>
    %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %3 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x128xf32>) outs(%5 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    %8 = tensor.empty() : tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.maxnumf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.subf %in, %in_1 : f32
      %17 = math.exp %16 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.addf %in, %out : f32
      linalg.yield %16 : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %14 : tensor<1x128xf32>, tensor<1xf32>) outs(%8 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %16 = arith.divf %in, %in_1 : f32
      linalg.yield %16 : f32
    } -> tensor<1x128xf32>
    return %15 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %0 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %1 = bufferization.alloc_tensor() : tensor<1xf32>
    %2 = bufferization.to_memref %1 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%2 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %3 = bufferization.to_tensor %2 : memref<1xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%arg0 : tensor<1x128xf32>) outs(%3 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %17 = arith.maxnumf %in, %out : f32
      linalg.yield %17 : f32
    } -> tensor<1xf32>
    %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %4 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %17 = arith.subf %in, %in_1 : f32
      %18 = math.exp %17 : f32
      linalg.yield %18 : f32
    } -> tensor<1x128xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%1 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x128xf32>) outs(%6 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %17 = arith.addf %in, %out : f32
      linalg.yield %17 : f32
    } -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %7 : tensor<1x128xf32>, tensor<1xf32>) outs(%0 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %17 = arith.divf %in, %in_1 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    %9 = tensor.empty() : tensor<1x128xf32>
    %10 = tensor.empty() : tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%8 : tensor<1x128xf32>) outs(%11 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %17 = arith.maxnumf %in, %out : f32
      linalg.yield %17 : f32
    } -> tensor<1xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%8, %12 : tensor<1x128xf32>, tensor<1xf32>) outs(%9 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %17 = arith.subf %in, %in_1 : f32
      %18 = math.exp %17 : f32
      linalg.yield %18 : f32
    } -> tensor<1x128xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%13 : tensor<1x128xf32>) outs(%14 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %17 = arith.addf %in, %out : f32
      linalg.yield %17 : f32
    } -> tensor<1xf32>
    %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%13, %15 : tensor<1x128xf32>, tensor<1xf32>) outs(%9 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %17 = arith.divf %in, %in_1 : f32
      linalg.yield %17 : f32
    } -> tensor<1x128xf32>
    return %16 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %1 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %2 = bufferization.alloc_tensor() : tensor<1xf32>
    %3 = bufferization.to_memref %2 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%3 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %4 = bufferization.to_tensor %3 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%0 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%3 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %18 = arith.maxnumf %in, %out : f32
      linalg.yield %18 : f32
    }
    %5 = bufferization.to_tensor %3 : memref<1xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %5 : tensor<1x128xf32>, tensor<1xf32>) outs(%1 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %18 = arith.subf %in, %in_1 : f32
      %19 = math.exp %18 : f32
      linalg.yield %19 : f32
    } -> tensor<1x128xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%2 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%6 : tensor<1x128xf32>) outs(%7 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %18 = arith.addf %in, %out : f32
      linalg.yield %18 : f32
    } -> tensor<1xf32>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%6, %8 : tensor<1x128xf32>, tensor<1xf32>) outs(%1 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %18 = arith.divf %in, %in_1 : f32
      linalg.yield %18 : f32
    } -> tensor<1x128xf32>
    %10 = tensor.empty() : tensor<1x128xf32>
    %11 = tensor.empty() : tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%11 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x128xf32>) outs(%12 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %18 = arith.maxnumf %in, %out : f32
      linalg.yield %18 : f32
    } -> tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %13 : tensor<1x128xf32>, tensor<1xf32>) outs(%10 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %18 = arith.subf %in, %in_1 : f32
      %19 = math.exp %18 : f32
      linalg.yield %19 : f32
    } -> tensor<1x128xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%11 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%14 : tensor<1x128xf32>) outs(%15 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %18 = arith.addf %in, %out : f32
      linalg.yield %18 : f32
    } -> tensor<1xf32>
    %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%14, %16 : tensor<1x128xf32>, tensor<1xf32>) outs(%10 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %18 = arith.divf %in, %in_1 : f32
      linalg.yield %18 : f32
    } -> tensor<1x128xf32>
    return %17 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.alloc_tensor() : tensor<1xf32>
    %5 = bufferization.to_memref %4 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%5 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %6 = bufferization.to_tensor %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%5 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.maxnumf %in, %out : f32
      linalg.yield %20 : f32
    }
    %7 = bufferization.to_tensor %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %5 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %20 = arith.subf %in, %in_1 : f32
      %21 = math.exp %20 : f32
      linalg.yield %21 : f32
    }
    %8 = bufferization.to_tensor %3 : memref<1x128xf32>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%4 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%8 : tensor<1x128xf32>) outs(%9 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.addf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%8, %10 : tensor<1x128xf32>, tensor<1xf32>) outs(%2 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %20 = arith.divf %in, %in_1 : f32
      linalg.yield %20 : f32
    } -> tensor<1x128xf32>
    %12 = tensor.empty() : tensor<1x128xf32>
    %13 = tensor.empty() : tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%11 : tensor<1x128xf32>) outs(%14 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.maxnumf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11, %15 : tensor<1x128xf32>, tensor<1xf32>) outs(%12 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %20 = arith.subf %in, %in_1 : f32
      %21 = math.exp %20 : f32
      linalg.yield %21 : f32
    } -> tensor<1x128xf32>
    %17 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%13 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%16 : tensor<1x128xf32>) outs(%17 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.addf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16, %18 : tensor<1x128xf32>, tensor<1xf32>) outs(%12 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %20 = arith.divf %in, %in_1 : f32
      linalg.yield %20 : f32
    } -> tensor<1x128xf32>
    return %19 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.alloc_tensor() : tensor<1xf32>
    %5 = bufferization.to_memref %4 : memref<1xf32>
    %6 = bufferization.to_memref %4 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %7 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %21 = arith.maxnumf %in, %out : f32
      linalg.yield %21 : f32
    }
    %8 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %6 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %21 = arith.subf %in, %in_1 : f32
      %22 = math.exp %21 : f32
      linalg.yield %22 : f32
    }
    %9 = bufferization.to_tensor %3 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%5 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %10 = bufferization.to_tensor %5 : memref<1xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x128xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %21 = arith.addf %in, %out : f32
      linalg.yield %21 : f32
    } -> tensor<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%2 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %21 = arith.divf %in, %in_1 : f32
      linalg.yield %21 : f32
    } -> tensor<1x128xf32>
    %13 = tensor.empty() : tensor<1x128xf32>
    %14 = tensor.empty() : tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%14 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%15 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %21 = arith.maxnumf %in, %out : f32
      linalg.yield %21 : f32
    } -> tensor<1xf32>
    %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %16 : tensor<1x128xf32>, tensor<1xf32>) outs(%13 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %21 = arith.subf %in, %in_1 : f32
      %22 = math.exp %21 : f32
      linalg.yield %22 : f32
    } -> tensor<1x128xf32>
    %18 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%14 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%17 : tensor<1x128xf32>) outs(%18 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %21 = arith.addf %in, %out : f32
      linalg.yield %21 : f32
    } -> tensor<1xf32>
    %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%17, %19 : tensor<1x128xf32>, tensor<1xf32>) outs(%13 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %21 = arith.divf %in, %in_1 : f32
      linalg.yield %21 : f32
    } -> tensor<1x128xf32>
    return %20 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.alloc_tensor() : tensor<1xf32>
    %5 = bufferization.to_memref %4 : memref<1xf32>
    %6 = bufferization.to_memref %4 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %7 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %21 = arith.maxnumf %in, %out : f32
      linalg.yield %21 : f32
    }
    %8 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %6 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %21 = arith.subf %in, %in_1 : f32
      %22 = math.exp %21 : f32
      linalg.yield %22 : f32
    }
    %9 = bufferization.to_tensor %3 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%5 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %10 = bufferization.to_tensor %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%3 : memref<1x128xf32>) outs(%5 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %21 = arith.addf %in, %out : f32
      linalg.yield %21 : f32
    }
    %11 = bufferization.to_tensor %5 : memref<1xf32>
    %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %11 : tensor<1x128xf32>, tensor<1xf32>) outs(%2 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %21 = arith.divf %in, %in_1 : f32
      linalg.yield %21 : f32
    } -> tensor<1x128xf32>
    %13 = tensor.empty() : tensor<1x128xf32>
    %14 = tensor.empty() : tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%14 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x128xf32>) outs(%15 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %21 = arith.maxnumf %in, %out : f32
      linalg.yield %21 : f32
    } -> tensor<1xf32>
    %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %16 : tensor<1x128xf32>, tensor<1xf32>) outs(%13 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %21 = arith.subf %in, %in_1 : f32
      %22 = math.exp %21 : f32
      linalg.yield %22 : f32
    } -> tensor<1x128xf32>
    %18 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%14 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%17 : tensor<1x128xf32>) outs(%18 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %21 = arith.addf %in, %out : f32
      linalg.yield %21 : f32
    } -> tensor<1xf32>
    %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%17, %19 : tensor<1x128xf32>, tensor<1xf32>) outs(%13 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %21 = arith.divf %in, %in_1 : f32
      linalg.yield %21 : f32
    } -> tensor<1x128xf32>
    return %20 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %5 = bufferization.alloc_tensor() : tensor<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %22 = arith.maxnumf %in, %out : f32
      linalg.yield %22 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %22 = arith.subf %in, %in_1 : f32
      %23 = math.exp %22 : f32
      linalg.yield %23 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %22 = arith.addf %in, %out : f32
      linalg.yield %22 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %22 = arith.divf %in, %in_1 : f32
      linalg.yield %22 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = tensor.empty() : tensor<1x128xf32>
    %15 = tensor.empty() : tensor<1xf32>
    %16 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%15 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%13 : tensor<1x128xf32>) outs(%16 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %22 = arith.maxnumf %in, %out : f32
      linalg.yield %22 : f32
    } -> tensor<1xf32>
    %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%13, %17 : tensor<1x128xf32>, tensor<1xf32>) outs(%14 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %22 = arith.subf %in, %in_1 : f32
      %23 = math.exp %22 : f32
      linalg.yield %23 : f32
    } -> tensor<1x128xf32>
    %19 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%15 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%18 : tensor<1x128xf32>) outs(%19 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %22 = arith.addf %in, %out : f32
      linalg.yield %22 : f32
    } -> tensor<1xf32>
    %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%18, %20 : tensor<1x128xf32>, tensor<1xf32>) outs(%14 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %22 = arith.divf %in, %in_1 : f32
      linalg.yield %22 : f32
    } -> tensor<1x128xf32>
    return %21 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.empty
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %5 = bufferization.alloc_tensor() : tensor<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %22 = arith.maxnumf %in, %out : f32
      linalg.yield %22 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %22 = arith.subf %in, %in_1 : f32
      %23 = math.exp %22 : f32
      linalg.yield %23 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %22 = arith.addf %in, %out : f32
      linalg.yield %22 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %22 = arith.divf %in, %in_1 : f32
      linalg.yield %22 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %15 = tensor.empty() : tensor<1xf32>
    %16 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%15 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%13 : tensor<1x128xf32>) outs(%16 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %22 = arith.maxnumf %in, %out : f32
      linalg.yield %22 : f32
    } -> tensor<1xf32>
    %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%13, %17 : tensor<1x128xf32>, tensor<1xf32>) outs(%14 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %22 = arith.subf %in, %in_1 : f32
      %23 = math.exp %22 : f32
      linalg.yield %23 : f32
    } -> tensor<1x128xf32>
    %19 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%15 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%18 : tensor<1x128xf32>) outs(%19 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %22 = arith.addf %in, %out : f32
      linalg.yield %22 : f32
    } -> tensor<1xf32>
    %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%18, %20 : tensor<1x128xf32>, tensor<1xf32>) outs(%14 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %22 = arith.divf %in, %in_1 : f32
      linalg.yield %22 : f32
    } -> tensor<1x128xf32>
    return %21 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: tensor.empty
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %5 = bufferization.alloc_tensor() : tensor<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %22 = arith.maxnumf %in, %out : f32
      linalg.yield %22 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %22 = arith.subf %in, %in_1 : f32
      %23 = math.exp %22 : f32
      linalg.yield %23 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %22 = arith.addf %in, %out : f32
      linalg.yield %22 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %22 = arith.divf %in, %in_1 : f32
      linalg.yield %22 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %15 = bufferization.alloc_tensor() : tensor<1xf32>
    %16 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%15 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%13 : tensor<1x128xf32>) outs(%16 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %22 = arith.maxnumf %in, %out : f32
      linalg.yield %22 : f32
    } -> tensor<1xf32>
    %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%13, %17 : tensor<1x128xf32>, tensor<1xf32>) outs(%14 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %22 = arith.subf %in, %in_1 : f32
      %23 = math.exp %22 : f32
      linalg.yield %23 : f32
    } -> tensor<1x128xf32>
    %19 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%15 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%18 : tensor<1x128xf32>) outs(%19 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %22 = arith.addf %in, %out : f32
      linalg.yield %22 : f32
    } -> tensor<1xf32>
    %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%18, %20 : tensor<1x128xf32>, tensor<1xf32>) outs(%14 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %22 = arith.divf %in, %in_1 : f32
      linalg.yield %22 : f32
    } -> tensor<1x128xf32>
    return %21 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %5 = bufferization.alloc_tensor() : tensor<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %23 = arith.maxnumf %in, %out : f32
      linalg.yield %23 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %23 = arith.subf %in, %in_1 : f32
      %24 = math.exp %23 : f32
      linalg.yield %24 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %23 = arith.addf %in, %out : f32
      linalg.yield %23 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %23 = arith.divf %in, %in_1 : f32
      linalg.yield %23 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %15 = bufferization.alloc_tensor() : tensor<1xf32>
    %16 = bufferization.to_memref %15 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%16 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %17 = bufferization.to_tensor %16 : memref<1xf32>
    %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%13 : tensor<1x128xf32>) outs(%17 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %23 = arith.maxnumf %in, %out : f32
      linalg.yield %23 : f32
    } -> tensor<1xf32>
    %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%13, %18 : tensor<1x128xf32>, tensor<1xf32>) outs(%14 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %23 = arith.subf %in, %in_1 : f32
      %24 = math.exp %23 : f32
      linalg.yield %24 : f32
    } -> tensor<1x128xf32>
    %20 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%15 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%19 : tensor<1x128xf32>) outs(%20 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %23 = arith.addf %in, %out : f32
      linalg.yield %23 : f32
    } -> tensor<1xf32>
    %22 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%19, %21 : tensor<1x128xf32>, tensor<1xf32>) outs(%14 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %23 = arith.divf %in, %in_1 : f32
      linalg.yield %23 : f32
    } -> tensor<1x128xf32>
    return %22 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %5 = bufferization.alloc_tensor() : tensor<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %23 = arith.maxnumf %in, %out : f32
      linalg.yield %23 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %23 = arith.subf %in, %in_1 : f32
      %24 = math.exp %23 : f32
      linalg.yield %24 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %23 = arith.addf %in, %out : f32
      linalg.yield %23 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %23 = arith.divf %in, %in_1 : f32
      linalg.yield %23 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %15 = bufferization.alloc_tensor() : tensor<1xf32>
    %16 = bufferization.to_memref %15 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%16 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %17 = bufferization.to_tensor %16 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%3 : memref<1x128xf32>) outs(%16 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %23 = arith.maxnumf %in, %out : f32
      linalg.yield %23 : f32
    }
    %18 = bufferization.to_tensor %16 : memref<1xf32>
    %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%13, %18 : tensor<1x128xf32>, tensor<1xf32>) outs(%14 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %23 = arith.subf %in, %in_1 : f32
      %24 = math.exp %23 : f32
      linalg.yield %24 : f32
    } -> tensor<1x128xf32>
    %20 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%15 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%19 : tensor<1x128xf32>) outs(%20 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %23 = arith.addf %in, %out : f32
      linalg.yield %23 : f32
    } -> tensor<1xf32>
    %22 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%19, %21 : tensor<1x128xf32>, tensor<1xf32>) outs(%14 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %23 = arith.divf %in, %in_1 : f32
      linalg.yield %23 : f32
    } -> tensor<1x128xf32>
    return %22 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %5 = bufferization.alloc_tensor() : tensor<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %24 = arith.maxnumf %in, %out : f32
      linalg.yield %24 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %24 = arith.subf %in, %in_1 : f32
      %25 = math.exp %24 : f32
      linalg.yield %25 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %24 = arith.addf %in, %out : f32
      linalg.yield %24 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %24 = arith.divf %in, %in_1 : f32
      linalg.yield %24 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %15 = bufferization.to_memref %14 : memref<1x128xf32>
    %16 = bufferization.alloc_tensor() : tensor<1xf32>
    %17 = bufferization.to_memref %16 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%17 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %18 = bufferization.to_tensor %17 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%3 : memref<1x128xf32>) outs(%17 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %24 = arith.maxnumf %in, %out : f32
      linalg.yield %24 : f32
    }
    %19 = bufferization.to_tensor %17 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %17 : memref<1x128xf32>, memref<1xf32>) outs(%15 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %24 = arith.subf %in, %in_1 : f32
      %25 = math.exp %24 : f32
      linalg.yield %25 : f32
    }
    %20 = bufferization.to_tensor %15 : memref<1x128xf32>
    %21 = linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%16 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1xf32>
    %22 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%20 : tensor<1x128xf32>) outs(%21 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %24 = arith.addf %in, %out : f32
      linalg.yield %24 : f32
    } -> tensor<1xf32>
    %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%20, %22 : tensor<1x128xf32>, tensor<1xf32>) outs(%14 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %24 = arith.divf %in, %in_1 : f32
      linalg.yield %24 : f32
    } -> tensor<1x128xf32>
    return %23 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %5 = bufferization.alloc_tensor() : tensor<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %25 = arith.maxnumf %in, %out : f32
      linalg.yield %25 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %25 = arith.subf %in, %in_1 : f32
      %26 = math.exp %25 : f32
      linalg.yield %26 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %25 = arith.addf %in, %out : f32
      linalg.yield %25 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %25 = arith.divf %in, %in_1 : f32
      linalg.yield %25 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %15 = bufferization.to_memref %14 : memref<1x128xf32>
    %16 = bufferization.alloc_tensor() : tensor<1xf32>
    %17 = bufferization.to_memref %16 : memref<1xf32>
    %18 = bufferization.to_memref %16 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %19 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%3 : memref<1x128xf32>) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %25 = arith.maxnumf %in, %out : f32
      linalg.yield %25 : f32
    }
    %20 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %18 : memref<1x128xf32>, memref<1xf32>) outs(%15 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %25 = arith.subf %in, %in_1 : f32
      %26 = math.exp %25 : f32
      linalg.yield %26 : f32
    }
    %21 = bufferization.to_tensor %15 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%17 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %22 = bufferization.to_tensor %17 : memref<1xf32>
    %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%21 : tensor<1x128xf32>) outs(%22 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %25 = arith.addf %in, %out : f32
      linalg.yield %25 : f32
    } -> tensor<1xf32>
    %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%21, %23 : tensor<1x128xf32>, tensor<1xf32>) outs(%14 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %25 = arith.divf %in, %in_1 : f32
      linalg.yield %25 : f32
    } -> tensor<1x128xf32>
    return %24 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %5 = bufferization.alloc_tensor() : tensor<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %25 = arith.maxnumf %in, %out : f32
      linalg.yield %25 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %25 = arith.subf %in, %in_1 : f32
      %26 = math.exp %25 : f32
      linalg.yield %26 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %25 = arith.addf %in, %out : f32
      linalg.yield %25 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %25 = arith.divf %in, %in_1 : f32
      linalg.yield %25 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %15 = bufferization.to_memref %14 : memref<1x128xf32>
    %16 = bufferization.alloc_tensor() : tensor<1xf32>
    %17 = bufferization.to_memref %16 : memref<1xf32>
    %18 = bufferization.to_memref %16 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %19 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%3 : memref<1x128xf32>) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %25 = arith.maxnumf %in, %out : f32
      linalg.yield %25 : f32
    }
    %20 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %18 : memref<1x128xf32>, memref<1xf32>) outs(%15 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %25 = arith.subf %in, %in_1 : f32
      %26 = math.exp %25 : f32
      linalg.yield %26 : f32
    }
    %21 = bufferization.to_tensor %15 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%17 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %22 = bufferization.to_tensor %17 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%15 : memref<1x128xf32>) outs(%17 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %25 = arith.addf %in, %out : f32
      linalg.yield %25 : f32
    }
    %23 = bufferization.to_tensor %17 : memref<1xf32>
    %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%21, %23 : tensor<1x128xf32>, tensor<1xf32>) outs(%14 : tensor<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %25 = arith.divf %in, %in_1 : f32
      linalg.yield %25 : f32
    } -> tensor<1x128xf32>
    return %24 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: linalg.generic
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %9 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_0 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_0 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_1 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_2 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_2 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_3 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_4 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_4 : memref<1x128xf32>
    %cast_5 = memref.cast %alloc_4 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_5 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_8 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_9 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_9 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %5 = bufferization.alloc_tensor() : tensor<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.subf %in, %in_1 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.divf %in, %in_1 : f32
      linalg.yield %26 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %15 = bufferization.to_memref %14 : memref<1x128xf32>
    %16 = bufferization.to_memref %14 : memref<1x128xf32>
    %17 = bufferization.alloc_tensor() : tensor<1xf32>
    %18 = bufferization.to_memref %17 : memref<1xf32>
    %19 = bufferization.to_memref %17 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %20 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%3 : memref<1x128xf32>) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %21 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %19 : memref<1x128xf32>, memref<1xf32>) outs(%16 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.subf %in, %in_1 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %22 = bufferization.to_tensor %16 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %23 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%16 : memref<1x128xf32>) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %24 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16, %18 : memref<1x128xf32>, memref<1xf32>) outs(%15 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.divf %in, %in_1 : f32
      linalg.yield %26 : f32
    }
    %25 = bufferization.to_tensor %15 : memref<1x128xf32>
    return %25 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %9 = bufferization.to_tensor %alloc_0 : memref<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_1 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %13 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_2 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_2 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_3 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_3 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_4 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_4 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_5 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_5 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_5 : memref<1x128xf32>
    %cast_6 = memref.cast %alloc_5 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_6 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_7 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_7 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_7 : memref<1x128xf32>
    %cast_8 = memref.cast %alloc_7 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_8 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_9 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_9 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_10 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_10 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %5 = bufferization.alloc_tensor() : tensor<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.subf %in, %in_1 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.divf %in, %in_1 : f32
      linalg.yield %26 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %15 = bufferization.to_memref %14 : memref<1x128xf32>
    %16 = bufferization.to_memref %14 : memref<1x128xf32>
    %17 = bufferization.alloc_tensor() : tensor<1xf32>
    %18 = bufferization.to_memref %17 : memref<1xf32>
    %19 = bufferization.to_memref %17 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %20 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%3 : memref<1x128xf32>) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %21 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %19 : memref<1x128xf32>, memref<1xf32>) outs(%16 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.subf %in, %in_1 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %22 = bufferization.to_tensor %16 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %23 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%16 : memref<1x128xf32>) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %24 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16, %18 : memref<1x128xf32>, memref<1xf32>) outs(%15 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.divf %in, %in_1 : f32
      linalg.yield %26 : f32
    }
    %25 = bufferization.to_tensor %15 : memref<1x128xf32>
    return %25 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %9 = bufferization.to_tensor %alloc_0 : memref<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_1 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %13 = bufferization.to_tensor %alloc_2 : memref<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_3 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_3 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_4 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_4 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_5 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_5 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_8 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_8 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_8 : memref<1x128xf32>
    %cast_9 = memref.cast %alloc_8 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_9 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %45 = bufferization.alloc_tensor() : tensor<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_10 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_10 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_11 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_11 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %5 = bufferization.alloc_tensor() : tensor<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.subf %in, %in_1 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.divf %in, %in_1 : f32
      linalg.yield %26 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %15 = bufferization.to_memref %14 : memref<1x128xf32>
    %16 = bufferization.to_memref %14 : memref<1x128xf32>
    %17 = bufferization.alloc_tensor() : tensor<1xf32>
    %18 = bufferization.to_memref %17 : memref<1xf32>
    %19 = bufferization.to_memref %17 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %20 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%3 : memref<1x128xf32>) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %21 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %19 : memref<1x128xf32>, memref<1xf32>) outs(%16 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.subf %in, %in_1 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %22 = bufferization.to_tensor %16 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %23 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%16 : memref<1x128xf32>) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %24 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16, %18 : memref<1x128xf32>, memref<1xf32>) outs(%15 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.divf %in, %in_1 : f32
      linalg.yield %26 : f32
    }
    %25 = bufferization.to_tensor %15 : memref<1x128xf32>
    return %25 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %9 = bufferization.to_tensor %alloc_0 : memref<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_1 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %13 = bufferization.to_tensor %alloc_2 : memref<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_3 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_3 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_4 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_4 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_5 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_5 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_8 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_8 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_8 : memref<1x128xf32>
    %cast_9 = memref.cast %alloc_8 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_9 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_10 = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    %45 = bufferization.to_tensor %alloc_10 : memref<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_11 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_11 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_12 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_12 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %2 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %5 = bufferization.alloc_tensor() : tensor<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.subf %in, %in_1 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.divf %in, %in_1 : f32
      linalg.yield %26 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %15 = bufferization.to_memref %14 : memref<1x128xf32>
    %16 = bufferization.to_memref %14 : memref<1x128xf32>
    %17 = bufferization.alloc_tensor() : tensor<1xf32>
    %18 = bufferization.to_memref %17 : memref<1xf32>
    %19 = bufferization.to_memref %17 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %20 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%3 : memref<1x128xf32>) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %21 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %19 : memref<1x128xf32>, memref<1xf32>) outs(%16 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.subf %in, %in_1 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %22 = bufferization.to_tensor %16 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %23 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%16 : memref<1x128xf32>) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %24 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16, %18 : memref<1x128xf32>, memref<1xf32>) outs(%15 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.divf %in, %in_1 : f32
      linalg.yield %26 : f32
    }
    %25 = bufferization.to_tensor %15 : memref<1x128xf32>
    return %25 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %9 = bufferization.to_tensor %alloc_0 : memref<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_1 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %13 = bufferization.to_tensor %alloc_2 : memref<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_3 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_3 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_4 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_4 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_5 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_5 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_8 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_8 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_8 : memref<1x128xf32>
    %cast_9 = memref.cast %alloc_8 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_9 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_10 = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    %45 = bufferization.to_tensor %alloc_10 : memref<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_11 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_11 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_12 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_12 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %2 = bufferization.to_tensor %alloc : memref<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %5 = bufferization.alloc_tensor() : tensor<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.subf %in, %in_1 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.divf %in, %in_1 : f32
      linalg.yield %26 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %15 = bufferization.to_memref %14 : memref<1x128xf32>
    %16 = bufferization.to_memref %14 : memref<1x128xf32>
    %17 = bufferization.alloc_tensor() : tensor<1xf32>
    %18 = bufferization.to_memref %17 : memref<1xf32>
    %19 = bufferization.to_memref %17 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %20 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%3 : memref<1x128xf32>) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %21 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %19 : memref<1x128xf32>, memref<1xf32>) outs(%16 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.subf %in, %in_1 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %22 = bufferization.to_tensor %16 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %23 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%16 : memref<1x128xf32>) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %24 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16, %18 : memref<1x128xf32>, memref<1xf32>) outs(%15 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %26 = arith.divf %in, %in_1 : f32
      linalg.yield %26 : f32
    }
    %25 = bufferization.to_tensor %15 : memref<1x128xf32>
    return %25 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %9 = bufferization.to_tensor %alloc_0 : memref<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_1 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %13 = bufferization.to_tensor %alloc_2 : memref<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_3 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_3 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_4 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_4 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_5 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_5 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_8 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_8 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_8 : memref<1x128xf32>
    %cast_9 = memref.cast %alloc_8 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_9 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_10 = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    %45 = bufferization.to_tensor %alloc_10 : memref<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_11 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_11 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_12 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_12 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %2 = bufferization.to_tensor %alloc : memref<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<1xf32>
    %5 = bufferization.to_tensor %alloc_1 : memref<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %26 = arith.subf %in, %in_2 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %26 = arith.divf %in, %in_2 : f32
      linalg.yield %26 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %14 = bufferization.alloc_tensor() : tensor<1x128xf32>
    %15 = bufferization.to_memref %14 : memref<1x128xf32>
    %16 = bufferization.to_memref %14 : memref<1x128xf32>
    %17 = bufferization.alloc_tensor() : tensor<1xf32>
    %18 = bufferization.to_memref %17 : memref<1xf32>
    %19 = bufferization.to_memref %17 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %20 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%3 : memref<1x128xf32>) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %21 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %19 : memref<1x128xf32>, memref<1xf32>) outs(%16 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %26 = arith.subf %in, %in_2 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %22 = bufferization.to_tensor %16 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %23 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%16 : memref<1x128xf32>) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %24 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16, %18 : memref<1x128xf32>, memref<1xf32>) outs(%15 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %26 = arith.divf %in, %in_2 : f32
      linalg.yield %26 : f32
    }
    %25 = bufferization.to_tensor %15 : memref<1x128xf32>
    return %25 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %9 = bufferization.to_tensor %alloc_0 : memref<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_1 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %13 = bufferization.to_tensor %alloc_2 : memref<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_3 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_3 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_4 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_4 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_5 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_5 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_8 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_8 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_8 : memref<1x128xf32>
    %cast_9 = memref.cast %alloc_8 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_9 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_10 = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    %45 = bufferization.to_tensor %alloc_10 : memref<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_11 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_11 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_12 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_12 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %2 = bufferization.to_tensor %alloc : memref<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<1xf32>
    %5 = bufferization.to_tensor %alloc_1 : memref<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %26 = arith.subf %in, %in_3 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %26 = arith.divf %in, %in_3 : f32
      linalg.yield %26 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %14 = bufferization.to_tensor %alloc_2 : memref<1x128xf32>
    %15 = bufferization.to_memref %14 : memref<1x128xf32>
    %16 = bufferization.to_memref %14 : memref<1x128xf32>
    %17 = bufferization.alloc_tensor() : tensor<1xf32>
    %18 = bufferization.to_memref %17 : memref<1xf32>
    %19 = bufferization.to_memref %17 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %20 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%3 : memref<1x128xf32>) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %21 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %19 : memref<1x128xf32>, memref<1xf32>) outs(%16 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %26 = arith.subf %in, %in_3 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %22 = bufferization.to_tensor %16 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %23 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%16 : memref<1x128xf32>) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %24 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16, %18 : memref<1x128xf32>, memref<1xf32>) outs(%15 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_3: f32, %out: f32):
      %26 = arith.divf %in, %in_3 : f32
      linalg.yield %26 : f32
    }
    %25 = bufferization.to_tensor %15 : memref<1x128xf32>
    return %25 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
//===-------------------------------------------===//
IR after bufferizing: bufferization.alloc_tensor
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %3 = bufferization.to_tensor %alloc : memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %4 = bufferization.to_tensor %cast : memref<*xf32>
    %5 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %4) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %6 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %7 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %5, %6 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %7 : i64, !llvm.ptr
    %8 = call @__NS__MakeBuffer_f32(%6, %7, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %9 = bufferization.to_tensor %alloc_0 : memref<1x128xf32>
    %10 = bufferization.to_memref %9 : memref<1x128xf32>
    %cast_1 = memref.cast %10 : memref<1x128xf32> to memref<*xf32>
    %11 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %12 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %11) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %13 = bufferization.to_tensor %alloc_2 : memref<1x128xf32>
    %14 = bufferization.to_memref %13 : memref<1x128xf32>
    %cast_3 = memref.cast %14 : memref<1x128xf32> to memref<*xf32>
    %15 = bufferization.to_tensor %cast_3 : memref<*xf32>
    %16 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %15) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %17 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %18 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %12, %17 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %18 : i64, !llvm.ptr
    %19 = llvm.getelementptr %17[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %16, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %20 = llvm.getelementptr %18[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %20 : i64, !llvm.ptr
    %21 = call @__NS__MakeBuffer_f32(%17, %18, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%8, %21) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %22 = call @__NS__GetTensor_f32(%c0_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %23 = call @__NS__NSMemrefToMemref_f32(%22) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %24 = bufferization.to_memref %23 : memref<*xf32>
    %cast_4 = memref.cast %24 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %25 = bufferization.to_tensor %cast_4 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %26 = call @__NS__GetTensor_f32(%c1_i64, %21) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %27 = call @__NS__NSMemrefToMemref_f32(%26) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %28 = bufferization.to_memref %27 : memref<*xf32>
    %cast_5 = memref.cast %28 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %29 = bufferization.to_tensor %cast_5 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %30 = call @softmax_1_128_softmax_1_128_fused_kernel(%25) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %31 = bufferization.to_memref %30 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %31, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %32 = bufferization.to_tensor %alloc_6 : memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %33 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %34 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %33) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %35 = call @softmax_1_128_softmax_1_128_fused_kernel(%29) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %36 = bufferization.to_memref %35 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_8 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %36, %alloc_8 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %37 = bufferization.to_tensor %alloc_8 : memref<1x128xf32>
    %cast_9 = memref.cast %alloc_8 : memref<1x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_9 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %34, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = llvm.getelementptr %40[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %39, %42 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %43 = llvm.getelementptr %41[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %43 : i64, !llvm.ptr
    %44 = call @__NS__MakeBuffer_f32(%40, %41, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_10 = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    %45 = bufferization.to_tensor %alloc_10 : memref<2x128xf32>
    %46 = bufferization.to_memref %45 : memref<2x128xf32>
    %cast_11 = memref.cast %46 : memref<2x128xf32> to memref<*xf32>
    %47 = bufferization.to_tensor %cast_11 : memref<*xf32>
    %48 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %47) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %49 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %50 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %48, %49 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %50 : i64, !llvm.ptr
    %51 = call @__NS__MakeBuffer_f32(%49, %50, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%44, %51) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %52 = call @__NS__GetTensor_f32(%c0_i64, %51) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %53 = call @__NS__NSMemrefToMemref_f32(%52) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %54 = bufferization.to_memref %53 : memref<*xf32>
    %cast_12 = memref.cast %54 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %55 = bufferization.to_tensor %cast_12 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %55 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %2 = bufferization.to_tensor %alloc : memref<1x128xf32>
    %3 = bufferization.to_memref %2 : memref<1x128xf32>
    %4 = bufferization.to_memref %2 : memref<1x128xf32>
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<1xf32>
    %5 = bufferization.to_tensor %alloc_1 : memref<1xf32>
    %6 = bufferization.to_memref %5 : memref<1xf32>
    %7 = bufferization.to_memref %5 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %8 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%7 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %9 = bufferization.to_tensor %7 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %7 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%4 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_4: f32, %out: f32):
      %26 = arith.subf %in, %in_4 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %10 = bufferization.to_tensor %4 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %11 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : memref<1x128xf32>) outs(%6 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %12 = bufferization.to_tensor %6 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : memref<1x128xf32>, memref<1xf32>) outs(%3 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_4: f32, %out: f32):
      %26 = arith.divf %in, %in_4 : f32
      linalg.yield %26 : f32
    }
    %13 = bufferization.to_tensor %3 : memref<1x128xf32>
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %14 = bufferization.to_tensor %alloc_2 : memref<1x128xf32>
    %15 = bufferization.to_memref %14 : memref<1x128xf32>
    %16 = bufferization.to_memref %14 : memref<1x128xf32>
    %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<1xf32>
    %17 = bufferization.to_tensor %alloc_3 : memref<1xf32>
    %18 = bufferization.to_memref %17 : memref<1xf32>
    %19 = bufferization.to_memref %17 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %20 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%3 : memref<1x128xf32>) outs(%19 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxnumf %in, %out : f32
      linalg.yield %26 : f32
    }
    %21 = bufferization.to_tensor %19 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %19 : memref<1x128xf32>, memref<1xf32>) outs(%16 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_4: f32, %out: f32):
      %26 = arith.subf %in, %in_4 : f32
      %27 = math.exp %26 : f32
      linalg.yield %27 : f32
    }
    %22 = bufferization.to_tensor %16 : memref<1x128xf32>
    linalg.generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cst : f32) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %23 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%16 : memref<1x128xf32>) outs(%18 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    }
    %24 = bufferization.to_tensor %18 : memref<1xf32>
    linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16, %18 : memref<1x128xf32>, memref<1xf32>) outs(%15 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_4: f32, %out: f32):
      %26 = arith.divf %in, %in_4 : f32
      linalg.yield %26 : f32
    }
    %25 = bufferization.to_tensor %15 : memref<1x128xf32>
    return %25 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}
//===-------------------------------------------===//
// -----// IR Dump After OneShotBufferize (one-shot-bufferize) //----- //
#map = affine_map<(d0) -> ()>
#map1 = affine_map<(d0) -> (d0)>
#map2 = affine_map<(d0, d1) -> (d0, d1)>
#map3 = affine_map<(d0, d1) -> (d0)>
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %3 = bufferization.to_tensor %cast : memref<*xf32>
    %4 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %3) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %5 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %6 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %4, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %6 : i64, !llvm.ptr
    %7 = call @__NS__MakeBuffer_f32(%5, %6, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %cast_1 = memref.cast %alloc_0 : memref<1x128xf32> to memref<*xf32>
    %8 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %9 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %8) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %cast_3 = memref.cast %alloc_2 : memref<1x128xf32> to memref<*xf32>
    %10 = bufferization.to_tensor %cast_3 : memref<*xf32>
    %11 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %10) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %12 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %13 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %9, %12 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %13 : i64, !llvm.ptr
    %14 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %11, %14 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %15 = llvm.getelementptr %13[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %15 : i64, !llvm.ptr
    %16 = call @__NS__MakeBuffer_f32(%12, %13, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%7, %16) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %17 = call @__NS__GetTensor_f32(%c0_i64, %16) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %18 = call @__NS__NSMemrefToMemref_f32(%17) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %19 = bufferization.to_memref %18 : memref<*xf32>
    %cast_4 = memref.cast %19 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %20 = bufferization.to_tensor %cast_4 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %21 = call @__NS__GetTensor_f32(%c1_i64, %16) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %22 = call @__NS__NSMemrefToMemref_f32(%21) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %23 = bufferization.to_memref %22 : memref<*xf32>
    %cast_5 = memref.cast %23 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %24 = bufferization.to_tensor %cast_5 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %25 = call @softmax_1_128_softmax_1_128_fused_kernel(%20) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %26 = bufferization.to_memref %25 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %26, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %27 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %28 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %27) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %29 = call @softmax_1_128_softmax_1_128_fused_kernel(%24) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %30 = bufferization.to_memref %29 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_8 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %30, %alloc_8 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %cast_9 = memref.cast %alloc_8 : memref<1x128xf32> to memref<*xf32>
    %31 = bufferization.to_tensor %cast_9 : memref<*xf32>
    %32 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %31) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %33 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %34 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %28, %33 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %34 : i64, !llvm.ptr
    %35 = llvm.getelementptr %33[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %32, %35 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %36 = llvm.getelementptr %34[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %36 : i64, !llvm.ptr
    %37 = call @__NS__MakeBuffer_f32(%33, %34, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_10 = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    %cast_11 = memref.cast %alloc_10 : memref<2x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_11 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %39, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = call @__NS__MakeBuffer_f32(%40, %41, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%37, %42) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %43 = call @__NS__GetTensor_f32(%c0_i64, %42) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %44 = call @__NS__NSMemrefToMemref_f32(%43) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %45 = bufferization.to_memref %44 : memref<*xf32>
    %cast_12 = memref.cast %45 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %46 = bufferization.to_tensor %cast_12 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %46 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<1xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%alloc_1 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %3 = arith.maxnumf %in, %out : f32
      linalg.yield %3 : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = ["parallel", "parallel"]} ins(%0, %alloc_1 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%alloc : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_4: f32, %out: f32):
      %3 = arith.subf %in, %in_4 : f32
      %4 = math.exp %3 : f32
      linalg.yield %4 : f32
    }
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel"]} ins(%cst : f32) outs(%alloc_1 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "reduction"]} ins(%alloc : memref<1x128xf32>) outs(%alloc_1 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %3 = arith.addf %in, %out : f32
      linalg.yield %3 : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = ["parallel", "parallel"]} ins(%alloc, %alloc_1 : memref<1x128xf32>, memref<1xf32>) outs(%alloc : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_4: f32, %out: f32):
      %3 = arith.divf %in, %in_4 : f32
      linalg.yield %3 : f32
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<1xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%alloc_3 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "reduction"]} ins(%alloc : memref<1x128xf32>) outs(%alloc_3 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %3 = arith.maxnumf %in, %out : f32
      linalg.yield %3 : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = ["parallel", "parallel"]} ins(%alloc, %alloc_3 : memref<1x128xf32>, memref<1xf32>) outs(%alloc_2 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_4: f32, %out: f32):
      %3 = arith.subf %in, %in_4 : f32
      %4 = math.exp %3 : f32
      linalg.yield %4 : f32
    }
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel"]} ins(%cst : f32) outs(%alloc_3 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "reduction"]} ins(%alloc_2 : memref<1x128xf32>) outs(%alloc_3 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %3 = arith.addf %in, %out : f32
      linalg.yield %3 : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = ["parallel", "parallel"]} ins(%alloc_2, %alloc_3 : memref<1x128xf32>, memref<1xf32>) outs(%alloc_2 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_4: f32, %out: f32):
      %3 = arith.divf %in, %in_4 : f32
      linalg.yield %3 : f32
    }
    %2 = bufferization.to_tensor %alloc_2 : memref<1x128xf32>
    return %2 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}


initializing north_star
register north_star  Type
register north_star  Attr
register north_star  Op
#map = affine_map<(d0) -> ()>
#map1 = affine_map<(d0) -> (d0)>
#map2 = affine_map<(d0, d1) -> (d0, d1)>
#map3 = affine_map<(d0, d1) -> (d0)>
module @NorthStar {
  func.func @main(%arg0: tensor<2x128xf32> {func.device_id = 0 : i64}) -> tensor<2x128xf32> {
    %0 = bufferization.to_memref %arg0 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    %1 = llvm.mlir.constant(2 : i64) : i64
    %2 = llvm.mlir.constant(1 : i64) : i64
    %c0_i64 = arith.constant 0 : i64
    %c1_i64 = arith.constant 1 : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %0, %alloc : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %3 = bufferization.to_tensor %cast : memref<*xf32>
    %4 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %3) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %5 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %6 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %4, %5 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %6 : i64, !llvm.ptr
    %7 = call @__NS__MakeBuffer_f32(%5, %6, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %cast_1 = memref.cast %alloc_0 : memref<1x128xf32> to memref<*xf32>
    %8 = bufferization.to_tensor %cast_1 : memref<*xf32>
    %9 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %8) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %cast_3 = memref.cast %alloc_2 : memref<1x128xf32> to memref<*xf32>
    %10 = bufferization.to_tensor %cast_3 : memref<*xf32>
    %11 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %10) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %12 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %13 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %9, %12 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %13 : i64, !llvm.ptr
    %14 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %11, %14 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %15 = llvm.getelementptr %13[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %15 : i64, !llvm.ptr
    %16 = call @__NS__MakeBuffer_f32(%12, %13, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%7, %16) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %17 = call @__NS__GetTensor_f32(%c0_i64, %16) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %18 = call @__NS__NSMemrefToMemref_f32(%17) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %19 = bufferization.to_memref %18 : memref<*xf32>
    %cast_4 = memref.cast %19 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %20 = bufferization.to_tensor %cast_4 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %21 = call @__NS__GetTensor_f32(%c1_i64, %16) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %22 = call @__NS__NSMemrefToMemref_f32(%21) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %23 = bufferization.to_memref %22 : memref<*xf32>
    %cast_5 = memref.cast %23 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %24 = bufferization.to_tensor %cast_5 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %25 = call @softmax_1_128_softmax_1_128_fused_kernel(%20) {device_id = 0 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %26 = bufferization.to_memref %25 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %26, %alloc_6 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %cast_7 = memref.cast %alloc_6 : memref<1x128xf32> to memref<*xf32>
    %27 = bufferization.to_tensor %cast_7 : memref<*xf32>
    %28 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %27) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %29 = call @softmax_1_128_softmax_1_128_fused_kernel(%24) {device_id = 1 : i64, device_kernel} : (tensor<1x128xf32>) -> tensor<1x128xf32>
    %30 = bufferization.to_memref %29 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_8 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %30, %alloc_8 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %cast_9 = memref.cast %alloc_8 : memref<1x128xf32> to memref<*xf32>
    %31 = bufferization.to_tensor %cast_9 : memref<*xf32>
    %32 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %31) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %33 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %34 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %28, %33 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %34 : i64, !llvm.ptr
    %35 = llvm.getelementptr %33[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %32, %35 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %36 = llvm.getelementptr %34[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %36 : i64, !llvm.ptr
    %37 = call @__NS__MakeBuffer_f32(%33, %34, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_10 = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    %cast_11 = memref.cast %alloc_10 : memref<2x128xf32> to memref<*xf32>
    %38 = bufferization.to_tensor %cast_11 : memref<*xf32>
    %39 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %38) : (i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %40 = llvm.alloca %2 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %41 = llvm.alloca %2 x i64 : (i64) -> !llvm.ptr
    llvm.store %39, %40 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %41 : i64, !llvm.ptr
    %42 = call @__NS__MakeBuffer_f32(%40, %41, %2) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%37, %42) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %43 = call @__NS__GetTensor_f32(%c0_i64, %42) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %44 = call @__NS__NSMemrefToMemref_f32(%43) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
    %45 = bufferization.to_memref %44 : memref<*xf32>
    %cast_12 = memref.cast %45 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    %46 = bufferization.to_tensor %cast_12 : memref<2x128xf32, strided<[?, ?], offset: ?>>
    return %46 : tensor<2x128xf32>
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: tensor<1x128xf32>) -> tensor<1x128xf32> attributes {device_kernel} {
    %0 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %1 = bufferization.to_memref %arg0 : memref<1x128xf32, strided<[?, ?], offset: ?>>
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -3.40282347E+38 : f32
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<1xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%alloc_1 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "reduction"]} ins(%1 : memref<1x128xf32, strided<[?, ?], offset: ?>>) outs(%alloc_1 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %3 = arith.maxnumf %in, %out : f32
      linalg.yield %3 : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = ["parallel", "parallel"]} ins(%0, %alloc_1 : memref<1x128xf32, strided<[?, ?], offset: ?>>, memref<1xf32>) outs(%alloc : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_4: f32, %out: f32):
      %3 = arith.subf %in, %in_4 : f32
      %4 = math.exp %3 : f32
      linalg.yield %4 : f32
    }
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel"]} ins(%cst : f32) outs(%alloc_1 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "reduction"]} ins(%alloc : memref<1x128xf32>) outs(%alloc_1 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %3 = arith.addf %in, %out : f32
      linalg.yield %3 : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = ["parallel", "parallel"]} ins(%alloc, %alloc_1 : memref<1x128xf32>, memref<1xf32>) outs(%alloc : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_4: f32, %out: f32):
      %3 = arith.divf %in, %in_4 : f32
      linalg.yield %3 : f32
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<1xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel"]} ins(%cst_0 : f32) outs(%alloc_3 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "reduction"]} ins(%alloc : memref<1x128xf32>) outs(%alloc_3 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %3 = arith.maxnumf %in, %out : f32
      linalg.yield %3 : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = ["parallel", "parallel"]} ins(%alloc, %alloc_3 : memref<1x128xf32>, memref<1xf32>) outs(%alloc_2 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_4: f32, %out: f32):
      %3 = arith.subf %in, %in_4 : f32
      %4 = math.exp %3 : f32
      linalg.yield %4 : f32
    }
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel"]} ins(%cst : f32) outs(%alloc_3 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "reduction"]} ins(%alloc_2 : memref<1x128xf32>) outs(%alloc_3 : memref<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %3 = arith.addf %in, %out : f32
      linalg.yield %3 : f32
    }
    linalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = ["parallel", "parallel"]} ins(%alloc_2, %alloc_3 : memref<1x128xf32>, memref<1xf32>) outs(%alloc_2 : memref<1x128xf32>) {
    ^bb0(%in: f32, %in_4: f32, %out: f32):
      %3 = arith.divf %in, %in_4 : f32
      linalg.yield %3 : f32
    }
    %2 = bufferization.to_tensor %alloc_2 : memref<1x128xf32>
    return %2 : tensor<1x128xf32>
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, tensor<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> tensor<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}

destroying north_star
