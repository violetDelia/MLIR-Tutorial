module @NorthStar {
  func.func @main(%arg0: memref<2x128xf32> {func.device_id = 0 : i64}, %arg1: memref<2x128xf32>) {
    %c1_i64 = arith.constant 1 : i64
    %c0_i64 = arith.constant 0 : i64
    %0 = llvm.mlir.constant(1 : i64) : i64
    %1 = llvm.mlir.constant(2 : i64) : i64
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    memref.copy %arg0, %alloc : memref<2x128xf32> to memref<2x128xf32>
    %cast = memref.cast %alloc : memref<2x128xf32> to memref<*xf32>
    %2 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast) : (i64, memref<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %3 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %4 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
    llvm.store %2, %3 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %4 : i64, !llvm.ptr
    %5 = call @__NS__MakeBuffer_f32(%3, %4, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %cast_1 = memref.cast %alloc_0 : memref<1x128xf32> to memref<*xf32>
    %6 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_1) : (i64, memref<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    %cast_3 = memref.cast %alloc_2 : memref<1x128xf32> to memref<*xf32>
    %7 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_3) : (i64, memref<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %8 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %9 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %6, %8 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %9 : i64, !llvm.ptr
    %10 = llvm.getelementptr %8[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %7, %10 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %11 = llvm.getelementptr %9[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %11 : i64, !llvm.ptr
    %12 = call @__NS__MakeBuffer_f32(%8, %9, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Scatter(%5, %12) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %13 = call @__NS__GetTensor_f32(%c0_i64, %12) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %14 = call @__NS__NSMemrefToMemref_f32(%13) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> memref<*xf32>
    %cast_4 = memref.cast %14 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_5 = memref.alloc() : memref<1x128xf32>
    memref.copy %cast_4, %alloc_5 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    %15 = call @__NS__GetTensor_f32(%c1_i64, %12) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %16 = call @__NS__NSMemrefToMemref_f32(%15) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> memref<*xf32>
    %cast_6 = memref.cast %16 : memref<*xf32> to memref<1x128xf32, strided<[?, ?], offset: ?>>
    %alloc_7 = memref.alloc() : memref<1x128xf32>
    memref.copy %cast_6, %alloc_7 : memref<1x128xf32, strided<[?, ?], offset: ?>> to memref<1x128xf32>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_8 = memref.alloc() : memref<1x128xf32>
    call @softmax_1_128_softmax_1_128_fused_kernel(%alloc_5, %alloc_8) : (memref<1x128xf32>, memref<1x128xf32>) -> ()
    %alloc_9 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %alloc_8, %alloc_9 : memref<1x128xf32> to memref<1x128xf32>
    %cast_10 = memref.cast %alloc_9 : memref<1x128xf32> to memref<*xf32>
    %17 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_10) : (i64, memref<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    call @__NS__SetDevice(%c1_i64) : (i64) -> ()
    %alloc_11 = memref.alloc() : memref<1x128xf32>
    call @softmax_1_128_softmax_1_128_fused_kernel(%alloc_7, %alloc_11) : (memref<1x128xf32>, memref<1x128xf32>) -> ()
    %alloc_12 = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    memref.copy %alloc_11, %alloc_12 : memref<1x128xf32> to memref<1x128xf32>
    %cast_13 = memref.cast %alloc_12 : memref<1x128xf32> to memref<*xf32>
    %18 = call @__NS__MemrefToNSMemref_f32(%c1_i64, %cast_13) : (i64, memref<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %19 = llvm.alloca %1 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %20 = llvm.alloca %1 x i64 : (i64) -> !llvm.ptr
    llvm.store %17, %19 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %20 : i64, !llvm.ptr
    %21 = llvm.getelementptr %19[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.struct<(i64, struct<(i64, ptr)>)>
    llvm.store %18, %21 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    %22 = llvm.getelementptr %20[1] : (!llvm.ptr) -> !llvm.ptr, i64
    llvm.store %c1_i64, %22 : i64, !llvm.ptr
    %23 = call @__NS__MakeBuffer_f32(%19, %20, %1) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__SetDevice(%c0_i64) : (i64) -> ()
    %alloc_14 = memref.alloc() {alignment = 64 : i64} : memref<2x128xf32>
    %cast_15 = memref.cast %alloc_14 : memref<2x128xf32> to memref<*xf32>
    %24 = call @__NS__MemrefToNSMemref_f32(%c0_i64, %cast_15) : (i64, memref<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %25 = llvm.alloca %0 x !llvm.struct<(i64, struct<(i64, ptr)>)> : (i64) -> !llvm.ptr
    %26 = llvm.alloca %0 x i64 : (i64) -> !llvm.ptr
    llvm.store %24, %25 : !llvm.struct<(i64, struct<(i64, ptr)>)>, !llvm.ptr
    llvm.store %c0_i64, %26 : i64, !llvm.ptr
    %27 = call @__NS__MakeBuffer_f32(%25, %26, %0) : (!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
    call @__NS__Gather(%23, %27) : (!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>) -> ()
    %28 = call @__NS__GetTensor_f32(%c0_i64, %27) : (i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
    %29 = call @__NS__NSMemrefToMemref_f32(%28) : (!llvm.struct<(i64, struct<(i64, ptr)>)>) -> memref<*xf32>
    %cast_16 = memref.cast %29 : memref<*xf32> to memref<2x128xf32, strided<[?, ?], offset: ?>>
    memref.copy %cast_16, %arg1 : memref<2x128xf32, strided<[?, ?], offset: ?>> to memref<2x128xf32>
    return
  }
  func.func private @__NS__SetDevice(i64)
  func.func private @softmax_1_128_softmax_1_128_fused_kernel(%arg0: memref<1x128xf32>, %arg1: memref<1x128xf32>) attributes {device_kernel} {
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x128xf32>
    linalg.softmax dimension(1) ins(%arg0 : memref<1x128xf32>) outs(%alloc : memref<1x128xf32>)
    linalg.softmax dimension(1) ins(%alloc : memref<1x128xf32>) outs(%arg1 : memref<1x128xf32>)
    return
  }
  func.func private @__NS__MemrefToNSMemref_f32(i64, memref<*xf32>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__MakeBuffer_f32(!llvm.ptr, !llvm.ptr, i64) -> !llvm.struct<(i64, ptr, ptr, ptr)>
  func.func private @__NS__Scatter(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
  func.func private @__NS__GetTensor_f32(i64, !llvm.struct<(i64, ptr, ptr, ptr)>) -> !llvm.struct<(i64, struct<(i64, ptr)>)>
  func.func private @__NS__NSMemrefToMemref_f32(!llvm.struct<(i64, struct<(i64, ptr)>)>) -> memref<*xf32>
  func.func private @__NS__Gather(!llvm.struct<(i64, ptr, ptr, ptr)>, !llvm.struct<(i64, ptr, ptr, ptr)>)
}

